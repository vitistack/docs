{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Vitistack","text":""},{"location":"#welcome-to-vitistack","title":"Welcome to Vitistack","text":"<p>Welcome to the Vitistack documentation! This site showcases a custom MkDocs theme designed around the vibrant Viti brand colors.</p>"},{"location":"#what-is-viti","title":"What is Viti?","text":"<p>Viti is a cloud platform software that allows its users to achieve increased cost-efficiency, flexibility, security and interoperability. By leveraging the power of open-source technologies and other software that can support open interfaces and protocols, Viti provides a robust, scalable, flexible and transparent cloud infrastructure solution.</p>"},{"location":"#why-use-viti","title":"Why Use Viti?","text":"<p>Viti combines the best of cloud technologies to deliver:</p> <ul> <li>Enterprise-grade reliability with community-driven innovation</li> <li>Cost-effective scaling without proprietary software limitations</li> <li>Complete operational transparency and control</li> <li>Seamless hybrid and multi-cloud deployments</li> <li>Future-ready architecture built on proven open standards</li> </ul> <p>Whether you're running microservices, traditional applications, or next-generation workloads, Viti provides the foundation for sustainable, efficient, and secure cloud operations.</p> <p></p>"},{"location":"#the-power-of-open-source","title":"The Power of Open Source","text":"<p>Open-source software forms the foundation of Viti's architecture, delivering numerous advantages:</p>"},{"location":"#transparency-trust","title":"\ud83d\udd13 Transparency &amp; Trust","text":"<ul> <li>Full visibility into the underlying code and infrastructure</li> <li>No vendor lock-in - you maintain control over your platform</li> <li>Community-driven security with thousands of eyes reviewing the code</li> <li>Auditable systems that meet compliance and regulatory requirements</li> </ul>"},{"location":"#cost-efficiency","title":"\ud83d\udcb0 Cost Efficiency","text":"<ul> <li>Reduced licensing costs compared to proprietary solutions</li> <li>Lower total cost of ownership through efficient resource utilization</li> <li>Predictable pricing without surprise licensing fees</li> <li>Freedom to scale without additional software licensing constraints</li> </ul>"},{"location":"#flexibility-customization","title":"\ud83d\udd27 Flexibility &amp; Customization","text":"<ul> <li>Adapt to your needs - modify and extend functionality as required</li> <li>Choose your preferred tools from a rich ecosystem of open-source solutions</li> <li>Avoid vendor dependencies that limit your technology choices</li> <li>Future-proof architecture that evolves with your business</li> </ul>"},{"location":"#enhanced-security","title":"\ud83d\udee1\ufe0f Enhanced Security","text":"<ul> <li>Rapid vulnerability patching through active community collaboration</li> <li>Security through obscurity myth debunked - open code enables better security practices</li> <li>No hidden backdoors or proprietary surveillance mechanisms</li> <li>Community-vetted security implementations with proven track records</li> </ul>"},{"location":"#interoperability-standards","title":"\ud83c\udf10 Interoperability &amp; Standards","text":"<ul> <li>Open standards compliance ensuring compatibility across systems</li> <li>API-first design enabling seamless integrations</li> <li>Multi-vendor ecosystem providing choice and preventing monopolies</li> <li>Portable workloads that can move between different environments</li> </ul>"},{"location":"#innovation-community","title":"\ud83d\ude80 Innovation &amp; Community","text":"<ul> <li>Accelerated development through collaborative contribution</li> <li>Access to cutting-edge features developed by global talent</li> <li>Knowledge sharing and best practices from the community</li> <li>Continuous improvement driven by real-world usage and feedback</li> </ul>"},{"location":"theme/","title":"Welcome to Vitistack","text":""},{"location":"theme/#welcome-to-vitistack","title":"Welcome to Vitistack","text":"<p>Welcome to the Vitistack documentation! This site showcases a custom MkDocs theme designed around the vibrant Viti brand colors.</p>"},{"location":"theme/#theme-features","title":"Theme Features","text":"<p>Our custom theme includes:</p> <ul> <li>Modern Color Palette: Inspired by the Viti logo with vibrant blues, energetic greens, and accent ambers</li> <li>Responsive Design: Looks great on desktop and mobile devices</li> <li>Dark Mode Support: Toggle between light and dark themes</li> <li>Enhanced Navigation: Smooth animations and hover effects</li> <li>Beautiful Typography: Carefully styled headings and content</li> </ul>"},{"location":"theme/#examples","title":"Examples","text":""},{"location":"theme/#code-blocks","title":"Code Blocks","text":"<p>Here's an example of how code looks with our theme:</p> <pre><code>def hello_viti():\n    \"\"\"A simple function to demonstrate code styling.\"\"\"\n    print(\"Hello from Vitistack!\")\n    return \"Theme looks great! \ud83c\udfa8\"\n</code></pre>"},{"location":"theme/#admonitions","title":"Admonitions","text":"<p>Theme Note</p> <p>This theme uses a custom color palette based on the Viti brand identity.</p> <p>Pro Tip</p> <p>Try switching between light and dark mode using the toggle in the header!</p> <p>Important</p> <p>Remember to customize the colors further if needed for your specific brand requirements.</p>"},{"location":"theme/#tables","title":"Tables","text":"Feature Status Description Custom Colors \u2705 Complete Brand-aligned color palette Dark Mode \u2705 Complete Full dark theme support Animations \u2705 Complete Smooth transitions and effects Mobile Support \u2705 Complete Responsive design"},{"location":"theme/#links-and-navigation","title":"Links and Navigation","text":"<p>Check out these example links: - GitHub Repository - MkDocs Material</p>"},{"location":"theme/#getting-started","title":"Getting Started","text":"<p>To use this theme in your own MkDocs project:</p> <ol> <li>Copy the <code>viti.css</code> file to your <code>docs/css/</code> directory</li> <li>Add it to your <code>mkdocs.yml</code> under <code>extra_css</code></li> <li>Configure the Material theme with the recommended settings</li> <li>Customize the color variables in CSS to match your brand</li> </ol> <p>Enjoy your beautiful new documentation site! \ud83d\ude80</p>"},{"location":"explanation/","title":"Explanation","text":""},{"location":"explanation/#explanation","title":"Explanation","text":"<p>The Vitistack documentation follows Divio documentation structure which is a method for organizing documentation in a clear and efficient way.</p> <p>Explanation is In-depth background knowledge and theoretical context, and they are understanding-oriented.</p>"},{"location":"explanation/architecture/","title":"Architecture","text":""},{"location":"explanation/architecture/#architecture","title":"Architecture","text":"<p>Work in progress - information to come!</p>"},{"location":"howtoguide/","title":"How-to-guides","text":""},{"location":"howtoguide/#how-to-guides","title":"How-to-guides","text":"<p>The Vitistack documentation follows Divio documentation structure which is a method for organizing documentation in a clear and efficient way. </p> <p>How-to guides is practical instructions for solving specific problems. They guide you through the steps involved in addressing key problems and use-cases. They are more advanced than tutorials and assume some knowledge</p>"},{"location":"howtoguide/setup-vitistack/","title":"How to setup vitistack","text":""},{"location":"howtoguide/setup-vitistack/#how-to-setup-vitistack","title":"How to setup vitistack","text":""},{"location":"howtoguide/setup-vitistack/#prerequisites","title":"Prerequisites","text":"<p>First of all you need a Kubernetes cluster to run all the Vitistack operators.</p>"},{"location":"howtoguide/setup-vitistack/#on-prem","title":"On prem","text":"<p>Create hardware nodes an install ex Talos, K0s or other kubernetes solutions</p>"},{"location":"howtoguide/setup-vitistack/#cloud","title":"Cloud","text":"<ul> <li>Azure Kubernetes Service (AKS)</li> <li>Elastic Kubernetes Service (EKS)</li> <li>Scaleway</li> <li>Upcloud</li> <li>or others</li> </ul>"},{"location":"howtoguide/setup-vitistack/#locally","title":"Locally","text":"<p>You could use Kind (https://kind.sigs.k8s.io/docs/user/quick-start/#installation) or Talosctl (https://docs.siderolabs.com/talos/v1.11/getting-started/talosctl) to install spin up a Kubernetes cluster locally.</p>"},{"location":"howtoguide/setup-vitistack/#visual-cluster-overview","title":"Visual cluster overview","text":"<p>It is also possible that the supervisor cluster also has Kubevirt installed, so there is also support for only one cluster. But it is wise to spread out the risk onto multiple clusters, incause of errors.</p>"},{"location":"howtoguide/setup-vitistack/#crds","title":"CRDS","text":"<p>Install vitistack crds</p>"},{"location":"howtoguide/setup-vitistack/#machine-classes","title":"Machine Classes","text":"<p>Install machineclasses</p>"},{"location":"howtoguide/setup-vitistack/#vitistack-operator","title":"Vitistack operator","text":"<p>Vitistack operator</p>"},{"location":"howtoguide/setup-vitistack/#network","title":"Network","text":"<p>To be continued</p>"},{"location":"howtoguide/setup-vitistack/#dhcp","title":"DHCP","text":"<p>We currently support:</p> <ul> <li>Kea DHCP</li> </ul>"},{"location":"howtoguide/setup-vitistack/#vitistack-machine-providers","title":"Vitistack Machine Providers","text":"<p>To make vitistack machines, we currently support</p> <ul> <li>Kubevirt</li> <li>Proxmox</li> <li>Physical</li> </ul>"},{"location":"howtoguide/setup-vitistack/#vitistack-kubernetes-providers","title":"Vitistack Kubernetes Providers","text":"<p>To install a vitistack Kubernetes cluster, we currently support</p> <ul> <li>Talos</li> <li>AKS</li> </ul>"},{"location":"howtoguide/applications/","title":"Index","text":""},{"location":"howtoguide/applications/argocd_ipam-api/","title":"Deploy IPAM-API w/ Argo CD","text":""},{"location":"howtoguide/applications/argocd_ipam-api/#deploy-ipam-api-w-argo-cd","title":"Deploy IPAM-API w/ Argo CD","text":"<p>Please replace parameter keys with valid secrets!</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: ipam-api\n  namespace: argocd\nspec:\n  project: default\n  source:\n    path: .\n    repoURL: oci://ghcr.io/vitistack/helm/ipam-api\n    targetRevision: 1.*.*\n    helm:\n      valueFiles:\n          - values.prod.yaml\n      parameters:\n      - name: secrets.mongodb\n        value: dc87572kdmfh48djak9375629jsnehj478292\n      - name: secrets.netbox\n        value: dc87572kdmfh48djak9375629jsnehj478292\n      - name: secrets.splunk\n        value: 022847f6a2b4b22877e45ca72345fsa1e4c05\n      - name: encryption.encKey\n        value: 1674361290ebdied\n      - name: encryption.encIv\n        value: tghdavghyjmlmnoi\n  destination:\n    server: \"https://kubernetes.default.svc\"\n    namespace: ipam-system\n  syncPolicy:\n      automated:\n          selfHeal: true\n          prune: true\n      syncOptions:\n      - CreateNamespace=true\n</code></pre>"},{"location":"howtoguide/clusters/","title":"Clusters","text":""},{"location":"howtoguide/clusters/#clusters","title":"Clusters","text":""},{"location":"howtoguide/clusters/example-kubernetesclusters/","title":"Example KubernetesClusters","text":""},{"location":"howtoguide/clusters/example-kubernetesclusters/#example-kubernetesclusters","title":"Example KubernetesClusters","text":""},{"location":"howtoguide/clusters/example-kubernetesclusters/#network-namespace","title":"Network namespace","text":"<p>First we need the NetworkNamespace object</p> <p>Filename: networknamespace.yaml</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: NetworkNamespace\nmetadata:\n  name: t-test01\nspec:\n  datacenterIdentifier: test-north-az1\n  supervisorIdentifier: test-viti\n</code></pre> <p>Apply with:</p> <pre><code>kubectl create namespace t-test01\nkubectl apply -f networknamespace.yaml -n t-test01\n</code></pre>"},{"location":"howtoguide/clusters/example-kubernetesclusters/#small-simple-kubernetescluster","title":"Small simple kubernetescluster","text":"<p>Filename: kubernetescluster.yaml</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: KubernetesCluster\nmetadata:\n  name: t-test-002-5tu8\nspec:\n  data:\n    clusterUid: \"a30fbc8d-596f-48d0-8541-dbc23bca28a1\"\n    clusterId: \"t-test-002-5tu8\"\n    provider: talos\n    environment: dev\n    datacenter: test-south-az1\n    project: simple-project\n    region: south\n    workorder: \"simple-workorder\"\n    zone: \"az1\"\n    workspace: \"simple-workspace\"\n  topology:\n    version: \"1.34.1\"\n    controlplane:\n      replicas: 1\n      version: \"1.34.1\"\n      machineClass: small\n      provider: kubevirt\n      storage:\n        - class: \"standard\"\n          path: \"/var/lib/vitistack/kubevirt\"\n          size: \"20Gi\"\n      metadata:\n        annotations:\n          environment: development\n          region: west-trondelag\n        labels:\n          environment: development\n          region: west-trondelag\n    workers:\n      nodePools:\n        - name: wp\n          taint: []\n          version: \"1.34.1\"\n          replicas: 1\n          machineClass: large\n          autoscaling:\n            enabled: false\n            minReplicas: 1\n            maxReplicas: 5\n            scalingRules:\n              - \"cpu\"\n          metadata:\n            annotations:\n              environment: development\n              region: west-trondelag\n            labels:\n              environment: development\n              region: west-trondelag\n          provider: kubevirt\n          storage:\n            - class: \"standard\"\n              path: \"/var/lib/vitistack/kubevirt\"\n              size: \"20Gi\"\n</code></pre> <p>Apply with:</p> <pre><code>kubectl apply -f kubernetescluster.yaml -n t-test01\n</code></pre>"},{"location":"howtoguide/clusters/install-aks/","title":"Install AKS Operator","text":""},{"location":"howtoguide/clusters/install-aks/#install-aks-operator","title":"Install AKS Operator","text":"<p>To run and install the aks-operator, you need to use or create a service principal in Azure.</p>"},{"location":"howtoguide/clusters/install-aks/#login-to-azure-via-azurecli","title":"Login to Azure via azurecli","text":"<p>(Install azurecli: https://learn.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest)</p> <pre><code>az login\n</code></pre> <p>Then pick the subscription you have access to, or select the relevant subscription if you have access to many subscriptions</p>"},{"location":"howtoguide/clusters/install-aks/#get-appid-and-tentantid-from-existing-service-principal","title":"Get AppId and TentantId from existing Service Principal","text":"<pre><code>az ad sp list --display-name \"vitistack-sp\" --query \"[].{appId:appId, tenant:appOwnerOrganizationId}\"\n[\n  {\n    \"appId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n    \"tenant\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n  }\n]\n</code></pre> <p> If you want to create a new secret (only if you have access to this):</p> <pre><code>az ad sp credential reset --id \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" --query \"{appId:appId, password:password, tenant:tenant}\"\n</code></pre> <p>Output:</p> <pre><code>{\n  \"appId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n  \"password\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"tenant\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n}\n</code></pre>"},{"location":"howtoguide/clusters/install-aks/#create-service-principal-in-azure","title":"Create Service Principal in Azure","text":"<pre><code># Get subscription ID\nSUBSCRIPTION_ID=$(az account show --query id -o tsv)\n\n# Create service principal with Contributor role\naz ad sp create-for-rbac \\\n  --name \"vitistack-sp\" \\\n  --role Contributor \\\n  --scopes /subscriptions/$SUBSCRIPTION_ID\n</code></pre> <p>Output</p> <pre><code>{\n  \"appId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n  \"displayName\": \"vitistack-sp\",\n  \"password\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"tenant\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n}\n</code></pre>"},{"location":"howtoguide/clusters/install-aks/#environment-variables","title":"Environment Variables","text":"Output Field Environment Variable Description <code>appId</code> <code>AZURE_CLIENT_ID</code> Application (client) ID <code>password</code> <code>AZURE_CLIENT_SECRET</code> Client secret (shown only once!) <code>tenant</code> <code>AZURE_TENANT_ID</code> Azure AD tenant ID (from account) <code>AZURE_SUBSCRIPTION_ID</code> Subscription ID"},{"location":"howtoguide/clusters/install-aks/#log-into-ghcrio-to-fetch-oci-helm-package","title":"Log into ghcr.io to fetch oci helm package","text":"<pre><code>helm registry login ghcr.io\n</code></pre>"},{"location":"howtoguide/clusters/install-aks/#create-kubernetes-secret-for-the-helm-chart","title":"Create kubernetes secret for the helm chart","text":"<p>Filename: vitistack-aks-credentials.yaml</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: vitistack-aks-credentials\ntype: Opaque\ndata:\n  subscriptionId: &lt;BASE64_SUBSCRIPTION_ID&gt;\n  tenantId: &lt;BASE64_TENANT_ID&gt;\n  clientId: &lt;BASE64_CLIENT_ID&gt;\n  clientSecret: &lt;BASE64_CLIENT_SECRET&gt;\n</code></pre> <pre><code>kubectl apply -f vitistack-aks-credentials.yaml -n vitistack\n</code></pre> <p>or via terminal:</p> <pre><code>kubectl create secret generic vitistack-aks-credentials \\\n  --from-literal=AZURE_SUBSCRIPTION_ID=&lt;your azure subscription id&gt; \\\n  --from-literal=AZURE_TENANT_ID=&lt;your tenant id here&gt; \\\n  --from-literal=AZURE_CLIENT_ID=&lt;your client id here&gt; \\\n  --from-literal=AZURE_CLIENT_SECRET='&lt;you secret here&gt;' \\\n  -n vitistack\n</code></pre>"},{"location":"howtoguide/clusters/install-aks/#install-the-operator","title":"Install the operator","text":"<pre><code>helm install vitistack-aks-operator oci://ghcr.io/vitistack/helm/aks-operator \\\n  --namespace vitistack \\\n  --set azure.existingSecret=vitistack-aks-credentials \\\n  --create-namespace\n</code></pre>"},{"location":"howtoguide/clusters/install-aks/#upgrade-the-operator-to-latest-version","title":"Upgrade the operator to latest version","text":"<pre><code>helm install vitistack-aks-operator oci://ghcr.io/vitistack/helm/aks-operator \\\n  --namespace vitistack \\\n  --create-namespace \\\n  --reuse-values\n</code></pre>"},{"location":"howtoguide/clusters/install-aks/#default-values-for-the-helm-chart","title":"Default values for the helm chart","text":"<p>Values.yaml from Helm chart</p> <pre><code># Default values for aks-operator.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n# This will set the replicaset count more information can be found here: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/\nreplicaCount: 1\n\n# This sets the container image more information can be found here: https://kubernetes.io/docs/concepts/containers/images/\nimage:\n  repository: ghcr.io/vitistack/viti-aks-operator\n  # This sets the pull policy for images.\n  pullPolicy: IfNotPresent\n  # Overrides the image tag whose default is the chart appVersion.\n  tag: \"\"\n\n# This is for the secrets for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\nimagePullSecrets: []\n# This is to override the chart name.\nnameOverride: \"\"\nfullnameOverride: \"\"\n\n# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/\nserviceAccount:\n  # Specifies whether a service account should be created.\n  create: true\n  # Automatically mount a ServiceAccount's API credentials?\n  automount: true\n  # Annotations to add to the service account.\n  annotations: {}\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template.\n  name: \"\"\n\n# RBAC configuration\nrbac:\n  # Specifies whether RBAC resources should be created.\n  create: true\n\n# Leader election configuration\nleaderElection:\n  # Enable leader election for controller manager.\n  # Enabling this will ensure there is only one active controller manager.\n  enabled: false\n\n# Azure credentials configuration\n# You can either set the credentials directly or use an existing secret\nazure:\n  # Use an existing secret containing Azure credentials\n  # The secret should contain: AZURE_SUBSCRIPTION_ID, AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET\n  existingSecret: \"\"\n  # Or set credentials directly (not recommended for production)\n  subscriptionId: \"\"\n  tenantId: \"\"\n  clientId: \"\"\n  clientSecret: \"\"\n\n# Additional environment variables to set on the container\n# Example:\n# env:\n#   - name: LOG_LEVEL\n#     value: \"debug\"\n#   - name: MY_VAR\n#     valueFrom:\n#       secretKeyRef:\n#         name: my-secret\n#         key: my-key\nenv: []\n\n# Additional envFrom sources to set on the container\n# Example:\n# envFrom:\n#   - secretRef:\n#       name: my-secret\n#   - configMapRef:\n#       name: my-configmap\nenvFrom: []\n\n# This is for setting Kubernetes Annotations to a Pod.\n# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\npodAnnotations: {}\n# This is for setting Kubernetes Labels to a Pod.\n# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\npodLabels: {}\n\npodSecurityContext:\n  fsGroup: 2000\n\nsecurityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n      - ALL\n  readOnlyRootFilesystem: true\n  runAsNonRoot: true\n  runAsUser: 1000\n  seccompProfile:\n    type: RuntimeDefault\n\n# This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/\nservice:\n  # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types\n  type: ClusterIP\n  # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports\n  port: 80\n\n# This block is for setting up the ingress for more information can be found here: https://kubernetes.io/docs/concepts/services-networking/ingress/\ningress:\n  enabled: false\n  className: \"\"\n  annotations:\n    {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n  hosts:\n    - host: chart-example.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls:\n    []\n    # - secretName: chart-example-tls\n    #   hosts:\n    #     - chart-example.local\n\n# -- Expose the service via gateway-api HTTPRoute\n# Requires Gateway API resources and suitable controller installed within the cluster\n# (see: https://gateway-api.sigs.k8s.io/guides/)\nhttpRoute:\n  # HTTPRoute enabled.\n  enabled: false\n  # HTTPRoute annotations.\n  annotations: {}\n  # Which Gateways this Route is attached to.\n  parentRefs:\n    - name: gateway\n      sectionName: http\n      # namespace: default\n  # Hostnames matching HTTP header.\n  hostnames:\n    - chart-example.local\n  # List of rules and filters applied.\n  rules:\n    - matches:\n        - path:\n            type: PathPrefix\n            value: /headers\n  #   filters:\n  #   - type: RequestHeaderModifier\n  #     requestHeaderModifier:\n  #       set:\n  #       - name: My-Overwrite-Header\n  #         value: this-is-the-only-value\n  #       remove:\n  #       - User-Agent\n  # - matches:\n  #   - path:\n  #       type: PathPrefix\n  #       value: /echo\n  #     headers:\n  #     - name: version\n  #       value: v2\n\nresources:\n  # We usually recommend not to specify default resources and to leave this as a conscious\n  # choice for the user. This also increases chances charts run on environments with little\n  # resources, such as Minikube. If you do want to specify resources, uncomment the following\n  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n\n# This is to setup the liveness and readiness probes more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: 9995\nreadinessProbe:\n  httpGet:\n    path: /readyz\n    port: 9995\n\n# This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/\nautoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 100\n  targetCPUUtilizationPercentage: 80\n  # targetMemoryUtilizationPercentage: 80\n\n# Additional volumes on the output Deployment definition.\nvolumes:\n  []\n  # - name: foo\n  #   secret:\n  #     secretName: mysecret\n  #     optional: false\n\n# Additional volumeMounts on the output Deployment definition.\nvolumeMounts:\n  []\n  # - name: foo\n  #   mountPath: \"/etc/foo\"\n  #   readOnly: true\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n</code></pre>"},{"location":"howtoguide/clusters/install-talos/","title":"Install Talos Operator","text":""},{"location":"howtoguide/clusters/install-talos/#install-talos-operator","title":"Install Talos Operator","text":"<pre><code>helm registry login ghcr.io\nhelm install vitistack-talos-operator oci://ghcr.io/vitistack/helm/talos-operator \\\n  --namespace vitistack \\\n  --create-namespace\n</code></pre>"},{"location":"howtoguide/clusters/install-talos/#upgrade-to-latest-version","title":"Upgrade to latest version","text":"<pre><code>helm install vitistack-talos-operator oci://ghcr.io/vitistack/helm/talos-operator \\\n  --namespace vitistack \\\n  --create-namespace \\\n  --reuse-values\n</code></pre> <p>Values.yaml from Helm chart</p> <pre><code># Default values for talos-operator.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n# This will set the replicaset count more information can be found here: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/\nreplicaCount: 1\n\n# This sets the container image more information can be found here: https://kubernetes.io/docs/concepts/containers/images/\nimage:\n  repository: ghcr.io/vitistack/viti-talos-operator\n  # This sets the pull policy for images.\n  pullPolicy: IfNotPresent\n  # Overrides the image tag whose default is the chart appVersion.\n  tag: \"\"\n\n# This is for the secrets for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\nimagePullSecrets: []\n# This is to override the chart name.\nnameOverride: \"\"\nfullnameOverride: \"\"\n\n# Talos Operator Configuration\n# Endpoint mode determines how the operator configures control plane endpoints.\n# Valid values:\n# - \"none\": Use control plane IPs directly (no load balancing)\n# - \"networkconfiguration\": Use ControlPlaneVirtualSharedIP from NetworkNamespace (default)\n# - \"talosvip\": Use Talos built-in VIP (requires additional Talos configuration)\n# - \"custom\": Use user-provided endpoint addresses\nendpointMode: \"networkconfiguration\"\n\n# Custom endpoint addresses (only used when endpointMode is \"custom\")\n# Can be a single IP/hostname or comma-separated list\ncustomEndpoint: \"\"\n\n# Boot image source configuration\n# Valid values: \"pxe\", \"bootimage\"\n# Default: \"pxe\" (uses PXE boot for machine provisioning)\nbootImageSource: \"pxe\"\n\n# Boot image URL (only used when bootImageSource is \"bootimage\")\nbootImage: \"https://factory.talos.dev/image/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515/v1.12.2/metal-amd64.iso\"\n\n# Log configuration\nlogLevel: \"info\"\nlogJson: true\n\n# Secret prefix for cluster secrets\nsecretPrefix: \"\"\n\n# Vitistack name\nvitistackName: \"vitistack\"\n\n# Kubernetes provider name\nkubernetesProviderName: \"talos-provider\"\n\n# Tenant configuration\ntenant:\n  configMapName: \"talos-tenant-config\"\n  configMapNamespace: \"default\"\n  configMapDataKey: \"config.yaml\"\n\n# Talos configuration\ntalos:\n  version: \"v1.12.2\"\n  defaultKubernetesVersion: \"1.35.0\"\n  predictableNetworkNames: true\n  vmInstallImage:\n    kubevirt: \"factory.talos.dev/metal-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.12.2\"\n    default: \"factory.talos.dev/metal-installer/ce4c980550dd2ab1b17bbf2b08801c7eb59418eafe8f279833297925d67c7515:v1.12.2\"\n\n# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/\nserviceAccount:\n  # Specifies whether a service account should be created\n  create: true\n  # Automatically mount a ServiceAccount's API credentials?\n  automount: true\n  # Annotations to add to the service account\n  annotations: {}\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: \"\"\n\n# This is for setting Kubernetes Annotations to a Pod.\n# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\npodAnnotations: {}\n# This is for setting Kubernetes Labels to a Pod.\n# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\npodLabels: {}\n\npodSecurityContext:\n  fsGroup: 2000\n\nsecurityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n      - ALL\n  readOnlyRootFilesystem: true\n  runAsNonRoot: true\n  runAsUser: 1000\n  seccompProfile:\n    type: RuntimeDefault\n\n# This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/\nservice:\n  # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types\n  type: ClusterIP\n  # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports\n  port: 9993\n\n# This block is for setting up the ingress for more information can be found here: https://kubernetes.io/docs/concepts/services-networking/ingress/\ningress:\n  enabled: false\n  className: \"\"\n  annotations: {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n  hosts:\n    - host: chart-example.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n\n# This is to setup the liveness and readiness probes more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: 9993\nreadinessProbe:\n  httpGet:\n    path: /readyz\n    port: 9993\n\n# This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/\n# Note: Operators typically run as single instances with leader election enabled\nautoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 3\n  targetCPUUtilizationPercentage: 80\n  # targetMemoryUtilizationPercentage: 80\n\n# Additional volumes on the output Deployment definition.\nvolumes: []\n# - name: foo\n#   secret:\n#     secretName: mysecret\n#     optional: false\n\n# Additional volumeMounts on the output Deployment definition.\nvolumeMounts: []\n# - name: foo\n#   mountPath: \"/etc/foo\"\n#   readOnly: true\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n\n# Extra environment variables to add to the container\n# Example:\n# extraEnv:\n#   - name: MY_VAR\n#     value: \"my-value\"\nextraEnv: []\n\n# Environment variables from Secrets or ConfigMaps\n# Use this for sensitive data - reference existing secrets/configmaps\n# Example:\n# envFrom:\n#   - secretRef:\n#       name: my-secret\n#   - configMapRef:\n#       name: my-configmap\nenvFrom: []\n</code></pre>"},{"location":"howtoguide/infrastructure/","title":"Infrastructure","text":""},{"location":"howtoguide/infrastructure/#infrastructure","title":"Infrastructure","text":"<p>Prerequisites:</p> <ul> <li>Supervisor cluster</li> <li>Network infrastructure</li> <li>Company network infrastructure</li> <li>DHCP</li> <li>DNS</li> </ul> <p>Optional:</p> <ul> <li>One or more Kubevirt clusters</li> <li>Kubevirt could also be installed in the supervisor cluster to, but do you want that?!</li> <li>One or more instances of Proxmox</li> <li>Matchbox</li> <li>PXE</li> </ul>"},{"location":"howtoguide/infrastructure/install-keadhcp/","title":"Install Kea DHCP and operator","text":""},{"location":"howtoguide/infrastructure/install-keadhcp/#install-kea-dhcp-and-operator","title":"Install Kea DHCP and operator","text":"<p>You need a instance of Kea DHCP, please read this doc for installation and configuration: https://kea.readthedocs.io/en/stable</p>"},{"location":"howtoguide/infrastructure/install-keadhcp/#install-kea-operator","title":"Install Kea-operator","text":"<pre><code>helm registry login ghcr.io\nhelm install vitistack-kea-operator oci://ghcr.io/vitistack/helm/kea-operator \\\n  --namespace vitistack \\\n  --create-namespace\n</code></pre>"},{"location":"howtoguide/infrastructure/install-keadhcp/#upgrade-to-latest-version","title":"Upgrade to latest version","text":"<pre><code>helm install vitistack-kea-operator oci://ghcr.io/vitistack/helm/kea-operator \\\n  --namespace vitistack \\\n  --create-namespace \\\n  --reuse-values\n</code></pre> <p>Values.yaml from helm chart</p> <pre><code># Default values for kea-operator.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n# This will set the replicaset count more information can be found here: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/\nreplicaCount: 1\n\n# This sets the container image more information can be found here: https://kubernetes.io/docs/concepts/containers/images/\nimage:\n  repository: ghcr.io/vitistack/kea-operator\n  # This sets the pull policy for images.\n  pullPolicy: IfNotPresent\n  # Overrides the image tag whose default is the chart appVersion.\n  tag: \"\"\n\n# This is for the secrets for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\nimagePullSecrets: []\n# This is to override the chart name.\nnameOverride: \"\"\nfullnameOverride: \"\"\n\n# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/\nserviceAccount:\n  # Specifies whether a service account should be created\n  create: true\n  # Automatically mount a ServiceAccount's API credentials?\n  automount: true\n  # Annotations to add to the service account\n  annotations: {}\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: \"\"\n\n# This is for setting Kubernetes Annotations to a Pod.\n# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\npodAnnotations: {}\n# This is for setting Kubernetes Labels to a Pod.\n# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\npodLabels: {}\n\npodSecurityContext:\n  fsGroup: 65532\n\nsecurityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n      - ALL\n  readOnlyRootFilesystem: true\n  runAsNonRoot: true\n  runAsUser: 65532\n  runAsGroup: 65532\n  seccompProfile:\n    type: RuntimeDefault\n\n# This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/\nservice:\n  # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types\n  type: ClusterIP\n  # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports\n  port: 80\n\n# This block is for setting up the ingress for more information can be found here: https://kubernetes.io/docs/concepts/services-networking/ingress/\ningress:\n  enabled: false\n  className: \"\"\n  annotations:\n    {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n  hosts:\n    - host: chart-example.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n\n# This is to setup the liveness and readiness probes more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: 9995\n  initialDelaySeconds: 15\n  periodSeconds: 20\nreadinessProbe:\n  httpGet:\n    path: /readyz\n    port: 9995\n  initialDelaySeconds: 5\n  periodSeconds: 10\n\n# This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/\nautoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 100\n  targetCPUUtilizationPercentage: 80\n  # targetMemoryUtilizationPercentage: 80\n\n# Additional volumes on the output Deployment definition.\nvolumes: []\n# - name: foo\n#   secret:\n#     secretName: mysecret\n#     optional: false\n\n# Additional volumeMounts on the output Deployment definition.\nvolumeMounts: []\n# - name: foo\n#   mountPath: \"/etc/foo\"\n#   readOnly: true\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n\n# KEA DHCP server configuration\nkea:\n  # Primary KEA server URL (e.g., https://kea-dhcp.example.com:8000)\n  url: \"\"\n  # Secondary KEA server URL for HA failover (optional)\n  secondaryUrl: \"\"\n  # KEA server port (used if url doesn't include port)\n  port: \"8000\"\n  # Timeout in seconds for KEA API requests\n  timeoutSeconds: \"10\"\n  # Disable HTTP keep-alive connections\n  disableKeepalives: \"true\"\n  # Comma-separated list of required client classes for pools\n  requireClientClasses: \"biosclients,ueficlients,ipxeclients\"\n\n  # Basic authentication credentials\n  # These should be overridden in your ArgoCD app or values override\n  auth:\n    username: \"\"\n    password: \"\"\n    # Reference to an existing secret containing credentials\n    # If set, username/password above are ignored\n    existingSecret: \"\"\n    # Key in the secret for username\n    usernameKey: \"username\"\n    # Key in the secret for password\n    passwordKey: \"password\"\n\n  # TLS configuration\n  tls:\n    enabled: \"false\"\n    insecure: \"false\"\n    serverName: \"\"\n    # Path to CA certificate file (mounted via volumes)\n    caFile: \"\"\n    # Path to client certificate file (for mTLS)\n    certFile: \"\"\n    # Path to client key file (for mTLS)\n    keyFile: \"\"\n    # Reference to an existing secret containing TLS certificates\n    secretName: \"\"\n    secretNamespace: \"\"\n\n# Logging configuration\nlogging:\n  level: \"info\"\n  jsonLogging: \"true\"\n  colorize: \"false\"\n  addCaller: \"true\"\n  disableStacktrace: \"false\"\n  unescapeMultiline: \"false\"\n\n# Development mode\ndevelopment: \"false\"\n</code></pre>"},{"location":"howtoguide/infrastructure/install-machineclasses/","title":"Install Machine Classes","text":""},{"location":"howtoguide/infrastructure/install-machineclasses/#install-machine-classes","title":"Install Machine Classes","text":"<p>These a example machine classes</p>"},{"location":"howtoguide/infrastructure/install-machineclasses/#small","title":"Small","text":"<p>filename: machineclass-small.yaml </p><pre><code>apiVersion: vitistack.io/v1alpha1\nkind: MachineClass\nmetadata:\n  name: small\nspec:\n  displayName: Small\n  description: Small instance with 2 cores and 8Gi memory\n  enabled: true\n  default: false\n  category: Standard\n  cpu:\n    cores: 2\n    sockets: 1\n    threads: 1\n  memory:\n    quantity: 8Gi\n  machineProviders: []\n</code></pre><p></p> <pre><code>kubectl apply -f machineclass-small.yaml\n</code></pre>"},{"location":"howtoguide/infrastructure/install-machineclasses/#medium","title":"Medium","text":"<p>filename: machineclass-medium.yaml </p><pre><code>apiVersion: vitistack.io/v1alpha1\nkind: MachineClass\nmetadata:\n  name: medium\nspec:\n  displayName: Medium\n  description: Medium instance with 4 cores and 16Gi memory\n  enabled: true\n  default: true\n  category: Standard\n  cpu:\n    cores: 4\n    sockets: 1\n    threads: 1\n  memory:\n    quantity: 16Gi\n  machineProviders: []\n</code></pre><p></p> <pre><code>kubectl apply -f machineclass-medium.yaml\n</code></pre>"},{"location":"howtoguide/infrastructure/install-machineclasses/#large","title":"Large","text":"<p>filename: machineclass-large.yaml </p><pre><code>apiVersion: vitistack.io/v1alpha1\nkind: MachineClass\nmetadata:\n  name: large\nspec:\n  displayName: Large\n  description: Large instance with 4 cores and 32Gi memory\n  enabled: true\n  default: false\n  category: Standard\n  cpu:\n    cores: 4\n    sockets: 1\n    threads: 1\n  memory:\n    quantity: 32Gi\n  machineProviders: []\n</code></pre><p></p> <pre><code>kubectl apply -f machineclass-large.yaml\n</code></pre>"},{"location":"howtoguide/infrastructure/install-network-operator/","title":"Install Network operator","text":""},{"location":"howtoguide/infrastructure/install-network-operator/#install-network-operator","title":"Install Network operator","text":"<p>To be continued</p>"},{"location":"howtoguide/infrastructure/install-vitistack-operator/","title":"Install Vitistack operator","text":""},{"location":"howtoguide/infrastructure/install-vitistack-operator/#install-vitistack-operator","title":"Install Vitistack operator","text":"<p>The vitistack operator handles the vitistack crd object. The operator fetches information and adds it to the vitistack crd object, so other solutions could show or integrate with the vitistack. One example is ROR (Release Operate Report) found here: https://github.com/norskHelsenett/ror</p> <p>Install the vitistack operator by:</p> <pre><code>helm registry login ghcr.io\nhelm install vitistack-operator oci://ghcr.io/vitistack/helm/vitistack-operator \\\n  --namespace vitistack \\\n  --create-namespace\n</code></pre>"},{"location":"howtoguide/infrastructure/install-vitistack-operator/#upgrade-to-latest-version","title":"Upgrade to latest version","text":"<pre><code>helm install vitistack-operator oci://ghcr.io/vitistack/helm/vitistack-operator \\\n  --namespace vitistack \\\n  --create-namespace \\\n  --reuse-values\n</code></pre> <p>Values.yaml from helm chart:</p> <pre><code># Default values for vitistack-operator.\n\ncrds:\n  install: true\n\nreplicaCount: 1\n\nimage:\n  repository: ghcr.io/vitistack/vitistack-operator\n  pullPolicy: IfNotPresent\n  tag: \"\"\n\nimagePullSecrets: []\nnameOverride: \"\"\nfullnameOverride: \"\"\n\nserviceAccount:\n  create: true\n  automount: true\n  annotations: {}\n  name: \"\"\n\nrbac:\n  create: true\n\nvitistackCrdName: \"vitistack\"\nconfigMapName: \"vitistack-config\"\ndevelopment: false\nname: \"test-stack\"\ndescription: \"A test vitistack deployment\"\nregion: \"central\"\ncountry: \"no\"\nzone: \"az1\"\ninfrastructure: \"test\"\n\nlogging:\n  jsonLogging: true\n  level: \"info\"\n  colorize: false\n  addCaller: true\n  disableStacktrace: false\n  unescapeMultiline: false\n\npodAnnotations: {}\npodLabels: {}\n\npodSecurityContext:\n  runAsNonRoot: true\n  fsGroup: 2000\n  runAsUser: 1001\n  runAsGroup: 1001\n  seccompProfile:\n    type: RuntimeDefault\n\nsecurityContext:\n  capabilities:\n    drop:\n      - ALL\n  readOnlyRootFilesystem: true\n  runAsNonRoot: true\n  runAsUser: 1000\n  allowPrivilegeEscalation: false\n\nservice:\n  type: NodePort\n  port: 9991\n\ningress:\n  enabled: false\n  className: \"\"\n  annotations: {}\n  hosts:\n    - host: chart-example.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls: []\n\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n\nlivenessProbe:\n  httpGet:\n    path: /health\n    port: 9991\nreadinessProbe:\n  httpGet:\n    path: /health\n    port: 9991\n\nautoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 100\n  targetCPUUtilizationPercentage: 80\n\nvolumes: []\nvolumeMounts: []\n\nnodeSelector: {}\ntolerations: []\naffinity: {}\n</code></pre>"},{"location":"howtoguide/infrastructure/vitistack-crds/","title":"Install Vitistack CRDs","text":""},{"location":"howtoguide/infrastructure/vitistack-crds/#install-vitistack-crds","title":"Install Vitistack CRDs","text":"<p>Using Helm (recommended)</p> <p>First, login to GitHub Container Registry</p> <p>Username: your GitHub username</p> <p>Password: a Personal Access Token (PAT) with <code>read:packages</code> scope</p> <p>Create a PAT at: https://github.com/settings/tokens/new?scopes=read:packages</p> <pre><code>helm registry login ghcr.io\nhelm install vitistack-crds oci://ghcr.io/vitistack/helm/crds \\\n  --namespace vitistack \\\n  --create-namespace\n</code></pre> <p>Or using kubectl (no authentication required)</p> <pre><code>kubectl apply -f https://github.com/vitistack/common/releases/latest/download/crds.yaml\n</code></pre>"},{"location":"howtoguide/infrastructure/vitistack-crds/#upgrade-to-latest-version","title":"Upgrade to latest version","text":"<pre><code>helm upgrade vitistack-crds oci://ghcr.io/vitistack/helm/crds \\\n  --namespace vitistack \\\n  --create-namespace \\\n  --reuse-values\n</code></pre>"},{"location":"howtoguide/machines/","title":"Machines","text":""},{"location":"howtoguide/machines/#machines","title":"Machines","text":""},{"location":"howtoguide/machines/example-machines/","title":"Example Machines","text":""},{"location":"howtoguide/machines/example-machines/#example-machines","title":"Example Machines","text":""},{"location":"howtoguide/machines/example-machines/#network-namespace","title":"Network namespace","text":"<p>First we need the NetworkNamespace object</p> <p>Filename: networknamespace.yaml</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: NetworkNamespace\nmetadata:\n  name: t-test01\nspec:\n  datacenterIdentifier: test-north-az1\n  supervisorIdentifier: test-viti\n</code></pre> <p>Apply with:</p> <pre><code>kubectl create namespace t-test01\nkubectl apply -f networknamespace.yaml -n t-test01\n</code></pre>"},{"location":"howtoguide/machines/example-machines/#debian-iso","title":"Debian iso","text":"<p>Dependent on the experimental feature CDI (https://kubevirt.io/user-guide/storage/containerized_data_importer)</p> <p>Filename: machine-iso-debian.yaml</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: Machine\nmetadata:\n  name: example-machine-iso-debian\n  annotations:\n    # Annotation to indicate we want to use a DataVolume for the boot source\n    kubevirt.io/boot-source: \"datavolume\"\n    kubevirt.io/boot-source-type: \"http\"\nspec:\n  machineClass: \"medium\"\n  name: \"debian-iso-vm\"\n  provider: kubevirt\n\n  # Operating system configuration\n  os:\n    family: linux\n    distribution: debian\n    version: \"13.2\"\n    architecture: amd64\n    # HTTP URL to the debian ISO image\n    # This will be used to create a DataVolume with CDI\n    imageID: \"https://cdimage.debian.org/debian-cd/current/amd64/iso-cd/debian-13.2.0-amd64-netinst.iso\"\n\n  # Define disks - the ISO will be attached as a cdrom\n  disks:\n    - name: \"root\"\n      sizeGB: 50\n      boot: true\n      type: \"virtio\"\n      encrypted: false\n</code></pre> <p>Apply with:</p> <pre><code>kubectl apply -f machine-iso-debian.yaml -n t-test01\n</code></pre>"},{"location":"howtoguide/machines/example-machines/#talos-iso","title":"Talos iso","text":"<p>Dependent on the experimental feature CDI (https://kubevirt.io/user-guide/storage/containerized_data_importer)</p> <p>Filename: machine-iso-talos.yaml</p> <pre><code>---\napiVersion: vitistack.io/v1alpha1\nkind: Machine\nmetadata:\n  name: example-machine-iso-talos\n  annotations:\n    # Annotation to indicate we want to use a DataVolume for the boot source\n    kubevirt.io/boot-source: \"datavolume\"\n    kubevirt.io/boot-source-type: \"http\"\nspec:\n  machineClass: \"medium\"\n  name: \"talos-iso-vm\"\n  provider: kubevirt\n\n  # Operating system configuration\n  os:\n    family: linux\n    distribution: talos\n    version: \"1.11.5\"\n    architecture: amd64\n    # HTTP URL to the Talos ISO image\n    # This will be used to create a DataVolume with CDI\n    imageID: \"https://github.com/siderolabs/talos/releases/download/v1.11.5/metal-amd64.iso\"\n\n  # Define disks - the ISO will be attached as a cdrom\n  disks:\n    - name: \"root\"\n      sizeGB: 50\n      boot: true\n      type: \"virtio\"\n      encrypted: false\n</code></pre> <p>Apply with:</p> <pre><code>kubectl apply -f machine-iso-talos.yaml -n t-test01\n</code></pre>"},{"location":"howtoguide/machines/install-kubevirt/","title":"Install Kubevirt and operator","text":""},{"location":"howtoguide/machines/install-kubevirt/#install-kubevirt-and-operator","title":"Install Kubevirt and operator","text":""},{"location":"howtoguide/machines/install-kubevirt/#kubevirt-in-kubernetes","title":"Kubevirt in kubernetes","text":"<p>If you are running one or more own kubevirt clusters, you have to install the vitistack crds into the kubevirt cluster(s) too: install vitistack crds</p> <p>(docs with kind: https://kubevirt.io/quickstart_kind)</p> <pre><code>export VERSION=$(curl -s https://storage.googleapis.com/kubevirt-prow/release/kubevirt/kubevirt/stable.txt)\n\necho $VERSION\nkubectl create -f \"https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/kubevirt-operator.yaml\"\n</code></pre>"},{"location":"howtoguide/machines/install-kubevirt/#install-the-crds","title":"Install the CRDS","text":"<pre><code>kubectl create -f \"https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/kubevirt-cr.yaml\"\n</code></pre>"},{"location":"howtoguide/machines/install-kubevirt/#verify-kubevirt-components","title":"Verify kubevirt components","text":"<pre><code>kubectl get kubevirt.kubevirt.io/kubevirt -n kubevirt -o=jsonpath=\"{.status.phase}\"\n</code></pre> <p>Check the components</p> <pre><code>kubectl get all -n kubevirt\n</code></pre> <p>Wait and see that the Kube Virt CRD object (<code>kubevirt</code>) has <code>Phase</code>: <code>Deployed</code></p> <pre><code>$ kubectl get kubevirts.kubevirt.io -n kubevirt\nNAME       AGE     PHASE\nkubevirt   4m24s   Deployed\n</code></pre>"},{"location":"howtoguide/machines/install-kubevirt/#virtualized-environment","title":"Virtualized environment","text":"<p>If running in a virtualized environment</p> <p>If the kind cluster runs on a virtual machine consider enabling nested virtualization. Follow the instructions described here. If for any reason nested virtualization cannot be enabled do enable KubeVirt emulation as follows:</p> <pre><code>kubectl -n kubevirt patch kubevirt kubevirt --type=merge --patch '{\"spec\":{\"configuration\":{\"developerConfiguration\":{\"useEmulation\":true}}}}'\n</code></pre>"},{"location":"howtoguide/machines/install-kubevirt/#virtctl","title":"Virtctl","text":"<p>KubeVirt provides an additional binary called virtctl for quick access to the serial and graphical ports of a VM and also handle start/stop operations.</p> <p>Install virtctl can be retrieved from the release page of the KubeVirt github page.</p> <pre><code>VERSION=$(kubectl get kubevirt.kubevirt.io/kubevirt -n kubevirt -o=jsonpath=\"{.status.observedKubeVirtVersion}\")\nARCH=$(uname -s | tr A-Z a-z)-$(uname -m | sed 's/x86_64/amd64/') || windows-amd64.exe\necho ${ARCH}\ncurl -L -o virtctl https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/virtctl-${VERSION}-${ARCH}\nsudo install -m 0755 virtctl /usr/local/bin\n</code></pre>"},{"location":"howtoguide/machines/install-kubevirt/#containerized-data-importer-cdi","title":"Containerized Data Importer (CDI)","text":"<p>Notice, this is an experimental feature from Kubevirt</p> <p>Docs: https://kubevirt.io/labs/kubernetes/lab2.html</p> <p>\"You can experiment with this lab online at Killercoda In this lab, you will learn how to use Containerized Data Importer (CDI) to import Virtual Machine images for use with Kubevirt. CDI simplifies the process of importing data from various sources into Kubernetes Persistent Volumes, making it easier to use that data within your virtual machines.</p> <p>CDI introduces DataVolumes, custom resources meant to be used as abstractions of PVCs. A custom controller watches for DataVolumes and handles the creation of a target PVC with all the spec and annotations required for importing the data. Depending on the type of source, other specific CDI controller will start the import process and create a raw image named disk.img with the desired content into the target PVC.\"</p> <p>To install:</p> <pre><code>export VERSION=$(basename $(curl -s -w %{redirect_url} https://github.com/kubevirt/containerized-data-importer/releases/latest))\nkubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-operator.yaml\nkubectl create -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-cr.yaml\n</code></pre>"},{"location":"howtoguide/machines/install-kubevirt/#multus","title":"Multus","text":"<p>The Vitistack uses Multus together with Kubevirt, so please install multus.</p> <p>Docs: https://github.com/k8snetworkplumbingwg/multus-cni and https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/quickstart.md</p>"},{"location":"howtoguide/machines/install-kubevirt/#kubevirt-operator","title":"Kubevirt Operator","text":""},{"location":"howtoguide/machines/install-kubevirt/#how-create-a-kubevirtconfig","title":"How create a KubeVirtConfig","text":"<p>Create a k8s secret from file content (kubeconfig file to the kubevirt cluster)</p> <pre><code>kubectl create secret generic kubevirt-provider --from-file=kubeconfig=&lt;path to kubevirt kubeconfig file&gt;`\n</code></pre> <p>Note, if you are using the supervisor cluster as the kubevirt cluster, use the kubernetes service address in the kubeconfig</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    server: https://kubernetes.default.svc:443\n....\n</code></pre>"},{"location":"howtoguide/machines/install-kubevirt/#create-the-kubevirtconfig","title":"Create the KubevirtConfig","text":"<p>Create and modify this yaml</p> <p>Filename: kubevirtconfig.yaml</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: KubevirtConfig\nmetadata:\n  name: kubevirt-provider\nspec:\n  name: kubevirt-provider\n  secretNamespace: default\n  kubeconfigSecretRef: kubevirt-provider\n</code></pre> <p>And then:</p> <p><code>kubectl apply -f kubevirtconfig.yaml</code></p>"},{"location":"howtoguide/machines/install-kubevirt/#install-the-kubevirt-operator","title":"Install the Kubevirt-operator","text":"<pre><code>helm registry login ghcr.io\nhelm install vitistack-kubevirt-operator oci://ghcr.io/vitistack/helm/kubevirt-operator \\\n  --namespace vitistack \\\n  --create-namespace\n</code></pre>"},{"location":"howtoguide/machines/install-kubevirt/#upgrade-to-latest-version","title":"Upgrade to latest version","text":"<pre><code>helm install vitistack-kubevirt-operator oci://ghcr.io/vitistack/helm/kubevirt-operator \\\n  --namespace vitistack \\\n  --create-namespace \\\n  --reuse-values\n</code></pre>"},{"location":"howtoguide/machines/install-kubevirt/#kubevirt-operator-helm-values","title":"Kubevirt-operator helm values","text":"<p>Values.yaml from Helm chart:</p> <pre><code># Default values for kubevirt-operator.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n# This will set the replicaset count more information can be found here: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/\nreplicaCount: 1\n\n# This sets the container image more information can be found here: https://kubernetes.io/docs/concepts/containers/images/\nimage:\n  repository: ghcr.io/vitistack/viti-kubevirt-operator\n  # This sets the pull policy for images.\n  pullPolicy: IfNotPresent\n  # Overrides the image tag whose default is the chart appVersion.\n  tag: \"\"\n\n# This is for the secrets for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\nimagePullSecrets: []\n# This is to override the chart name.\nnameOverride: \"\"\nfullnameOverride: \"\"\n\n# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/\nserviceAccount:\n  # Specifies whether a service account should be created\n  create: true\n  # Automatically mount a ServiceAccount's API credentials?\n  automount: true\n  # Annotations to add to the service account\n  annotations: {}\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: \"\"\n\n# This is for setting Kubernetes Annotations to a Pod.\n# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\npodAnnotations: {}\n# This is for setting Kubernetes Labels to a Pod.\n# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\npodLabels: {}\n\npodSecurityContext:\n  fsGroup: 2000\n  runAsGroup: 2000\n  runAsUser: 1000\n  runAsNonRoot: true\n\nsecurityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n      - ALL\n  readOnlyRootFilesystem: true\n  runAsNonRoot: true\n  runAsUser: 1000\n  runAsGroup: 2000\n  seccompProfile:\n    type: RuntimeDefault\n\n# This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/\nservice:\n  # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types\n  type: ClusterIP\n  # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports\n  port: 80\n\n# This block is for setting up the ingress for more information can be found here: https://kubernetes.io/docs/concepts/services-networking/ingress/\ningress:\n  enabled: false\n  className: \"\"\n  annotations: {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n  hosts:\n    - host: chart-example.local\n      paths:\n        - path: /\n          pathType: ImplementationSpecific\n  tls: []\n  #  - secretName: chart-example-tls\n  #    hosts:\n  #      - chart-example.local\n\nresources:\n  limits:\n    cpu: 500m\n    memory: 512Mi\n  requests:\n    cpu: 100m\n    memory: 256Mi\n\n# This is to setup the liveness and readiness probes more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: 9992\n  initialDelaySeconds: 15\n  periodSeconds: 20\nreadinessProbe:\n  httpGet:\n    path: /readyz\n    port: 9992\n  initialDelaySeconds: 5\n  periodSeconds: 10\n\n# This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/\nautoscaling:\n  enabled: true\n  minReplicas: 1\n  maxReplicas: 100\n  targetCPUUtilizationPercentage: 80\n  # targetMemoryUtilizationPercentage: 80\n\n# Additional volumes on the output Deployment definition.\nvolumes: []\n# - name: foo\n#   secret:\n#     secretName: mysecret\n#     optional: false\n\n# Additional volumeMounts on the output Deployment definition.\nvolumeMounts: []\n# - name: foo\n#   mountPath: \"/etc/foo\"\n#   readOnly: true\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n\n# Operator configuration\n# These settings are passed as environment variables to the operator\n\n# Namespace the operator manages (uses Release.Namespace if empty)\nnamespace: \"\"\n# CPU model for VMs: \"host-model\" (default for x86), \"host-passthrough\" (required for ARM)\ncpuModel: \"\"\n# CNI version for NetworkAttachmentDefinitions\nnadCniVersion: \"1.0.0\"\n# Where to fetch public IPs from: \"vmi\" (default, from KubeVirt VMI) or \"networkconfiguration\"\nipSource: \"vmi\"\n# Enable Containerized Data Importer (CDI) support\nkubevirtSupportCDI: true\n# Name of the MachineProvider\nmachineProviderName: \"kubevirt-provider\"\n# PVC volume mode: \"Block\" (default) or \"Filesystem\"\npvcVolumeMode: \"Block\"\n# PVC access mode for DataVolumes: \"ReadWriteMany\" (default), \"ReadWriteOnce\", \"ReadOnlyMany\"\n# Use \"ReadWriteOnce\" for local-path, Ceph RBD; \"ReadWriteMany\" for CephFS, NFS\npvcAccessMode: \"ReadWriteMany\"\n# Storage class name for PVCs. If empty (default), uses the cluster's default storage class.\n# Set this to a specific storage class name to override the default (e.g., \"local-path\", \"ceph-rbd\")\nstorageClassName: \"\"\n# Optional prefix for VM names\nvmNamePrefix: \"\"\n# Vitistack name (optional)\nvitistackName: \"vitistack\"\n\n# Logging configuration\nlogging:\n  # Log level: debug, info, warn, error\n  level: \"info\"\n  # Output logs as JSON\n  json: true\n  # Add caller information to logs\n  addCaller: false\n  # Disable stacktrace in logs\n  disableStacktrace: true\n  # Unescape multiline log messages\n  unescapedMultiline: false\n  # Colorize log lines (for development)\n  colorizeLine: false\n\n# Leader election for HA deployments\nleaderElection:\n  enabled: true\n\n# Metrics configuration\nmetrics:\n  # Enable metrics endpoint\n  enabled: true\n  # Metrics bind address (use \":8443\" for HTTPS, \":8080\" for HTTP, \"0\" to disable)\n  bindAddress: \":8443\"\n  # Serve metrics over HTTPS\n  secure: true\n\n# Health probe configuration\nhealthProbe:\n  # Health probe bind address\n  bindAddress: \":9992\"\n\n# RBAC configuration\nrbac:\n  # Create ClusterRole and ClusterRoleBinding\n  create: true\n</code></pre>"},{"location":"howtoguide/machines/install-physical-operator/","title":"Install Physical operator","text":""},{"location":"howtoguide/machines/install-physical-operator/#install-physical-operator","title":"Install Physical operator","text":"<p>To be continued</p>"},{"location":"howtoguide/machines/install-proxmox/","title":"Install Proxmox and operator","text":""},{"location":"howtoguide/machines/install-proxmox/#install-proxmox-and-operator","title":"Install Proxmox and operator","text":"<p>You need one or more Proxmox instances.</p> <p>To install proxmox, follow this installation guide:</p> <ul> <li>https://proxmox.com/en/products/proxmox-virtual-environment/get-started</li> <li>https://pve.proxmox.com/pve-docs/chapter-pve-installation.html</li> </ul>"},{"location":"howtoguide/machines/install-proxmox/#install-the-proxmox-operator","title":"Install the Proxmox operator","text":"<p>Setup the kubernetes secret for Proxmox:</p> <p>Existing secret containing Proxmox credentials The secret should contain: PROXMOX_ENDPOINT, PROXMOX_USERNAME, PROXMOX_PASSWORD or PROXMOX_TOKEN_ID, PROXMOX_TOKEN_SECRET</p>"},{"location":"howtoguide/machines/install-proxmox/#create-the-secret-with","title":"Create the secret with:","text":"<pre><code>kubectl create secret generic proxmox-credentials \\\n    --from-literal=PROXMOX_ENDPOINT=https://proxmox.example.com:8006/api2/json \\\n    --from-literal=PROXMOX_USERNAME=root@pam \\\n    --from-literal=PROXMOX_PASSWORD=yourpassword\n</code></pre>"},{"location":"howtoguide/machines/install-proxmox/#or-for-token-auth","title":"Or for token auth:","text":"<pre><code>kubectl create secret generic proxmox-credentials \\\n    --from-literal=PROXMOX_ENDPOINT=https://proxmox.example.com:8006/api2/json \\\n    --from-literal=PROXMOX_TOKEN_ID=root@pam!mytoken \\\n    --from-literal=PROXMOX_TOKEN_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n</code></pre>"},{"location":"howtoguide/machines/install-proxmox/#or-with-yaml","title":"or with yaml","text":"<p>Filename: proxmox-credentials.yaml</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: proxmox-credentials\ntype: Opaque\nstringData:\n  PROXMOX_ENDPOINT: \"https://proxmox.example.com:8006/api2/json\"\n  # Use username/password auth:\n  PROXMOX_USERNAME: \"root@pam\"\n  PROXMOX_PASSWORD: \"yourpassword\"\n  # OR use token auth (comment out username/password above):\n  # PROXMOX_TOKEN_ID: \"root@pam!mytoken\"\n  # PROXMOX_TOKEN_SECRET: \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n</code></pre> <p>Apply with</p> <pre><code>kubectl apply -f proxmox-credentials.yaml\n</code></pre>"},{"location":"howtoguide/machines/install-proxmox/#proxmox-operator","title":"Proxmox Operator","text":"<pre><code>helm registry login ghcr.io\nhelm install vitistack-proxmox-operator oci://ghcr.io/vitistack/helm/proxmox-operator \\\n  --namespace vitistack \\\n  --create-namespace\n</code></pre>"},{"location":"howtoguide/machines/install-proxmox/#upgrade-to-latest-version","title":"Upgrade to latest version","text":"<pre><code>helm install vitistack-proxmox-operator oci://ghcr.io/vitistack/helm/proxmox-operator \\\n  --namespace vitistack \\\n  --create-namespace \\\n  --reuse-values\n</code></pre>"},{"location":"howtoguide/machines/install-proxmox/#operator-helm-values","title":"Operator helm values","text":"<p>Values.yaml from helm chart:</p> <pre><code># Default values for proxmox-operator.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n# This will set the replicaset count more information can be found here: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/\nreplicaCount: 1\n\n# This sets the container image more information can be found here: https://kubernetes.io/docs/concepts/containers/images/\nimage:\n  repository: ghcr.io/vitistack/viti-proxmox-operator\n  # This sets the pull policy for images.\n  pullPolicy: IfNotPresent\n  # Overrides the image tag whose default is the chart appVersion.\n  tag: \"\"\n\n# This is for the secrets for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\nimagePullSecrets: []\n# This is to override the chart name.\nnameOverride: \"\"\nfullnameOverride: \"\"\n\n# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/\nserviceAccount:\n  # Specifies whether a service account should be created\n  create: true\n  # Automatically mount a ServiceAccount's API credentials?\n  automount: true\n  # Annotations to add to the service account\n  annotations: {}\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: \"\"\n\n# This is for setting Kubernetes Annotations to a Pod.\n# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/\npodAnnotations: {}\n# This is for setting Kubernetes Labels to a Pod.\n# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\npodLabels: {}\n\npodSecurityContext:\n  runAsNonRoot: true\n  fsGroup: 65532\n\nsecurityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n      - ALL\n  readOnlyRootFilesystem: true\n  runAsNonRoot: true\n  runAsUser: 65532\n  runAsGroup: 65532\n  seccompProfile:\n    type: RuntimeDefault\n\n# This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/\nservice:\n  # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types\n  type: ClusterIP\n  # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports\n  port: 8081\n\nresources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n\n# This is to setup the liveness and readiness probes more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: 8081\n  initialDelaySeconds: 15\n  periodSeconds: 20\nreadinessProbe:\n  httpGet:\n    path: /readyz\n    port: 8081\n  initialDelaySeconds: 5\n  periodSeconds: 10\n\n# This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/\nautoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 100\n  targetCPUUtilizationPercentage: 80\n  # targetMemoryUtilizationPercentage: 80\n\n# Additional volumes on the output Deployment definition.\nvolumes: []\n# - name: foo\n#   secret:\n#     secretName: mysecret\n#     optional: false\n\n# Additional volumeMounts on the output Deployment definition.\nvolumeMounts: []\n# - name: foo\n#   mountPath: \"/etc/foo\"\n#   readOnly: true\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n\n# Existing secret containing Proxmox credentials\n# The secret should contain: PROXMOX_ENDPOINT, PROXMOX_USERNAME, PROXMOX_PASSWORD\n# or PROXMOX_TOKEN_ID, PROXMOX_TOKEN_SECRET\n# Create the secret with:\n#   kubectl create secret generic proxmox-credentials \\\n#     --from-literal=PROXMOX_ENDPOINT=https://proxmox.example.com:8006/api2/json \\\n#     --from-literal=PROXMOX_USERNAME=root@pam \\\n#     --from-literal=PROXMOX_PASSWORD=yourpassword\n# Or for token auth:\n#   kubectl create secret generic proxmox-credentials \\\n#     --from-literal=PROXMOX_ENDPOINT=https://proxmox.example.com:8006/api2/json \\\n#     --from-literal=PROXMOX_TOKEN_ID=root@pam!mytoken \\\n#     --from-literal=PROXMOX_TOKEN_SECRET=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nexistingSecret: \"proxmox-credentials\"\n\n# Logging configuration\nlogging:\n  # Log level (debug, info, warn, error)\n  level: \"info\"\n  # Output logs in JSON format\n  json: true\n\n# Proxmox connection settings (required)\nproxmox:\n  # Proxmox API endpoint URL (e.g., https://proxmox.example.com:8006/api2/json)\n  endpoint: \"\"\n  # Authentication - use either username/password OR token\n  # Username authentication (e.g., root@pam)\n  username: \"\"\n  password: \"\"\n  # Token authentication (alternative to username/password)\n  tokenId: \"\"\n  tokenSecret: \"\"\n  # Skip TLS certificate verification (not recommended for production)\n  insecureTLS: false\n\n# VM management settings\nvm:\n  # Starting VM ID for new machines\n  idStart: 2000\n  # Node selection strategy: first, random, round-robin\n  nodeSelection: \"first\"\n  # Comma-separated list of allowed nodes (empty means all nodes)\n  allowedNodes: \"\"\n  # Default storage pool for VM disks\n  defaultStorage: \"local-lvm\"\n  # Default network bridge for VMs\n  defaultNetwork: \"vmbr0\"\n  # Network model for VM interfaces (virtio, e1000, e1000e, rtl8139, vmxnet3)\n  # virtio = VirtIO paravirtualized (best performance)\n  networkModel: \"virtio\"\n  # Default VLAN tag for VM network interfaces (0 = no VLAN tagging)\n  defaultVlan: 0\n  # Default MTU for VM network interfaces (0 = use Proxmox default of 1500)\n  # Common values: 1500 (standard), 9000 (jumbo frames)\n  defaultMtu: 0\n  # Default CPU type (e.g., host, kvm64, x86-64-v2-AES)\n  defaultCpuType: \"x86-64-v2-AES\"\n  # Enable NUMA for VMs\n  enableNuma: true\n  # SCSI controller type\n  scsiController: \"virtio-scsi-single\"\n  # Enable QEMU Guest Agent (required for network status)\n  enableQemuAgent: true\n  # Start VM immediately after creation\n  startOnCreate: true\n\n# Network configuration settings\nnetwork:\n  # IP address source for VMs:\n  # - \"proxmox\": Get IP from Proxmox/QEMU Guest Agent (default)\n  # - \"networkconfiguration\": Get IP from NetworkConfiguration CRD (Kea DHCP reservation)\n  ipSource: \"proxmox\"\n  # Second octet for MAC address generation (format: 02:XX:RR:RR:RR:RR where XX is this value)\n  # Default is 24 (0x18 in hex), resulting in MAC addresses like 02:18:XX:XX:XX:XX\n  macSet: \"24\"\n</code></pre>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#reference","title":"Reference","text":"<p>The Vitistack documentation follows Divio documentation structure which is a method for organizing documentation in a clear and efficient way.</p> <p>Reference is detailed and accurate information, such as API docs or configuration settings. They describe how it works and how to use it but assume that you have a basic understanding of key concepts.</p> <p>Reference have one job only: to describe. They are code-determined, because ultimately that's what they describe: key classes, functions, APIs, and so they should list things like functions, fields, attributes and methods, and set out how to use them.</p>"},{"location":"reference/dns/","title":"DNS","text":""},{"location":"reference/dns/#dns","title":"DNS","text":""},{"location":"reference/dns/#concept","title":"Concept","text":"<ul> <li>Each datacenter has its own set of dns-servers and one or more gslb-operators</li> <li>Every cluster has a dns-operator responsible for populating the gslb config for its ingress/svc/httprout/etc.</li> <li></li> </ul>"},{"location":"reference/dns/#model","title":"model","text":""},{"location":"reference/opensourcesoftware/","title":"Open Source Software","text":""},{"location":"reference/opensourcesoftware/#open-source-software","title":"Open Source Software","text":"<p>The Vitistack consists of the following open-source software:</p>"},{"location":"reference/opensourcesoftware/#talos-linux","title":"Talos Linux","text":"<p>Container native operating system specially developed for cloud/container environments and container-based applications.</p> <p>For more information see - https://github.com/siderolabs/talos </p>"},{"location":"reference/opensourcesoftware/#kubevirt","title":"KubeVirt","text":"<p>KubeVirt is an open-source solution that enables virtual machines (VMs) to run side-by-side with containers in a Kubernetes cluster.</p> <p>For more information see - https://github.com/kubevirt</p>"},{"location":"reference/opensourcesoftware/#libvirt","title":"LibVirt","text":"<p>LibVirt is an open source library and toolkit for managing virtualization platforms, acting as an abstraction layer that enables control of different hypervisors and virtualization engines via a common API.</p> <p>For more information see - https://github.com/libvirt/libvirt</p>"},{"location":"reference/opensourcesoftware/#ceph","title":"Ceph","text":"<p>Ceph is an open-source, distributed storage system for both block storage, object storage, and file systems.</p> <p>For more information see - https://github.com/ceph/ceph</p>"},{"location":"reference/opensourcesoftware/#rook","title":"Rook","text":"<p>Rook is an open-source, storage orchestrator for Kubernetes that makes it easy to set up, manage, and use distributed storage within Kubernetes clusters.</p> <p>For more information see - https://github.com/rook/rook</p>"},{"location":"reference/opensourcesoftware/#cilium","title":"Cilium","text":"<p>Cilium is a cloud-native networking and security solution for Kubernetes clusters, providing advanced network policies, load balancing, and observability using eBPF technology for high-performance packet processing.</p> <p>For more information see - https://cilium.io</p>"},{"location":"reference/opensourcesoftware/#metallb","title":"MetalLB","text":"<p>MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols to expose the service outside the cluster.</p> <p>For more information see - https://metallb.io</p>"},{"location":"reference/opensourcesoftware/#cert-manager","title":"cert-manager","text":"<p>cert-manager is a Kubernetes add-on that automates the management and issuance of TLS certificates, and to assign certificate to Admission Webhook components.</p> <p>For more information see - https://cert-manager.io/</p>"},{"location":"reference/repository/","title":"Repository","text":""},{"location":"reference/repository/#repository","title":"Repository","text":"<p>Vististack repositories:</p> <ul> <li> https://github.com/orgs/vitistack/repositories</li> </ul>"},{"location":"reference/api/","title":"API","text":""},{"location":"reference/api/#api","title":"API","text":"<p>An API (Application Programming Interface) is an interface that allows different programs or systems to communicate with each other. It lets you request data or perform actions without needing to know how the other system is built.</p>"},{"location":"reference/api/ipam-api/","title":"IPAM API","text":""},{"location":"reference/api/ipam-api/#ipam-api","title":"IPAM API","text":""},{"location":"reference/api/ipam-api/#concept","title":"Concept","text":"<p>An API for retrieving ip addresses from Netbox and storing them in MongoDB along with services connected to the ip address.</p>"},{"location":"reference/api/ipam-api/#ipam-api-requirements","title":"IPAM-API Requirements","text":"<ul> <li>Go 1.24+</li> <li>MongoDB instance</li> <li>NetBox instance and API token</li> </ul>"},{"location":"reference/api/ipam-api/#netbox-requirements","title":"Netbox Requirements","text":"<p>We use Netbox for controlling zones and which prefixes are available for each zone. </p>"},{"location":"reference/api/ipam-api/#custom-field-choices","title":"Custom Field Choices","text":"<ul> <li><code>k8s_zone_choices</code> is used to define the zones available for the api. These sones are fetched from Netbox and used to validate the request. This custom field should be a select with the valid zones.</li> </ul>"},{"location":"reference/api/ipam-api/#custom-fields","title":"Custom Fields","text":"<ul> <li><code>k8s_zone</code> is the <code>k8s_zone_choices</code> selection with valid zones</li> <li><code>k8s_uuid</code> is used for the mongodb _id for the registered address</li> </ul>"},{"location":"reference/api/ipam-api/#prefixes","title":"Prefixes","text":"<ul> <li><code>type</code> needs to be <code>\"container\"</code></li> <li>Custom field <code>k8s_zone</code> needs to be added to the prefix</li> <li>Custom field <code>k8s_uuid</code> needs to be added to the prefix</li> </ul> <p>When <code>k8s_zone</code> and <code>container</code> is set the prefix is available for IPAM-API</p>"},{"location":"reference/api/ipam-api/#environment-setup","title":"Environment setup","text":"<p>Create a <code>config.json</code></p> <pre><code>{\n  \"mongodb\": {\n    \"username\": \"Admin\",\n    \"password_path\": \"mongodb.secret\",\n    \"host\": \"localhost\",\n    \"port\": 27017,\n    \"database\": \"vitistack-ipam-api\"\n  },\n  \"netbox\": {\n    \"url\": \"https://netbox.example.com\",\n    \"token_path\": \"netbox.secret\",\n    \"constraint_tag\": \"vitistack\"\n  },\n  \"splunk\": {\n    \"enable\": true,\n    \"url\": \"https://splunk-hec.example.com\",\n    \"token_path\": \"splunk.secret\",\n    \"index\": \"vitistack\",\n    \"source\": \"vitistack:ipam-api\",\n    \"sourcetype_app\": \"ipam-api:app\",\n    \"sourcetype_http\": \"ipam-api:http\"\n  },\n  \"encryption_secrets\": {\n    \"path\": \"secrets.json\"\n  }\n}\n</code></pre>"},{"location":"reference/api/ipam-api/#clone-and-run","title":"Clone and run","text":"<pre><code>git clone https://github.com/vitistack/ipam-api.git\ncd ipam-api\ngo mod tidy\ngo run cmd/ipam-api/main.go\n</code></pre>"},{"location":"reference/api/ipam-api/#api-endpoints","title":"API Endpoints","text":"Method Path Description <code>POST</code> <code>/</code> Register or update a ip address, returns IP <code>DELETE</code> <code>/</code> Set expiration for a service for an IP address"},{"location":"reference/api/ipam-api/#swagger-doc","title":"Swagger doc","text":"<p>Swagger documentation is available at ipam-api:3000/swagger/index.html</p>"},{"location":"reference/api/ipam-api/#request-body","title":"Request Body","text":"<pre><code>{\n    \"address\": \"83.118.168.10/32\",\n    \"ip_family\": \"ipv4\",\n    \"zone\": \"inet\", \n    \"secret\": \"SuperSecret\",\n    \"new_secret\": \"SuperSecret\",\n    \"service\": {\n        \"service_name\": \"ingress_inet\",\n        \"namespace_id\": \"467579ae-b8d5-4524-9ce8-bcb66ee02ce0\",\n        \"cluster_id\": \"0f3c7805-6b1d-4387-b8c4-b8c5d0e9b878\",\n        \"retention_period_days\": 0,\n        \"deny_external_cleanup\": false\n    },\n}\n</code></pre> Top level fields Field Required Description <code>address</code> Optional Specific IP address to register (must be available in NetBox zone) <code>ip_family</code> Yes IP family, either <code>\"ipv4\"</code> or <code>\"ipv6\"</code> <code>zone</code> Yes Logical zone (must match a Netbox custom field <code>k8s_zone</code>) <code>secret</code> Yes Unique identifier for the service; used for authentication and tracking <code>new_secret</code> Optional If provided, replaces the <code>secret</code> after successful update <code>Service</code> Yes Object containing metadata about the service being registered (see below) <p>Service fields</p> Field Required Description <code>service_name</code> Yes Name of the service being registered <code>namespace_id</code> Yes UUID of the namespace associated with the service <code>cluster_id</code> Yes UUID of the cluster the service belongs to <code>retention_period_days</code> Optional Days to retain the IP even if the service is deleted <code>deny_external_cleanup</code> Optional Prevents the IP from being cleaned by external tools"},{"location":"reference/api/ipam-api/#examples","title":"Examples","text":""},{"location":"reference/api/ipam-api/#register-a-service","title":"Register a service","text":"<pre><code>curl -X POST http://localhost:3000/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"address\": \"83.118.168.10/32\",\n    \"ip_family\": \"ipv4\",\n    \"zone\": \"inet\",\n    \"secret\": \"SuperSecret\",\n    \"service\": {\n        \"service_name\": \"ingress_inet\",\n        \"namespace_id\": \"467579ae-b8d5-4524-9ce8-bcb66ee02ce0\",\n        \"cluster_id\": \"0f3c7805-6b1d-4387-b8c4-b8c5d0e9b878\",\n        \"retention_period_days\": 0,\n        \"deny_external_cleanup\": false\n    },\n}'\n</code></pre>"},{"location":"reference/api/ipam-api/#set-expiration-for-a-service","title":"Set expiration for a service","text":"<pre><code>curl -X DELETE http://localhost:3000/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"secret\": \"supersecret\",\n    \"zone\": \"inet\",\n    \"address\": \"83.118.168.10/32\",\n    \"ip_family\": \"ipv4\",\n    \"service\": {\n        \"service_name\": \"ingress_inet\",\n        \"namespace_id\": \"467579ae-b8d5-4524-9ce8-bcb66ee02ce0\",\n        \"cluster_id\": \"0f3c7805-6b1d-4387-b8c4-b8c5d0e9b878\",\n        \"retention_period_days\": 0,\n        \"deny_external_cleanup\": false\n    },\n}'\n</code></pre>"},{"location":"reference/api/ipam-api/#api-request-flows","title":"API Request flows","text":""},{"location":"reference/api/ipam-api/#post-request","title":"POST request","text":""},{"location":"reference/api/ipam-api/#delete-request","title":"DELETE request","text":""},{"location":"reference/api/ipam-api/#ipam-cli","title":"IPAM-CLI","text":"<p>The ipam-cli can be used to display or manipulate ip addresses and services stored in MongoDB, and to create a Mongodb dump for backup</p>"},{"location":"reference/api/ipam-api/#requirements","title":"Requirements","text":"<ul> <li><code>config.json</code> must be available and correctly configured (see main IPAM-API docs)</li> <li><code>ipam-cli</code> assumes access to the same MongoDB as the IPAM-API </li> </ul>"},{"location":"reference/api/ipam-api/#build","title":"Build","text":"<pre><code>go build -o bin/ipam-cli ./cmd/cli\n</code></pre>"},{"location":"reference/api/ipam-api/#ipam-api-help","title":"ipam-api help","text":"<pre><code>Command-line interface for interacting with the Vitistack IPAM-API.\n\nUsage:\n  ipam-cli [flags]\n  ipam-cli [command]\n\nAvailable Commands:\n  completion     Generate the autocompletion script for the specified shell\n  delete-cluster Set exiresAt == time.Now() for services linked to a cluster id\n  delete-service Delete service from address\n  help           Help about any command\n  mongo-backup   Backup MongoDB data\n  replace-secret Replace address secret\n  show-secret    Show address secret\n  show-services  Show address services\n  version        Print the version\n\nFlags:\n  -h, --help   help for ipam-cli\n\nUse \"ipam-cli [command] --help\" for more information about a command.\n</code></pre>"},{"location":"reference/api/ipam-api/#examples_1","title":"Examples","text":""},{"location":"reference/api/ipam-api/#show-all-services-for-an-address","title":"Show all services for an address","text":"<pre><code>ipam-cli show-services --zone inet --address 83.118.168.10/32\n</code></pre>"},{"location":"reference/api/ipam-api/#delete-cluster","title":"Delete cluster","text":"<pre><code>ipam-cli delete-cluster --cluster-id 0f3c7805-6b1d-4387-b8c4-b8c5d0e9b878\n</code></pre>"},{"location":"reference/api/ipam-api/#mongodump-for-backup","title":"MongoDump for backup","text":"<pre><code>ipam-cli mongo-backup --out ipam-backup.gz\n</code></pre> This will save the backup file to ./backups/ipam-backup.gz"},{"location":"reference/api/ipam-api/#logging","title":"Logging","text":""},{"location":"reference/api/ipam-api/#files","title":"Files","text":""},{"location":"reference/api/ipam-api/#local-filesystem","title":"Local filesystem","text":"<p>ipam-api logs can be found in ./logs</p> <ul> <li>ipam-api.log - Application logs</li> <li>http.log - HTTP logs</li> </ul>"},{"location":"reference/api/ipam-api/#splunk","title":"Splunk","text":""},{"location":"reference/api/ipam-api/#configuration","title":"Configuration","text":"<p>When <code>splunk</code> is present in <code>config.json</code> logs from the ipam-api app and http server will be forwarded to a Splunk HEC.  </p><pre><code>{\n  \"splunk\": {\n    \"enable\": true,\n    \"url\": \"https://splunk-hec.example.com\",\n    \"token_path\": \"splunk.secret\",\n    \"index\": \"vitistack\",\n    \"source\": \"vitistack:ipam-api\",\n    \"sourcetype_app\": \"ipam-api:app\",\n    \"sourcetype_http\": \"ipam-api:http\"\n  }\n}  \n</code></pre><p></p> Key Type Description <code>enable</code> <code>boolean</code> Whether Splunk integration is enabled (<code>true</code>) or disabled (<code>false</code>). <code>url</code> <code>string</code> URL of the Splunk HEC endpoint, in the form <code>https://splunk-hec.example.com</code>. <code>token_path</code> <code>string</code> Path to the file or secret containing the Splunk HEC token. <code>index</code> <code>string</code> The Splunk index where logs will be stored. <code>source</code> <code>string</code> The name of the source, e.g., <code>vitistack:ipam-api</code>. <code>sourcetype_app</code> <code>string</code> The Splunk sourcetype used for application logs. <code>sourcetype_http</code> <code>string</code> The Splunk sourcetype used for HTTP request logs."},{"location":"reference/api/ipam-api/#splunk-overview","title":"Splunk overview","text":""},{"location":"reference/configuration/","title":"Configuration","text":""},{"location":"reference/configuration/#configuration","title":"Configuration","text":"<p>Work in progress - information to come!</p>"},{"location":"reference/configuration/clusterdiscovery/","title":"Cluster Discovery","text":""},{"location":"reference/configuration/clusterdiscovery/#cluster-discovery","title":"Cluster Discovery","text":""},{"location":"reference/configuration/clusterdiscovery/#annotations","title":"Annotations","text":"<p>A cluster that is created or should be adopted by vitistack must have the following annotations or labels on at least one of its nodes</p> Annotation/Label Description Example vitistack.io/clustername the name of the cluster t-mgmt-001 vitistack.io/clusterworkspace the workspace of the cluster nhn-l44t vitistack.io/country the country abreviation no vitistack.io/region the region west vitistack.io/infrastructure the infrastructure, if omited = prod mgmt,test vitistack.io/az the availability zone az1 vitistack.io/vmprovider the provider of the machine kubevirt vitistack.io/vmid the name of the vm in the vm provider t-mgmt-001-ctp01 vitistack.io/kubernetesprovider the provider of kubernetes talos vitistack.io/clusterid an unique id of the cluster t-mgmt-001-l33t vitistack.io/kubernetes-endpoint-addr the loadbalanced api endpoint https://10.20.30.40:6443"},{"location":"reference/configuration/clusterdiscovery/#dns-names-may-be-used","title":"DNS-names (may be used)","text":"Type Pattern Example cluster [clusterId].[az].[region].[country].platform.nhn.no t-mgmt-001-l33t.az1.west.no.platform.nhn.no node [hostname].[clusterId].[workspaceId].[az].[region].[country].platform.nhn.no t-mgmt-001-cpl01.t-mgmt-001-l33t.t-nhn-l44t.az1.west.no.platform.nhn.no"},{"location":"reference/configuration/clusterdiscovery/#infrastructure","title":"Infrastructure","text":"Infrastructure DNS prod t-mgmt-001-l33t.az1.west.no.platform.nhn.no mgmt t-mgmt-001-l33t.az1.west.no.mgmt.platform.nhn.no test t-mgmt-001-l33t.az1.west.no.test.platform.nhn.no"},{"location":"reference/crd/","title":"Custom Resource Definitions (CRDs)","text":""},{"location":"reference/crd/#custom-resource-definitions-crds","title":"Custom Resource Definitions (CRDs)","text":"<p>Work in progress!</p> <p>The Viti Stack Custom Resource Definitions (CRDs) provide declarative API specifications for managing infrastructure resources in Kubernetes clusters. These CRDs define the data structures and schemas used across all Viti Stack operators to ensure consistent resource management and interoperability.</p>"},{"location":"reference/crd/#api-overview","title":"API Overview","text":""},{"location":"reference/crd/#api-group-and-version","title":"API Group and Version","text":"<p>All Viti Stack CRDs belong to the <code>vitistack.io/v1alpha1</code> API group:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: &lt;ResourceKind&gt;\n</code></pre>"},{"location":"reference/crd/#supported-resource-types","title":"Supported Resource Types","text":"Resource Kind Scope Purpose Vitistack <code>Vitistack</code> Namespaced Core infrastructure configuration Machine <code>Machine</code> Namespaced Virtual machine and compute resource Machine Provider <code>MachineProvider</code> Namespaced Machine provisioning backend Kubernetes Cluster <code>KubernetesCluster</code> Namespaced Kubernetes cluster configuration Kubernetes Provider <code>KubernetesProvider</code> Namespaced Kubernetes provisioning backend Network Configuration <code>NetworkConfiguration</code> Namespaced Network interface configuration Network Namespace <code>NetworkNamespace</code> Namespaced Network isolation boundary Load Balancer <code>LoadBalancer</code> Namespaced Load balancing configuration KubeVirt Config <code>KubevirtConfig</code> Namespaced KubeVirt operator configuration Proxmox Config <code>ProxmoxConfig</code> Namespaced Proxmox operator configuration"},{"location":"reference/crd/#core-resource-specifications","title":"Core Resource Specifications","text":""},{"location":"reference/crd/#vitistack-resource","title":"Vitistack Resource","text":"<p>Primary configuration resource for Viti Stack infrastructure:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: Vitistack\nmetadata:\n  name: string                     # Infrastructure identifier\n  namespace: string               # Kubernetes namespace\nspec:\n  # Infrastructure Configuration\n  infrastructure:\n    providers:\n    - type: string                 # Provider type: proxmox, talos, kubevirt, kea\n      name: string                # Provider instance name\n      endpoint: string            # Provider API endpoint\n      region: string              # Provider region/zone\n      credentials:\n        secretRef:\n          name: string            # Secret containing credentials\n          namespace: string       # Secret namespace\n\n  # Global Settings\n  settings:\n    defaultStorageClass: string   # Default storage class for volumes\n    defaultNetworkPolicy: string  # Default network policy\n    monitoring:\n      enabled: bool              # Enable monitoring integration\n      namespace: string          # Monitoring namespace\n    logging:\n      enabled: bool              # Enable centralized logging\n      endpoint: string           # Log aggregation endpoint\n\n  # Resource Quotas\n  quotas:\n    machines:\n      total: int                 # Maximum machines across providers\n      perProvider: int           # Maximum machines per provider\n    storage:\n      total: string              # Total storage quota (e.g., \"1Ti\")\n      perMachine: string         # Maximum storage per machine\n    network:\n      maxNetworks: int           # Maximum networks per namespace\n\nstatus:\n  phase: string                  # Current phase: Pending, Active, Failed\n  conditions: []Condition        # Status conditions\n  providers: []ProviderStatus    # Provider status information\n  resources:\n    machines: int               # Current machine count\n    networks: int               # Current network count\n    storage: string             # Current storage usage\n</code></pre>"},{"location":"reference/crd/#machine-resource","title":"Machine Resource","text":"<p>Defines virtual machine or compute resource specifications:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: Machine\nmetadata:\n  name: string\n  namespace: string\n  labels:\n    cluster.vitistack.io/cluster-name: string\n    vitistack.io/provider: string\nspec:\n  # Provider Configuration\n  providerRef:\n    apiVersion: string           # Provider API version\n    kind: string                # Provider kind: MachineProvider\n    name: string                # Provider instance name\n    namespace: string           # Provider namespace\n\n  # Resource Specification\n  resources:\n    cpu:\n      cores: int                # CPU cores (1-128)\n      threads: int              # Threads per core (1-2)\n      sockets: int              # CPU sockets (1-4)\n    memory:\n      size: string              # Memory size (e.g., \"4Gi\", \"8Gi\")\n    gpu:\n      type: string              # GPU type: nvidia, amd, intel\n      count: int                # GPU count\n      model: string             # Specific GPU model\n\n  # Storage Configuration\n  storage:\n    rootVolume:\n      size: string              # Root volume size\n      storageClass: string      # Storage class name\n      type: string              # Volume type: ssd, hdd, nvme\n    dataVolumes:\n    - name: string              # Volume identifier\n      size: string              # Volume size\n      storageClass: string      # Storage class\n      mountPath: string         # Mount path in VM\n\n  # Network Configuration\n  networking:\n    interfaces:\n    - name: string              # Interface name\n      networkRef:\n        name: string            # NetworkConfiguration name\n        namespace: string       # Network namespace\n      ipAddress: string         # Static IP (optional)\n      macAddress: string        # MAC address (optional)\n\n  # Operating System\n  operatingSystem:\n    type: string                # OS type: linux, windows, freebsd\n    distribution: string        # Distribution: ubuntu, centos, windows-server\n    version: string             # OS version\n    image:\n      source: string            # Image source: iso, template, cloud-image\n      url: string               # Image URL or template name\n\n  # Boot Configuration\n  boot:\n    order: []string             # Boot order: disk, network, cdrom\n    firmware:\n      type: string              # Firmware: bios, uefi\n      secureBoot: bool          # Enable secure boot\n\n  # Cloud-Init Configuration\n  cloudInit:\n    enabled: bool               # Enable cloud-init\n    userData: string            # Cloud-init user data\n    metaData: string            # Cloud-init metadata\n    networkData: string         # Cloud-init network data\n\nstatus:\n  phase: string                 # Machine lifecycle phase\n  conditions: []Condition       # Detailed conditions\n  providerStatus: {}            # Provider-specific status\n  networkStatus:\n    interfaces: []InterfaceStatus\n  addresses:\n    internal: []string          # Internal IP addresses\n    external: []string          # External IP addresses\n</code></pre>"},{"location":"reference/crd/#networkconfiguration-resource","title":"NetworkConfiguration Resource","text":"<p>Defines network interface configuration and VLAN settings:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: NetworkConfiguration\nmetadata:\n  name: string\n  namespace: string\nspec:\n  # Network Identification\n  networkId: string             # Unique network identifier\n  vlan: int                     # VLAN ID (1-4094)\n\n  # Layer 2 Configuration\n  bridge: string                # Bridge interface name\n  mtu: int                      # Maximum Transmission Unit\n\n  # IP Configuration\n  ipam:\n    type: string                # IPAM type: static, dhcp, kea\n    subnet: string              # Network subnet (CIDR)\n    gateway: string             # Default gateway\n    dns:\n      servers: []string         # DNS server addresses\n      searchDomains: []string   # DNS search domains\n\n  # DHCP Configuration (when type=kea)\n  dhcp:\n    enabled: bool               # Enable DHCP server\n    poolStart: string           # DHCP pool start address\n    poolEnd: string             # DHCP pool end address\n    leaseTime: string           # DHCP lease duration\n    reservations:\n    - macAddress: string        # MAC address for reservation\n      ipAddress: string         # Reserved IP address\n      hostname: string          # Reserved hostname\n\n  # Security Configuration\n  security:\n    isolation: bool             # Enable network isolation\n    firewallRules:\n    - direction: string         # Rule direction: ingress, egress\n      protocol: string          # Protocol: tcp, udp, icmp\n      port: string              # Port or port range\n      source: string            # Source CIDR or IP\n      destination: string       # Destination CIDR or IP\n      action: string            # Action: allow, deny\n\nstatus:\n  ready: bool                   # Network readiness status\n  allocatedIPs: []string        # Currently allocated IP addresses\n  connectedMachines: []string   # Connected machine references\n</code></pre>"},{"location":"reference/crd/#networknamespace-resource","title":"NetworkNamespace Resource","text":"<p>Defines network isolation boundaries and multi-tenancy:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: NetworkNamespace\nmetadata:\n  name: string\n  namespace: string\nspec:\n  # Isolation Configuration\n  isolation:\n    level: string               # Isolation level: strict, moderate, none\n    allowedNamespaces: []string # Allowed namespace communication\n\n  # Network Policies\n  defaultPolicy:\n    ingress: string             # Default ingress: allow, deny\n    egress: string              # Default egress: allow, deny\n\n  # Resource Limits\n  limits:\n    networks: int               # Maximum networks in namespace\n    ipAddresses: int            # Maximum IP addresses\n    bandwidth: string           # Bandwidth limit (e.g., \"1Gbps\")\n\n  # Quality of Service\n  qos:\n    class: string               # QoS class: guaranteed, burstable, best-effort\n    priority: int               # Network priority (0-255)\n\nstatus:\n  phase: string                 # Namespace phase: Active, Terminating\n  networkCount: int             # Current network count\n  ipAddressCount: int           # Current IP address usage\n</code></pre>"},{"location":"reference/crd/#provider-configuration-resources","title":"Provider Configuration Resources","text":""},{"location":"reference/crd/#machineprovider-resource","title":"MachineProvider Resource","text":"<p>Configuration for machine provisioning backends:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: MachineProvider\nmetadata:\n  name: string\n  namespace: string\nspec:\n  # Provider Type and Configuration\n  type: string                  # Provider type: proxmox, kubevirt, vmware\n\n  # Connection Configuration\n  endpoint:\n    url: string                 # Provider API URL\n    insecureSkipTLSVerify: bool # Skip TLS verification\n    timeout: string             # Connection timeout\n\n  # Authentication\n  credentials:\n    type: string                # Auth type: password, token, certificate\n    secretRef:\n      name: string              # Secret name\n      namespace: string         # Secret namespace\n\n  # Provider-Specific Configuration\n  config:\n    # Proxmox specific\n    proxmox:\n      node: string              # Default Proxmox node\n      storage: string           # Default storage pool\n      networkBridge: string     # Default network bridge\n\n    # KubeVirt specific  \n    kubevirt:\n      storageClass: string      # Default storage class\n      networkPolicy: string     # Default network policy\n\n  # Resource Templates\n  templates:\n  - name: string                # Template name\n    resources:\n      cpu: int                  # CPU cores\n      memory: string            # Memory size\n      storage: string           # Storage size\n\n  # Capacity Management\n  capacity:\n    maxMachines: int            # Maximum concurrent machines\n    reservedResources:\n      cpu: int                  # Reserved CPU cores\n      memory: string            # Reserved memory\n      storage: string           # Reserved storage\n\nstatus:\n  ready: bool                   # Provider readiness\n  capacity:\n    total: ResourceQuota        # Total available resources\n    available: ResourceQuota    # Currently available resources\n    used: ResourceQuota         # Currently used resources\n  machines: int                 # Active machine count\n</code></pre>"},{"location":"reference/crd/#kubernetesprovider-resource","title":"KubernetesProvider Resource","text":"<p>Configuration for Kubernetes cluster provisioning:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: KubernetesProvider\nmetadata:\n  name: string\n  namespace: string\nspec:\n  # Provider Configuration\n  type: string                  # Provider type: talos, kubeadm, k3s\n\n  # Cluster Configuration Template\n  clusterTemplate:\n    kubernetesVersion: string   # Kubernetes version\n    cni:\n      type: string              # CNI type: calico, flannel, cilium\n      config: {}                # CNI-specific configuration\n    controlPlane:\n      replicas: int             # Control plane node count\n      machineTemplate:          # Control plane machine template\n        resources: {}\n    workers:\n    - name: string              # Worker group name\n      replicas: int             # Worker node count\n      machineTemplate:          # Worker machine template\n        resources: {}\n\n  # Network Configuration\n  networking:\n    podCIDR: string             # Pod network CIDR\n    serviceCIDR: string         # Service network CIDR\n    dnsDomain: string           # Cluster DNS domain\n\n  # Add-ons Configuration\n  addons:\n    dns:\n      enabled: bool             # Enable CoreDNS\n      config: {}                # DNS configuration\n    ingress:\n      enabled: bool             # Enable ingress controller\n      type: string              # Ingress type: nginx, traefik, istio\n    storage:\n      enabled: bool             # Enable storage classes\n      defaultClass: string      # Default storage class\n\nstatus:\n  ready: bool                   # Provider readiness\n  supportedVersions: []string   # Supported Kubernetes versions\n  activeConlusters: int         # Active cluster count\n</code></pre>"},{"location":"reference/crd/#operator-configuration-resources","title":"Operator Configuration Resources","text":""},{"location":"reference/crd/#kubevirtconfig-resource","title":"KubevirtConfig Resource","text":"<p>Configuration for KubeVirt operator integration:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: KubevirtConfig\nmetadata:\n  name: string\n  namespace: string\nspec:\n  # KubeVirt Configuration\n  kubevirt:\n    version: string             # KubeVirt version\n    namespace: string           # KubeVirt namespace\n\n  # Feature Gates\n  featureGates:\n  - name: string                # Feature gate name\n    enabled: bool               # Feature enabled status\n\n  # Resource Configuration\n  resources:\n    vmiCPUModel: string         # Default CPU model\n    machineType: string         # Default machine type\n    emulatedMachines: []string  # Supported emulated machines\n\n  # Storage Configuration\n  storage:\n    defaultStorageClass: string # Default storage class\n    volumeModes: []string       # Supported volume modes\n    accessModes: []string       # Supported access modes\n\n  # Network Configuration\n  network:\n    defaultNetworkInterface: string # Default network interface\n    binding: {}                 # Network binding configuration\n\nstatus:\n  phase: string                 # Configuration phase\n  appliedVersion: string        # Applied KubeVirt version\n  conditions: []Condition       # Status conditions\n</code></pre>"},{"location":"reference/crd/#proxmoxconfig-resource","title":"ProxmoxConfig Resource","text":"<p>Configuration for Proxmox operator integration:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: ProxmoxConfig\nmetadata:\n  name: string\n  namespace: string\nspec:\n  # Proxmox Cluster Configuration\n  cluster:\n    nodes: []string             # Proxmox node list\n    endpoint: string            # Cluster API endpoint\n\n  # Authentication Configuration\n  authentication:\n    type: string                # Auth type: pam, pve, ad, ldap\n    realm: string               # Authentication realm\n\n  # Default Configuration\n  defaults:\n    storage: string             # Default storage pool\n    network: string             # Default network bridge\n    osType: string              # Default OS type\n\n  # Resource Limits\n  limits:\n    maxVMsPerNode: int          # Maximum VMs per node\n    maxCPUCores: int            # Maximum CPU cores per VM\n    maxMemory: string           # Maximum memory per VM\n\n  # Template Configuration\n  templates:\n  - id: string                  # Template ID\n    name: string                # Template name\n    osType: string              # Operating system type\n    resources: {}               # Default resources\n\nstatus:\n  ready: bool                   # Configuration readiness\n  clusterVersion: string        # Proxmox cluster version\n  availableNodes: []string      # Available Proxmox nodes\n</code></pre>"},{"location":"reference/crd/#data-type-specifications","title":"Data Type Specifications","text":""},{"location":"reference/crd/#common-data-types","title":"Common Data Types","text":""},{"location":"reference/crd/#resourcerequirements","title":"ResourceRequirements","text":"<pre><code>resources:\n  requests:\n    cpu: string                 # CPU request (e.g., \"100m\", \"1\")\n    memory: string              # Memory request (e.g., \"128Mi\", \"1Gi\")\n    storage: string             # Storage request (e.g., \"10Gi\")\n  limits:\n    cpu: string                 # CPU limit\n    memory: string              # Memory limit\n    storage: string             # Storage limit\n</code></pre>"},{"location":"reference/crd/#condition","title":"Condition","text":"<pre><code>conditions:\n- type: string                  # Condition type\n  status: string                # Status: True, False, Unknown\n  reason: string                # Reason code\n  message: string               # Human-readable message\n  lastTransitionTime: string    # Last transition timestamp\n  observedGeneration: int       # Observed resource generation\n</code></pre>"},{"location":"reference/crd/#secretreference","title":"SecretReference","text":"<pre><code>secretRef:\n  name: string                  # Secret name\n  namespace: string             # Secret namespace (optional)\n  key: string                   # Secret key (optional)\n</code></pre>"},{"location":"reference/crd/#objectreference","title":"ObjectReference","text":"<pre><code>objectRef:\n  apiVersion: string            # Referenced object API version\n  kind: string                  # Referenced object kind\n  name: string                  # Referenced object name\n  namespace: string             # Referenced object namespace\n  uid: string                   # Referenced object UID\n</code></pre>"},{"location":"reference/crd/#network-data-types","title":"Network Data Types","text":""},{"location":"reference/crd/#ipamconfiguration","title":"IPAMConfiguration","text":"<pre><code>ipam:\n  type: string                  # IPAM type: static, dhcp, kea, external\n  subnet: string                # Network subnet in CIDR notation\n  gateway: string               # Default gateway address\n  dns:\n    servers: []string           # DNS server addresses\n    searchDomains: []string     # DNS search domains\n  dhcp:\n    enabled: bool               # Enable DHCP\n    poolStart: string           # DHCP pool start\n    poolEnd: string             # DHCP pool end\n    leaseTime: string           # DHCP lease duration\n</code></pre>"},{"location":"reference/crd/#networkinterface","title":"NetworkInterface","text":"<pre><code>interfaces:\n- name: string                  # Interface name\n  type: string                  # Interface type: bridge, macvlan, ipvlan\n  macAddress: string            # MAC address\n  mtu: int                      # Maximum Transmission Unit\n  vlan: int                     # VLAN ID\n  ipConfiguration:\n    type: string                # IP config type: static, dhcp\n    address: string             # Static IP address\n    netmask: string             # Network mask\n    gateway: string             # Gateway address\n</code></pre>"},{"location":"reference/crd/#validation-and-constraints","title":"Validation and Constraints","text":""},{"location":"reference/crd/#resource-naming-conventions","title":"Resource Naming Conventions","text":"Field Pattern Example Description <code>metadata.name</code> <code>^[a-z0-9]([-a-z0-9]*[a-z0-9])?$</code> <code>web-server-01</code> RFC 1123 compliant <code>spec.networkId</code> <code>^[a-z0-9]([-a-z0-9]*[a-z0-9])?$</code> <code>prod-network</code> Network identifier <code>spec.vlan</code> <code>1-4094</code> <code>100</code> Valid VLAN range"},{"location":"reference/crd/#resource-constraints","title":"Resource Constraints","text":""},{"location":"reference/crd/#cpu-specifications","title":"CPU Specifications","text":"Field Minimum Maximum Default Format <code>cpu.cores</code> 1 128 1 Integer <code>cpu.threads</code> 1 2 1 Integer <code>cpu.sockets</code> 1 4 1 Integer"},{"location":"reference/crd/#memory-specifications","title":"Memory Specifications","text":"Field Minimum Maximum Format Example <code>memory.size</code> 128Mi 1024Gi Kubernetes quantity \"4Gi\", \"512Mi\""},{"location":"reference/crd/#storage-specifications","title":"Storage Specifications","text":"Field Minimum Maximum Format Example <code>storage.size</code> 1Gi 100Ti Kubernetes quantity \"20Gi\", \"1Ti\""},{"location":"reference/crd/#network-constraints","title":"Network Constraints","text":"Field Validation Description <code>subnet</code> Valid CIDR Must be valid IPv4/IPv6 CIDR <code>ipAddress</code> Valid IP Must be valid IPv4/IPv6 address <code>macAddress</code> Valid MAC Must be valid MAC address format <code>vlan</code> 1-4094 Must be in valid VLAN range"},{"location":"reference/crd/#schema-evolution-and-compatibility","title":"Schema Evolution and Compatibility","text":""},{"location":"reference/crd/#api-version-management","title":"API Version Management","text":"<p>The CRDs follow Kubernetes API versioning conventions:</p> <ul> <li>v1alpha1: Initial API version with breaking changes possible</li> <li>v1beta1: Stable API with backward compatibility (future)</li> <li>v1: Stable API with long-term compatibility (future)</li> </ul>"},{"location":"reference/crd/#conversion-strategy","title":"Conversion Strategy","text":"<p>When upgrading between API versions:</p> <pre><code># Conversion webhook configuration\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: machines.vitistack.io\nspec:\n  conversion:\n    strategy: Webhook\n    webhook:\n      service:\n        name: vitistack-conversion-webhook\n        namespace: vitistack-system\n        path: /convert\n</code></pre>"},{"location":"reference/crd/#backward-compatibility","title":"Backward Compatibility","text":"Change Type v1alpha1 v1beta1 v1 Add optional field \u2705 \u2705 \u2705 Add required field \u26a0\ufe0f \u274c \u274c Remove field \u26a0\ufe0f \u274c \u274c Rename field \u26a0\ufe0f \u274c \u274c Change field type \u26a0\ufe0f \u274c \u274c"},{"location":"reference/crd/#integration-patterns","title":"Integration Patterns","text":""},{"location":"reference/crd/#go-api-integration","title":"Go API Integration","text":"<pre><code>import (\n    v1alpha1 \"github.com/vitistack/crds/pkg/v1alpha1\"\n    \"k8s.io/apimachinery/pkg/runtime\"\n    \"sigs.k8s.io/controller-runtime/pkg/client\"\n)\n\n// Create scheme with CRD types\nscheme := runtime.NewScheme()\nv1alpha1.AddToScheme(scheme)\n\n// Create client\nclient, err := client.New(config, client.Options{\n    Scheme: scheme,\n})\n\n// Use typed resources\nmachine := &amp;v1alpha1.Machine{}\nerr = client.Get(ctx, types.NamespacedName{\n    Namespace: \"default\",\n    Name: \"my-machine\",\n}, machine)\n</code></pre>"},{"location":"reference/crd/#unstructured-conversion","title":"Unstructured Conversion","text":"<pre><code>import (\n    unstructuredutil \"github.com/vitistack/crds/pkg/unstructuredutil\"\n    v1alpha1 \"github.com/vitistack/crds/pkg/v1alpha1\"\n)\n\n// Convert typed to unstructured\nmachine := &amp;v1alpha1.Machine{/* ... */}\nunstructured, err := unstructuredutil.MachineToUnstructured(machine)\n\n// Convert unstructured to typed\ntyped, err := unstructuredutil.MachineFromUnstructured(unstructured)\n</code></pre>"},{"location":"reference/crd/#dynamic-client-usage","title":"Dynamic Client Usage","text":"<pre><code>import (\n    \"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\"\n    \"k8s.io/apimachinery/pkg/runtime/schema\"\n    \"k8s.io/client-go/dynamic\"\n)\n\n// Create dynamic client\ndynamicClient, err := dynamic.NewForConfig(config)\n\n// Define GVR\nmachineGVR := schema.GroupVersionResource{\n    Group:    \"vitistack.io\",\n    Version:  \"v1alpha1\", \n    Resource: \"machines\",\n}\n\n// Create resource\nmachine := &amp;unstructured.Unstructured{\n    Object: map[string]interface{}{\n        \"apiVersion\": \"vitistack.io/v1alpha1\",\n        \"kind\":       \"Machine\",\n        // ... spec\n    },\n}\n\nresult, err := dynamicClient.Resource(machineGVR).\n    Namespace(\"default\").\n    Create(ctx, machine, metav1.CreateOptions{})\n</code></pre>"},{"location":"reference/crd/#installation-and-management","title":"Installation and Management","text":""},{"location":"reference/crd/#crd-installation","title":"CRD Installation","text":"<pre><code># Install all CRDs\nmake install-crds\n\n# Install specific CRD\nkubectl apply -f crds/vitistack.io_machines.yaml\n\n# Verify installation\nkubectl get crd | grep vitistack.io\n</code></pre>"},{"location":"reference/crd/#crd-generation","title":"CRD Generation","text":"<pre><code># Generate CRDs from Go types\nmake manifests\n\n# Generate deep copy methods\nmake generate\n\n# Sanitize CRDs (remove unsupported integer formats)\nmake sanitize-crds\n\n# Verify sanitized CRDs\nmake verify-crds\n</code></pre>"},{"location":"reference/crd/#development-workflow","title":"Development Workflow","text":"<pre><code># Full development cycle\nmake generate     # Generate code\nmake manifests   # Generate CRDs\nmake sanitize-crds # Clean up CRDs\nmake test        # Run tests\nmake lint        # Lint code\n</code></pre> <p>This reference documentation provides comprehensive technical specifications for all Viti Stack CRDs, assuming familiarity with Kubernetes Custom Resource Definitions, API design patterns, and infrastructure as code concepts.</p>"},{"location":"reference/operators/","title":"Operators","text":""},{"location":"reference/operators/#operators","title":"Operators","text":"<p>Work in progress - information to come!</p>"},{"location":"reference/operators/ipam-operator/","title":"IPAM Operator","text":""},{"location":"reference/operators/ipam-operator/#ipam-operator","title":"IPAM Operator","text":""},{"location":"reference/operators/ipam-operator/#concept","title":"Concept","text":"<p>The IPAM-operator (IP Address Management operator) plays a critical role in managing IP addresses for services within healthcare network and internet.</p> <p>IPAM-operator depends on:</p> <ol> <li>cert-manager</li> <li>MetalLB</li> <li>IPAM-API for requesting ip-addresses</li> </ol> <p>IPAM-operator is built upon framework Kubebuilder for writing controllers and operators in a structured and standardized way.</p>"},{"location":"reference/operators/ipam-operator/#service","title":"Service","text":"<p>IPAM-operator will assign default IPAM annotations to the service object during create &amp; update process. However, you may configure those values manually by choice! IPAM annotations require that end-user set <code>.spec.type</code> to <code>LoadBalancer</code>. If kubernetes API is happy with dryRun - f.ex all required kubernetes parameters is valid - the service will be populated with all required default annotations through IPAM-operator. </p><pre><code> apiVersion: v1\n kind: Service\n metadata:\n   name: my-service\n   namespace: services\n spec:\n   ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 100\n   type: LoadBalancer\n</code></pre><p></p>"},{"location":"reference/operators/ipam-operator/#annotations","title":"Annotations","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    ipam.vitistack.io/addresses: 83.118.168.10/32\n    ipam.vitistack.io/allow-local-shared-ip: pre-shared-key-sa4OuI3e0o\n    ipam.vitistack.io/deny-external-cleanup: \"false\"\n    ipam.vitistack.io/ip-family: ipv4\n    ipam.vitistack.io/retention-period-days: \"0\"\n    ipam.vitistack.io/secret: default\n    ipam.vitistack.io/zone: inet\n</code></pre> Let\u00b4s go through the annotations, one by one: <ul> <li>ipam.vitistack.io/addresses: Metallb support only one (1) ip-address pr ip-family. IPAM-operator will fetch ip-address automatically, if not provided manually by consumer. Commas are used to separate multiple addresses.</li> <li>ipam.vitistack.io/allow-local-shared-ip: Metallb support sharing single ip-address between services within same cluster if the pre-shared-key is the same for both services with different ports. It requires that both use the <code>Cluster</code> external traffic policy, or they both point to the exact same set of pods (i.e. the pod selectors are identical).</li> <li>ipam.vitistack.io/deny-external-cleanup: IPAM-API initiate communication (one-way) with ROR to support SSE (Server Sent Events), where ROR can send an event to IPAM-API with the cluster UUID to mark all services for a specific cluster to deletion. By toggeling this setting, consumer can prevent ROR from deleting services.</li> <li>ipam.vitistack.io/ip-family: IPAM-API supports <code>ipv4</code>, <code>ipv6</code> &amp; <code>dual</code> keywords. If you would like create a service that only supports ipv6, please make sure that <code>.spec.ipFamilies</code> is set to <code>IPv6</code> during initial creation.</li> <li>ipam.vitistack.io/retention-period-days: Number of days the service should be reserved in IPAM-API before it`s deleted. You need the secret to re-call the ip-address.</li> <li>ipam.vitistack.io/secret: IPAM-operator will create a default secret in namespace ipam-system while serving the first request. The secret is noted as \"default\" in annotations. Let say you would like to create an anycast service from multiple datacenters. In that case, you need to replace the secret with a opaque secret that exists on all required locations. The opaque secret must be stored in the same namespace as the service. Note: The secret cannot be replaced if it\u00b4s used by multiple services. <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n name: my-secret\n namespace: services\ntype: Opaque\ndata:\n secret: bXktYXdlc29tZS1wYXNzd29yZA==\n</code></pre></li> <li>ipam.vitistack.io/zone: NHN supports three (3) zones for the time being:<ul> <li><code>hnet-private</code> - healthcare network - supports only ipv4-family - f.ex 10.0.0.0/8 &amp; 172.16.0.0/12</li> <li><code>hnet-public</code> - healthcare network - supports only ipv4-family - f.ex 83.118.128.0/22 &amp; 91.186.80/20<ul> <li>zone <code>hnet-public</code> should be used with caution due to limited addresses in that space.</li> </ul> </li> <li><code>inet</code> - internet - supports both ipv4- &amp; ipv6-family</li> </ul> </li> </ul>"},{"location":"reference/operators/ipam-operator/#request-body","title":"Request Body","text":"<p>IPAM-API has no authentication methods available, hence not needed. The person who holds the secret, owns the ip-address. Multiple services can consume the same ip-address as long as the secret matches. Each service will be added as a dependency to the ip-address. The ip-address will be released when dependencies no longer exists. Key <code>new_secret</code> is optional and only required when replacing existing secret! </p><pre><code>{\n    \"address\": \"83.118.168.10/32\",\n    \"ip_family\": \"ipv4\",\n    \"zone\": \"inet\",\n    \"secret\": \"SuperSecret\",\n    \"new_secret\": \"SuperSecret\",\n    \"service\": {\n        \"service_name\": \"ingress_inet\",\n        \"namespace_id\": \"467579ae-b8d5-4524-9ce8-bcb66ee02ce0\",\n        \"cluster_id\": \"0f3c7805-6b1d-4387-b8c4-b8c5d0e9b878\",\n        \"retention_period_days\": 0,\n        \"deny_external_cleanup\": false\n    },\n}\n</code></pre><p></p>"},{"location":"reference/operators/ipam-operator/#response-body","title":"Response Body","text":"<pre><code>{\n    \"message\": \"IP-address updated!\",\n    \"address\": \"83.118.168.10/32\"\n}\n</code></pre>"},{"location":"reference/operators/ipam-operator/#controller-logs","title":"Controller Logs","text":"<p>Admission Webhook has a default timeout of <code>10 seconds</code>. We have configured the timeout to <code>30 seconds</code>, which is the maximum allowed value. A \"webhook timeout exceeded\" error in Kubernetes, indicates that the Kubernetes API server couldn\u00b4t get a response from the webhook within the configured timeout period. Before drawing any conclusion, please verify the logs in IPAM-controller! </p><pre><code>$ kubectl -n ipam-system logs ipam-controller-manager-7795669cd6-qrnzb\n2025-06-23T12:27:08Z    INFO    ipam-operator   Mutation Started for Service    {\"name\": \"my-service\"}\n2025-06-23T12:27:08Z    INFO    ipam-operator   Dry run .Spec:  {\"name\": \"my-service\"}\n2025-06-23T12:27:08Z    INFO    ipam-operator   Mutation Started for Service    {\"name\": \"my-service\"}\n2025-06-23T12:27:08Z    INFO    ipam-operator   Dry run mode detected, skipping mutating for Service:   {\"name\": \"my-service\"}\n2025-06-23T12:27:08Z    INFO    ipam-operator   Validation Create Started for Service   {\"name\": \"my-service\"}\n2025-06-23T12:27:08Z    INFO    ipam-operator   Dry run mode detected, skipping validate create for Service:    {\"name\": \"my-service\"}\n2025-06-23T12:27:08Z    INFO    ipam-operator   Initialized default secret\n2025-06-23T12:27:08Z    INFO    ipam-operator   Request IPv4-address for Service:       {\"name\": \"my-service\"}\n2025-06-23T12:27:08Z    INFO    ipam-operator   Received IPv4-address for Service:      {\"name\": \"my-service\", \"address\": \"83.118.168.10\"}\n2025-06-23T12:27:08Z    INFO    ipam-operator   Mutating Completed for Service  {\"name\": \"my-service\"}\n2025-06-23T12:27:08Z    INFO    ipam-operator   Validation Create Started for Service   {\"name\": \"my-service\"}\n2025-06-23T12:27:09Z    INFO    ipam-operator   Validate IP-address succeeded!  {\"name\": \"my-service\", \"ip\": \"83.118.168.10\"}\n2025-06-23T12:27:09Z    INFO    ipam-operator   Validation Create Completed for Service {\"name\": \"my-service\"}\n</code></pre><p></p>"},{"location":"reference/operators/ipam-operator/#splunk","title":"Splunk","text":"<p>IPAM-API forwards all HTTP &amp; APP events to Splunk HEC. </p>"},{"location":"reference/operators/ipam-operator/#model-service-overview","title":"model : Service - Overview","text":""},{"location":"reference/operators/ipam-operator/#model-service-create","title":"model : Service - Create","text":""},{"location":"reference/operators/ipam-operator/#model-service-update","title":"model : Service - Update","text":""},{"location":"reference/operators/ipam-operator/#model-service-delete","title":"model : Service - Delete","text":""},{"location":"reference/operators/kea-operator/","title":"Kea Operator","text":""},{"location":"reference/operators/kea-operator/#kea-operator","title":"Kea Operator","text":"<p>Work in progress!</p> <p>The Kea Operator manages ISC Kea DHCP server reservations through Kubernetes Custom Resource Definitions. It reconciles <code>NetworkConfiguration</code> resources to ensure DHCP reservations match declared network interface configurations.</p>"},{"location":"reference/operators/kea-operator/#architecture","title":"Architecture","text":""},{"location":"reference/operators/kea-operator/#controller-structure","title":"Controller Structure","text":"<p>The operator implements the standard Kubernetes controller pattern with the following components:</p> <ul> <li>NetworkConfiguration Controller: Reconciles <code>vitistack.io/v1alpha1/NetworkConfiguration</code> resources</li> <li>Kea Service Layer: Abstracts DHCP server interactions</li> <li>HTTP Client: Manages REST API communication with Kea servers</li> <li>Unstructured Converter: Handles dynamic CRD access</li> </ul>"},{"location":"reference/operators/kea-operator/#repository-structure","title":"Repository Structure","text":"<pre><code>\u251c\u2500\u2500 cmd/                           # Main entry point\n\u251c\u2500\u2500 internal/\n\u2502   \u251c\u2500\u2500 controller/v1alpha1/       # NetworkConfiguration reconciler\n\u2502   \u251c\u2500\u2500 services/\n\u2502   \u2502   \u251c\u2500\u2500 kea/                   # DHCP management logic\n\u2502   \u2502   \u2514\u2500\u2500 initialchecks/         # Startup validation\n\u2502   \u251c\u2500\u2500 clients/                   # HTTP client wrappers\n\u2502   \u251c\u2500\u2500 settings/                  # Configuration management\n\u2502   \u2514\u2500\u2500 util/unstructuredconv/     # CRD conversion utilities\n\u251c\u2500\u2500 pkg/\n\u2502   \u251c\u2500\u2500 clients/keaclient/         # Kea REST API client\n\u2502   \u251c\u2500\u2500 interfaces/keainterface/   # Service interfaces\n\u2502   \u2514\u2500\u2500 models/keamodels/          # Data structures\n\u251c\u2500\u2500 config/                        # Kubernetes manifests\n\u2514\u2500\u2500 charts/kea-operator/          # Helm deployment\n</code></pre>"},{"location":"reference/operators/kea-operator/#api-reference","title":"API Reference","text":""},{"location":"reference/operators/kea-operator/#networkconfiguration-resource","title":"NetworkConfiguration Resource","text":"<p>The operator watches <code>NetworkConfiguration</code> resources with the following specification:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: NetworkConfiguration\nmetadata:\n  name: string\n  namespace: string\nspec:\n  clusterName: string        # Required\n  datacenterName: string     # Required  \n  supervisorName: string     # Required\n  provider: string          # Required\n  networkInterfaces:        # Required\n  - name: string           # Interface identifier\n    macAddress: string     # MAC address (normalized format)\n</code></pre>"},{"location":"reference/operators/kea-operator/#networknamespace-dependency","title":"NetworkNamespace Dependency","text":"<p>Requires corresponding <code>NetworkNamespace</code> resource with IPv4 prefix in status:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: NetworkNamespace\nmetadata:\n  name: string\nstatus:\n  ipv4Prefix: string       # CIDR notation (e.g., \"10.100.1.0/24\")\n</code></pre>"},{"location":"reference/operators/kea-operator/#configuration-reference","title":"Configuration Reference","text":""},{"location":"reference/operators/kea-operator/#environment-variables","title":"Environment Variables","text":""},{"location":"reference/operators/kea-operator/#connection-configuration","title":"Connection Configuration","text":"Variable Type Default Description <code>KEA_URL</code> string - Primary Kea server URL (preferred) <code>KEA_HOST</code> string - Alternative: Kea server hostname <code>KEA_PORT</code> int 8000 Alternative: Kea server port <code>KEA_BASE_URL</code> string - Alternative: Base URL without port <code>KEA_SECONDARY_URL</code> string - Secondary server for HA failover <code>KEA_TIMEOUT_SECONDS</code> int 10 HTTP request timeout <code>KEA_DISABLE_KEEPALIVES</code> bool false Disable HTTP keep-alive"},{"location":"reference/operators/kea-operator/#authentication-configuration","title":"Authentication Configuration","text":"<p>Basic Authentication (mutually exclusive with TLS certificates):</p> Variable Type Description <code>KEA_BASIC_AUTH_USERNAME</code> string HTTP Basic Auth username <code>KEA_BASIC_AUTH_PASSWORD</code> string HTTP Basic Auth password <p>TLS Client Certificates (mutually exclusive with basic auth):</p> Variable Type Description <code>KEA_TLS_CERT_FILE</code> path Client certificate file path <code>KEA_TLS_KEY_FILE</code> path Client private key file path <code>KEA_TLS_CA_FILE</code> path Certificate Authority file path"},{"location":"reference/operators/kea-operator/#tls-configuration","title":"TLS Configuration","text":"Variable Type Default Description <code>KEA_TLS_ENABLED</code> bool false Enable TLS encryption <code>KEA_TLS_INSECURE</code> bool false Skip certificate verification <code>KEA_TLS_SERVER_NAME</code> string - Override server name for verification"},{"location":"reference/operators/kea-operator/#operational-reference","title":"Operational Reference","text":""},{"location":"reference/operators/kea-operator/#reconciliation-logic","title":"Reconciliation Logic","text":"<p>The controller implements the following reconciliation sequence:</p> <ol> <li>Resource Validation: Validates NetworkConfiguration specification</li> <li>NetworkNamespace Lookup: Retrieves IPv4 prefix from status field</li> <li>MAC Address Extraction: Processes <code>spec.networkInterfaces[].macAddress</code></li> <li>MAC Normalization: Converts to lowercase, colon-separated format</li> <li>Subnet Resolution: Maps IPv4 prefix to Kea subnet using <code>subnet4-list</code></li> <li>Lease Discovery: Queries existing leases via <code>lease4-get-by-hw-address</code></li> <li>Reservation Management: Creates reservations using <code>reservation-add</code></li> <li>Status Updates: Reports reconciliation results in resource status</li> <li>Cleanup Handling: Removes reservations on resource deletion</li> </ol>"},{"location":"reference/operators/kea-operator/#kea-rest-api-integration","title":"Kea REST API Integration","text":""},{"location":"reference/operators/kea-operator/#command-structure","title":"Command Structure","text":"<p>All Kea commands follow this JSON structure:</p> <pre><code>{\n  \"command\": \"command-name\",\n  \"service\": [\"dhcp4\"],\n  \"arguments\": {}\n}\n</code></pre>"},{"location":"reference/operators/kea-operator/#core-api-operations","title":"Core API Operations","text":"<p>Subnet Discovery: </p><pre><code>{\n  \"command\": \"subnet4-list\",\n  \"service\": [\"dhcp4\"]\n}\n</code></pre><p></p> <p>Lease Query: </p><pre><code>{\n  \"command\": \"lease4-get-by-hw-address\", \n  \"service\": [\"dhcp4\"],\n  \"arguments\": {\n    \"hw-address\": \"aa:bb:cc:dd:ee:ff\"\n  }\n}\n</code></pre><p></p> <p>Reservation Creation: </p><pre><code>{\n  \"command\": \"reservation-add\",\n  \"service\": [\"dhcp4\"], \n  \"arguments\": {\n    \"reservation\": {\n      \"subnet-id\": 1,\n      \"hw-address\": \"aa:bb:cc:dd:ee:ff\",\n      \"ip-address\": \"10.100.1.50\"\n    }\n  }\n}\n</code></pre><p></p> <p>Reservation Removal: </p><pre><code>{\n  \"command\": \"reservation-del\",\n  \"service\": [\"dhcp4\"],\n  \"arguments\": {\n    \"subnet-id\": 1,\n    \"identifier-type\": \"hw-address\", \n    \"identifier\": \"aa:bb:cc:dd:ee:ff\"\n  }\n}\n</code></pre><p></p>"},{"location":"reference/operators/kea-operator/#response-codes","title":"Response Codes","text":"Code Meaning Action 0 Success Continue processing 1 Generic error Log error, potentially retry 2 Malformed command Fix request format 3 Unsupported command Check Kea version/configuration 4 Empty command Validate request structure"},{"location":"reference/operators/kea-operator/#processing-specifications","title":"Processing Specifications","text":""},{"location":"reference/operators/kea-operator/#mac-address-normalization","title":"MAC Address Normalization","text":"<p>Input formats automatically converted to canonical form:</p> <ul> <li><code>AA:BB:CC:DD:EE:FF</code> \u2192 <code>aa:bb:cc:dd:ee:ff</code></li> <li><code>aa-bb-cc-dd-ee-ff</code> \u2192 <code>aa:bb:cc:dd:ee:ff</code></li> <li><code>AABBCCDDEEFF</code> \u2192 Rejected (requires separators)</li> </ul> <p>Validation regex: <code>^([0-9a-f]{2}:){5}[0-9a-f]{2}$</code></p>"},{"location":"reference/operators/kea-operator/#subnet-matching-algorithm","title":"Subnet Matching Algorithm","text":"<p>The operator matches NetworkNamespace IPv4 prefixes to Kea subnets using network overlap detection:</p> <pre><code>NetworkNamespace: 10.100.1.0/24\nKea Subnet:      10.100.0.0/16\nResult:          Match (namespace subnet within Kea subnet)\n</code></pre>"},{"location":"reference/operators/kea-operator/#high-availability-behavior","title":"High Availability Behavior","text":"<p>Primary Server Available:</p> <ul> <li>All operations directed to primary URL</li> <li>Health status monitored via <code>list-commands</code></li> </ul> <p>Primary Server Failure:</p> <ul> <li>Automatic failover to secondary URL</li> <li>Operations continue without interruption</li> <li>Primary server marked unhealthy</li> </ul> <p>Recovery Behavior:</p> <ul> <li>Health checks every 30 seconds</li> <li>Automatic return to primary when available</li> <li>Reservation state synchronized between servers</li> </ul>"},{"location":"reference/operators/kea-operator/#error-handling-reference","title":"Error Handling Reference","text":""},{"location":"reference/operators/kea-operator/#error-classification","title":"Error Classification","text":"Error Type Retry Behavior Requeue Interval Network/Timeout Yes 30 seconds Authentication Yes 5 minutes Permission No Manual intervention Validation No Fix configuration Resource Yes 1 minute"},{"location":"reference/operators/kea-operator/#circuit-breaker-configuration","title":"Circuit Breaker Configuration","text":"Parameter Default Value Description Max Failures 5 Failures before opening circuit Reset Timeout 60 seconds Time before retry attempt Half-Open Limit 3 Test requests in half-open state"},{"location":"reference/operators/kea-operator/#reconciliation-backoff","title":"Reconciliation Backoff","text":"<p>Implements exponential backoff with the following parameters:</p> <ul> <li>Initial Delay: 100ms</li> <li>Backoff Factor: 2.0</li> <li>Maximum Delay: 30 seconds</li> <li>Maximum Retries: 5 attempts</li> </ul>"},{"location":"reference/operators/kea-operator/#monitoring-reference","title":"Monitoring Reference","text":""},{"location":"reference/operators/kea-operator/#prometheus-metrics","title":"Prometheus Metrics","text":"Metric Name Type Labels Description <code>kea_operator_dhcp_reservations_total</code> Counter <code>operation</code>, <code>status</code>, <code>subnet_id</code> Total DHCP operations <code>kea_operator_dhcp_operation_duration_seconds</code> Histogram <code>operation</code>, <code>server</code> Operation latency <code>kea_operator_server_health</code> Gauge <code>server</code>, <code>type</code> Server health status <code>kea_operator_active_network_configurations</code> Gauge - Active resources <code>kea_operator_reconciliation_errors_total</code> Counter <code>error_type</code>, <code>controller</code> Error counts"},{"location":"reference/operators/kea-operator/#health-endpoints","title":"Health Endpoints","text":"Endpoint Purpose Response Codes <code>/healthz</code> Liveness probe 200 (healthy), 503 (unhealthy) <code>/readyz</code> Readiness probe 200 (ready), 503 (not ready) <code>/metrics</code> Prometheus metrics 200 (metrics data)"},{"location":"reference/operators/kea-operator/#security-specifications","title":"Security Specifications","text":""},{"location":"reference/operators/kea-operator/#rbac-requirements","title":"RBAC Requirements","text":"<p>Minimum required permissions:</p> <pre><code>rules:\n- apiGroups: [\"vitistack.io\"]\n  resources: [\"networkconfigurations\"]\n  verbs: [\"get\", \"list\", \"watch\", \"update\", \"patch\"]\n- apiGroups: [\"vitistack.io\"]  \n  resources: [\"networknamespaces\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"events\"]\n  verbs: [\"create\", \"patch\"]\n</code></pre>"},{"location":"reference/operators/kea-operator/#tls-configuration_1","title":"TLS Configuration","text":"<p>Minimum TLS Version: 1.2 Supported Cipher Suites:</p> <ul> <li><code>TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384</code></li> <li><code>TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256</code> </li> <li><code>TLS_RSA_WITH_AES_256_GCM_SHA384</code></li> </ul> <p>Certificate Requirements:</p> <ul> <li>Client certificates must include Extended Key Usage for client authentication</li> <li>CA certificates must be valid and trusted</li> <li>Private keys must be RSA 2048-bit minimum or ECDSA P-256</li> </ul>"},{"location":"reference/operators/kea-operator/#deployment-reference","title":"Deployment Reference","text":""},{"location":"reference/operators/kea-operator/#helm-chart-values","title":"Helm Chart Values","text":"Parameter Default Description <code>image.repository</code> <code>ghcr.io/vitistack/kea-operator</code> Container image <code>image.tag</code> <code>latest</code> Image tag <code>replicaCount</code> 1 Number of operator pods <code>resources.limits.cpu</code> <code>100m</code> CPU limit <code>resources.limits.memory</code> <code>128Mi</code> Memory limit <code>serviceAccount.create</code> true Create service account <code>rbac.create</code> true Create RBAC resources"},{"location":"reference/operators/kea-operator/#resource-requirements","title":"Resource Requirements","text":"<p>Minimum:</p> <ul> <li>CPU: 10m</li> <li>Memory: 64Mi</li> </ul> <p>Recommended:</p> <ul> <li>CPU: 100m</li> <li>Memory: 128Mi</li> </ul>"},{"location":"reference/operators/kea-operator/#scaling-considerations","title":"Scaling Considerations","text":"<ul> <li>Single Instance: Recommended for most deployments</li> <li>High Availability: Deploy multiple replicas with leader election</li> <li>Resource Scaling: Linear scaling with NetworkConfiguration count</li> <li>Network Latency: Performance depends on Kea server response times</li> </ul>"},{"location":"reference/operators/kea-operator/#troubleshooting-reference","title":"Troubleshooting Reference","text":""},{"location":"reference/operators/kea-operator/#common-error-patterns","title":"Common Error Patterns","text":"Error Message Cause Resolution <code>no lease found for MAC</code> Device hasn't obtained DHCP lease Ensure device requests DHCP <code>subnet not found for prefix</code> NetworkNamespace prefix mismatch Verify Kea subnet configuration <code>connection refused</code> Kea server unavailable Check server status and connectivity <code>unsupported command</code> Kea version/build issue Verify REST API enabled <code>reservation already exists</code> Duplicate reservation attempt Check existing reservations"},{"location":"reference/operators/kea-operator/#debug-commands","title":"Debug Commands","text":"<p>Check Operator Status:</p> <pre><code>kubectl get pods -n kea-operator-system\nkubectl logs -n kea-operator-system deployment/kea-operator-controller-manager\n</code></pre> <p>Verify CRD Resources:</p> <pre><code>kubectl get networkconfigurations -A\nkubectl get networknamespaces -A -o jsonpath='{.items[*].status.ipv4Prefix}'\n</code></pre> <p>Test Kea Connectivity:</p> <pre><code>curl -X POST http://kea-server:8000 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"command\": \"list-commands\"}'\n</code></pre> <p>This reference documentation provides comprehensive technical details for operators and developers working with the Kea Operator, assuming familiarity with Kubernetes operators, DHCP protocols, and REST APIs.</p>"},{"location":"reference/operators/kea-operator1/","title":"Kea Operator","text":""},{"location":"reference/operators/kea-operator1/#kea-operator","title":"Kea Operator","text":"<p>Work in progress!</p> <p>The Kea Operator is a specialized Kubernetes operator that manages ISC Kea DHCP server reservations based on Viti network configurations. It automatically ensures that devices with known MAC addresses receive consistent IP addresses by creating and managing DHCP reservations in Kea DHCP servers.</p>"},{"location":"reference/operators/kea-operator1/#overview","title":"Overview","text":"<p>ISC Kea is a modern, high-performance DHCP server developed by the Internet Systems Consortium (ISC). The Kea Operator bridges the gap between Kubernetes-native network configuration and traditional DHCP infrastructure, enabling declarative management of IP address reservations through Viti CRDs.</p>"},{"location":"reference/operators/kea-operator1/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Automatic DHCP Reservations: Creates IP reservations based on NetworkConfiguration CRDs</li> <li>Lease Discovery: Discovers existing DHCP leases for devices</li> <li>Subnet Resolution: Automatically finds appropriate Kea subnets for reservations</li> <li>High Availability Support: Supports secondary Kea servers for failover scenarios</li> <li>Secure Communication: TLS support with client certificates and basic authentication</li> </ul>"},{"location":"reference/operators/kea-operator1/#architecture","title":"Architecture","text":""},{"location":"reference/operators/kea-operator1/#core-components","title":"Core Components","text":""},{"location":"reference/operators/kea-operator1/#1-controller-manager-cmd","title":"1. Controller Manager (<code>cmd/</code>)","text":"<ul> <li>NetworkConfiguration Controller: Watches for NetworkConfiguration CRD changes</li> <li>Event Processing: Handles create, update, and delete operations</li> <li>Reconciliation Engine: Ensures desired DHCP reservations match actual state</li> </ul>"},{"location":"reference/operators/kea-operator1/#2-kea-service-wrapper-internalserviceskea","title":"2. Kea Service Wrapper (<code>internal/services/kea/</code>)","text":"<ul> <li>Subnet Management: Queries Kea for available subnets</li> <li>Lease Operations: Retrieves existing leases by MAC address</li> <li>Reservation Management: Creates, updates, and removes DHCP reservations</li> </ul>"},{"location":"reference/operators/kea-operator1/#3-kea-http-client-pkgclientskeaclient","title":"3. Kea HTTP Client (<code>pkg/clients/keaclient/</code>)","text":"<ul> <li>REST API Integration: Communicates with Kea's REST API</li> <li>Authentication Support: Basic auth and mTLS client certificates</li> <li>Connection Management: Timeout handling and keep-alive configuration</li> <li>High Availability: Primary and secondary server support</li> </ul>"},{"location":"reference/operators/kea-operator1/#how-it-works","title":"How It Works","text":""},{"location":"reference/operators/kea-operator1/#1-detailed-architecture-overview","title":"1. Detailed Architecture Overview","text":"graph TB     subgraph \"Kubernetes Cluster\"         NC[NetworkConfiguration CRD]         NN[NetworkNamespace CRD]         KO[Kea Operator Pod]          subgraph \"Operator Components\"             CTL[NetworkConfiguration Controller]             KS[Kea Service Layer]             KC[Kea HTTP Client]             IC[Initial Checks Service]             UC[Unstructured Converter]         end     end      subgraph \"External Kea Infrastructure\"         KP[Primary Kea Server]         KS2[Secondary Kea Server]         DB[(Kea Lease Database)]     end      NC --&gt; CTL     NN --&gt; CTL     CTL --&gt; KS     KS --&gt; KC     KC --&gt; KP     KC -.-&gt;|Failover| KS2     KP --&gt; DB     KS2 --&gt; DB      CTL --&gt; IC     CTL --&gt; UC"},{"location":"reference/operators/kea-operator1/#2-networkconfiguration-processing-flow","title":"2. NetworkConfiguration Processing Flow","text":"sequenceDiagram     participant K8s as Kubernetes API     participant Ctrl as NetworkConfiguration Controller     participant KS as Kea Service     participant KC as Kea Client     participant Kea as Kea DHCP Server      K8s-&gt;&gt;Ctrl: Watch: NetworkConfiguration Created/Updated     Ctrl-&gt;&gt;Ctrl: Extract MAC Addresses from Spec     Ctrl-&gt;&gt;K8s: Fetch NetworkNamespace Status     K8s--&gt;&gt;Ctrl: IPv4 Prefix (e.g., 10.100.1.0/24)      Ctrl-&gt;&gt;KS: Process MAC Addresses     KS-&gt;&gt;KC: subnet4-list     KC-&gt;&gt;Kea: HTTP POST /subnet4-list     Kea--&gt;&gt;KC: Available Subnets     KC--&gt;&gt;KS: Subnet Information      loop For Each MAC Address         KS-&gt;&gt;KC: lease4-get-by-hw-address         KC-&gt;&gt;Kea: HTTP POST /lease4-get-by-hw-address         Kea--&gt;&gt;KC: Lease Information or Empty         KC--&gt;&gt;KS: Current Lease Data          alt Lease Exists             KS-&gt;&gt;KC: reservation-add             KC-&gt;&gt;Kea: HTTP POST /reservation-add             Kea--&gt;&gt;KC: Reservation Result             KC--&gt;&gt;KS: Success/Failure         else No Lease Found             KS-&gt;&gt;Ctrl: Log Warning - Device needs DHCP lease         end     end      Ctrl-&gt;&gt;K8s: Update NetworkConfiguration Status"},{"location":"reference/operators/kea-operator1/#2-dhcp-reservation-process","title":"2. DHCP Reservation Process","text":"<p>The operator follows this sequence to manage reservations:</p> <ol> <li>Watch NetworkConfiguration: Detects changes to network interface configurations</li> <li>Extract MAC Addresses: Reads MAC addresses from <code>spec.networkInterfaces[].macAddress</code></li> <li>Get Network Prefix: Retrieves IPv4 prefix from <code>NetworkNamespace.status.ipv4Prefix</code></li> <li>Resolve Subnet: Uses Kea's <code>subnet4-list</code> command to find matching subnet</li> <li>Check Current Lease: Queries existing lease with <code>lease4-get-by-hw-address</code></li> <li>Create Reservation: Uses <code>reservation-add</code> to ensure IP reservation for the MAC</li> <li>Handle Deletion: Removes reservations when NetworkConfiguration is deleted</li> </ol>"},{"location":"reference/operators/kea-operator1/#3-detailed-component-architecture","title":"3. Detailed Component Architecture","text":""},{"location":"reference/operators/kea-operator1/#controller-layer-internalcontrollerv1alpha1","title":"Controller Layer (<code>internal/controller/v1alpha1/</code>)","text":"<p>NetworkConfiguration Controller (<code>networkconfiguration_controller.go</code>):</p> <pre><code>type NetworkConfigurationReconciler struct {\n    client.Client\n    Scheme           *runtime.Scheme\n    KeaService      KeaServiceInterface\n    InitialChecks   InitialChecksInterface\n}\n\n// Reconcile implements the main reconciliation loop\nfunc (r *NetworkConfigurationReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {\n    // 1. Fetch NetworkConfiguration resource\n    // 2. Validate resource requirements\n    // 3. Extract MAC addresses from networkInterfaces\n    // 4. Get NetworkNamespace IPv4 prefix\n    // 5. Process each MAC through Kea service\n    // 6. Update resource status with results\n    // 7. Handle cleanup on deletion\n}\n</code></pre> <p>Key Controller Features:</p> <ul> <li>Event-Driven Processing: Responds to NetworkConfiguration create/update/delete events</li> <li>Finalizer Management: Ensures proper cleanup of DHCP reservations on deletion</li> <li>Status Reporting: Updates resource status with reservation results</li> <li>Error Handling: Graceful handling of Kea connectivity issues</li> <li>Concurrent Processing: Handles multiple NetworkConfiguration resources simultaneously</li> </ul>"},{"location":"reference/operators/kea-operator1/#service-layer-internalserviceskea","title":"Service Layer (<code>internal/services/kea/</code>)","text":"<p>Kea Service Implementation: </p><pre><code>type KeaService struct {\n    client      KeaClientInterface\n    logger      logr.Logger\n}\n\n// Core methods for DHCP management\nfunc (s *KeaService) ProcessMACAddresses(ctx context.Context, macs []string, ipv4Prefix string) error\nfunc (s *KeaService) CreateReservation(ctx context.Context, mac, ip string, subnetID int) error\nfunc (s *KeaService) RemoveReservation(ctx context.Context, mac string, subnetID int) error\nfunc (s *KeaService) FindSubnetForPrefix(ctx context.Context, ipv4Prefix string) (*Subnet, error)\nfunc (s *KeaService) GetLeaseForMAC(ctx context.Context, mac string) (*Lease, error)\n</code></pre><p></p> <p>Service Responsibilities:</p> <ul> <li>MAC Address Processing: Normalize and validate MAC addresses</li> <li>Subnet Resolution: Match IPv4 prefixes to Kea subnet configurations</li> <li>Lease Discovery: Query existing DHCP leases for devices</li> <li>Reservation Management: Create, update, and remove IP reservations</li> <li>Error Aggregation: Collect and report errors across multiple operations</li> </ul>"},{"location":"reference/operators/kea-operator1/#http-client-layer-pkgclientskeaclient","title":"HTTP Client Layer (<code>pkg/clients/keaclient/</code>)","text":"<p>Kea HTTP Client Implementation: </p><pre><code>type KeaClient struct {\n    httpClient     *http.Client\n    primaryURL     string\n    secondaryURL   string\n    timeout        time.Duration\n    authentication AuthConfig\n    tlsConfig     *tls.Config\n}\n\n// Core Kea API methods\nfunc (c *KeaClient) ListSubnets(ctx context.Context) ([]Subnet, error)\nfunc (c *KeaClient) GetLeaseByHWAddress(ctx context.Context, mac string) (*Lease, error)\nfunc (c *KeaClient) AddReservation(ctx context.Context, reservation *Reservation) error\nfunc (c *KeaClient) DeleteReservation(ctx context.Context, mac string, subnetID int) error\nfunc (c *KeaClient) ListCommands(ctx context.Context) ([]string, error)\n</code></pre><p></p> <p>Client Features:</p> <ul> <li>High Availability: Automatic failover between primary and secondary servers</li> <li>Authentication: Support for basic auth and mTLS client certificates</li> <li>Connection Pooling: Efficient HTTP connection reuse</li> <li>Timeout Management: Configurable request timeouts</li> <li>TLS Security: Full TLS configuration with certificate validation</li> <li>Retry Logic: Automatic retry for transient network failures</li> </ul>"},{"location":"reference/operators/kea-operator1/#4-kea-rest-api-integration-details","title":"4. Kea REST API Integration Details","text":""},{"location":"reference/operators/kea-operator1/#api-command-structure","title":"API Command Structure","text":"<p>All Kea API calls follow this JSON structure: </p><pre><code>{\n    \"command\": \"command-name\",\n    \"service\": [\"dhcp4\"],\n    \"arguments\": {\n        // Command-specific parameters\n    }\n}\n</code></pre><p></p>"},{"location":"reference/operators/kea-operator1/#core-api-operations","title":"Core API Operations","text":"<p>Subnet Discovery (<code>subnet4-list</code>): </p><pre><code>{\n    \"command\": \"subnet4-list\",\n    \"service\": [\"dhcp4\"],\n    \"arguments\": {}\n}\n</code></pre> Response provides subnet configuration including:<p></p> <ul> <li>Subnet ID numbers</li> <li>CIDR blocks (e.g., \"10.100.1.0/24\")</li> <li>Pool ranges and reservations</li> <li>Subnet-specific options</li> </ul> <p>Lease Query (<code>lease4-get-by-hw-address</code>): </p><pre><code>{\n    \"command\": \"lease4-get-by-hw-address\",\n    \"service\": [\"dhcp4\"],\n    \"arguments\": {\n        \"hw-address\": \"aa:bb:cc:dd:ee:ff\"\n    }\n}\n</code></pre> Returns current lease information:<p></p> <ul> <li>Assigned IP address</li> <li>Lease expiration time</li> <li>Subnet ID</li> <li>Client identifier</li> <li>Hardware address</li> </ul> <p>Reservation Management (<code>reservation-add</code>): </p><pre><code>{\n    \"command\": \"reservation-add\",\n    \"service\": [\"dhcp4\"],\n    \"arguments\": {\n        \"reservation\": {\n            \"subnet-id\": 1,\n            \"hw-address\": \"aa:bb:cc:dd:ee:ff\",\n            \"ip-address\": \"10.100.1.100\"\n        }\n    }\n}\n</code></pre><p></p> <p>Reservation Removal (<code>reservation-del</code>): </p><pre><code>{\n    \"command\": \"reservation-del\",\n    \"service\": [\"dhcp4\"],\n    \"arguments\": {\n        \"subnet-id\": 1,\n        \"identifier-type\": \"hw-address\",\n        \"identifier\": \"aa:bb:cc:dd:ee:ff\"\n    }\n}\n</code></pre><p></p>"},{"location":"reference/operators/kea-operator1/#error-handling-and-response-processing","title":"Error Handling and Response Processing","text":"<p>Successful Response Format: </p><pre><code>{\n    \"result\": 0,\n    \"text\": \"Operation successful\",\n    \"arguments\": {\n        // Response data\n    }\n}\n</code></pre><p></p> <p>Error Response Format: </p><pre><code>{\n    \"result\": 1,\n    \"text\": \"Error description\",\n    \"arguments\": {}\n}\n</code></pre><p></p> <p>Common Error Codes:</p> <ul> <li><code>0</code>: Success</li> <li><code>1</code>: Generic error</li> <li><code>2</code>: Malformed command</li> <li><code>3</code>: Unsupported command</li> <li><code>4</code>: Empty command</li> </ul>"},{"location":"reference/operators/kea-operator1/#configuration","title":"Configuration","text":""},{"location":"reference/operators/kea-operator1/#environment-variables","title":"Environment Variables","text":""},{"location":"reference/operators/kea-operator1/#kea-server-connection","title":"Kea Server Connection","text":"<pre><code># Primary connection (preferred method)\nKEA_URL=http://kea-server:8000\n\n# Alternative: separate host/port\nKEA_HOST=kea-server\nKEA_PORT=8000\n\n# High availability secondary server\nKEA_SECONDARY_URL=http://kea-secondary:8000\n\n# Connection settings\nKEA_TIMEOUT_SECONDS=10\nKEA_DISABLE_KEEPALIVES=false\n</code></pre>"},{"location":"reference/operators/kea-operator1/#authentication-options","title":"Authentication Options","text":"<p>Basic Authentication: </p><pre><code>KEA_BASIC_AUTH_USERNAME=dhcp-admin\nKEA_BASIC_AUTH_PASSWORD=secure-password\n</code></pre><p></p> <p>mTLS Client Certificates: </p><pre><code>KEA_TLS_CERT_FILE=/etc/certs/client.crt\nKEA_TLS_KEY_FILE=/etc/certs/client.key\nKEA_TLS_CA_FILE=/etc/certs/ca.crt\n</code></pre><p></p>"},{"location":"reference/operators/kea-operator1/#tls-configuration","title":"TLS Configuration","text":"<pre><code>KEA_TLS_ENABLED=true\nKEA_TLS_INSECURE=false\nKEA_TLS_SERVER_NAME=kea-server.example.com\n</code></pre>"},{"location":"reference/operators/kea-operator1/#usage-examples","title":"Usage Examples","text":""},{"location":"reference/operators/kea-operator1/#1-networknamespace-setup","title":"1. NetworkNamespace Setup","text":"<p>First, ensure you have a NetworkNamespace with an IPv4 prefix:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: NetworkNamespace\nmetadata:\n  name: production-network\nspec:\n  # NetworkNamespace configuration\nstatus:\n  ipv4Prefix: \"10.100.1.0/24\"  # Set by network controller\n</code></pre>"},{"location":"reference/operators/kea-operator1/#2-networkconfiguration-for-dhcp-reservations","title":"2. NetworkConfiguration for DHCP Reservations","text":"<pre><code>apiVersion: vitistack.io/v1alpha1\nkind: NetworkConfiguration\nmetadata:\n  name: web-servers\n  namespace: production-network\nspec:\n  clusterName: \"production-cluster\"\n  datacenterName: \"dc-west-1\"\n  supervisorName: \"production\"\n  provider: \"vsphere\"\n  networkInterfaces:\n    - name: \"eth0\"\n      macAddress: \"00:50:56:12:34:56\"\n    - name: \"eth1\"  \n      macAddress: \"00:50:56:78:90:ab\"\n    - name: \"eth2\"\n      macAddress: \"1a:2b:3c:4d:5e:6f\"  # Different format - normalized automatically\n</code></pre>"},{"location":"reference/operators/kea-operator1/#3-multiple-network-interfaces","title":"3. Multiple Network Interfaces","text":"<pre><code>apiVersion: vitistack.io/v1alpha1\nkind: NetworkConfiguration\nmetadata:\n  name: database-cluster\n  namespace: production-network\nspec:\n  clusterName: \"db-cluster\"\n  datacenterName: \"dc-west-1\"\n  supervisorName: \"database\"\n  provider: \"vmware\"\n  networkInterfaces:\n    - name: \"mgmt-interface\"\n      macAddress: \"00:02:12:34:56:78\"\n    - name: \"data-interface\"\n      macAddress: \"00:02:12:34:56:79\"\n    - name: \"backup-interface\"\n      macAddress: \"00:02:12:34:56:80\"\n</code></pre>"},{"location":"reference/operators/kea-operator1/#installation-and-deployment","title":"Installation and Deployment","text":""},{"location":"reference/operators/kea-operator1/#local-development-setup","title":"Local Development Setup","text":"<ol> <li>Start Local Kea Server:</li> </ol> <pre><code># Clone the repository\ngit clone https://github.com/vitistack/kea-operator.git\ncd kea-operator\n\n# Set up Docker socket permissions (macOS)\nchmod 750 ./hack/docker/sockets\n\n# Start Kea with REST API\ndocker-compose up -d\n</code></pre> <ol> <li>Configure Environment:</li> </ol> <pre><code>export KEA_URL=http://localhost:8000\nexport KEA_SECONDARY_URL=http://localhost:8001  # Optional HA setup\n</code></pre> <ol> <li>Run Operator Locally:</li> </ol> <pre><code>make run\n</code></pre>"},{"location":"reference/operators/kea-operator1/#cluster-deployment","title":"Cluster Deployment","text":"<ol> <li>Install Viti CRDs:</li> </ol> <pre><code># Install NetworkConfiguration and NetworkNamespace CRDs\nmake install\n</code></pre> <ol> <li>Deploy Operator:</li> </ol> <pre><code># Deploy to cluster\nmake deploy IMG=ghcr.io/vitistack/kea-operator:latest\n</code></pre> <ol> <li>Verify Installation:</li> </ol> <pre><code># Check operator deployment\nkubectl get deployment kea-operator-controller-manager -n kea-operator-system\n\n# Verify CRDs\nkubectl get crd | grep vitistack.io\n</code></pre>"},{"location":"reference/operators/kea-operator1/#helm-installation","title":"Helm Installation","text":"<p>The operator includes Helm charts (<code>charts/kea-operator/</code>) with:</p> <ul> <li>CRD Management: Automatic installation of required CRDs</li> <li>RBAC Setup: Proper service accounts and permissions</li> <li>ConfigMap Support: Environment variable configuration</li> <li>Secret Management: Secure handling of authentication credentials</li> </ul>"},{"location":"reference/operators/kea-operator1/#operational-features","title":"Operational Features","text":""},{"location":"reference/operators/kea-operator1/#mac-address-normalization","title":"MAC Address Normalization","text":"<p>The operator automatically normalizes MAC addresses:</p> <ul> <li>Case Insensitive: <code>AA:BB:CC:DD:EE:FF</code> \u2192 <code>aa:bb:cc:dd:ee:ff</code></li> <li>Separator Normalization: <code>aa-bb-cc-dd-ee-ff</code> \u2192 <code>aa:bb:cc:dd:ee:ff</code></li> <li>Format Validation: Ensures proper MAC address format</li> </ul>"},{"location":"reference/operators/kea-operator1/#lease-discovery-requirements","title":"Lease Discovery Requirements","text":"<ul> <li>Existing Lease Required: Devices must have already obtained a DHCP lease</li> <li>Automatic Discovery: Operator finds current IP assignment for MAC address</li> <li>Reservation Creation: Creates permanent reservation for discovered IP</li> </ul>"},{"location":"reference/operators/kea-operator1/#advanced-processing-logic","title":"Advanced Processing Logic","text":""},{"location":"reference/operators/kea-operator1/#mac-address-normalization-pipeline","title":"MAC Address Normalization Pipeline","text":"<pre><code>func NormalizeMAC(mac string) (string, error) {\n    // 1. Convert to lowercase\n    mac = strings.ToLower(mac)\n\n    // 2. Replace dashes with colons\n    mac = strings.ReplaceAll(mac, \"-\", \":\")\n\n    // 3. Validate format using regex\n    macRegex := regexp.MustCompile(`^([0-9a-f]{2}:){5}[0-9a-f]{2}$`)\n    if !macRegex.MatchString(mac) {\n        return \"\", fmt.Errorf(\"invalid MAC address format: %s\", mac)\n    }\n\n    return mac, nil\n}\n</code></pre> <p>Supported Input Formats:</p> <ul> <li><code>AA:BB:CC:DD:EE:FF</code> \u2192 <code>aa:bb:cc:dd:ee:ff</code></li> <li><code>aa-bb-cc-dd-ee-ff</code> \u2192 <code>aa:bb:cc:dd:ee:ff</code> </li> <li><code>aabbccddeeff</code> \u2192 Rejected (requires separators)</li> </ul>"},{"location":"reference/operators/kea-operator1/#subnet-matching-algorithm","title":"Subnet Matching Algorithm","text":"<pre><code>func FindMatchingSubnet(subnets []Subnet, ipv4Prefix string) (*Subnet, error) {\n    targetNetwork, err := netip.ParsePrefix(ipv4Prefix)\n    if err != nil {\n        return nil, err\n    }\n\n    for _, subnet := range subnets {\n        subnetNetwork, err := netip.ParsePrefix(subnet.Subnet)\n        if err != nil {\n            continue\n        }\n\n        // Check if networks overlap or match\n        if networksOverlap(targetNetwork, subnetNetwork) {\n            return &amp;subnet, nil\n        }\n    }\n\n    return nil, fmt.Errorf(\"no matching subnet found for prefix %s\", ipv4Prefix)\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#lease-discovery-and-validation","title":"Lease Discovery and Validation","text":"<pre><code>func (s *KeaService) ProcessMACForReservation(ctx context.Context, mac, ipv4Prefix string) error {\n    // 1. Normalize MAC address\n    normalizedMAC, err := NormalizeMAC(mac)\n    if err != nil {\n        return fmt.Errorf(\"MAC normalization failed: %w\", err)\n    }\n\n    // 2. Find appropriate subnet\n    subnet, err := s.FindSubnetForPrefix(ctx, ipv4Prefix)\n    if err != nil {\n        return fmt.Errorf(\"subnet resolution failed: %w\", err)\n    }\n\n    // 3. Query existing lease\n    lease, err := s.GetLeaseForMAC(ctx, normalizedMAC)\n    if err != nil {\n        return fmt.Errorf(\"lease query failed: %w\", err)\n    }\n\n    // 4. Validate lease is in correct subnet\n    if !s.IsIPInSubnet(lease.IPAddress, subnet.Subnet) {\n        return fmt.Errorf(\"lease IP %s not in target subnet %s\", lease.IPAddress, subnet.Subnet)\n    }\n\n    // 5. Create reservation\n    return s.CreateReservation(ctx, normalizedMAC, lease.IPAddress, subnet.ID)\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#high-availability-and-failover","title":"High Availability and Failover","text":""},{"location":"reference/operators/kea-operator1/#dual-server-configuration","title":"Dual-Server Configuration","text":"<pre><code>type KeaClient struct {\n    primaryURL     string\n    secondaryURL   string\n    currentServer  string\n    healthStatus   map[string]bool\n    failoverDelay  time.Duration\n}\n\nfunc (c *KeaClient) executeWithFailover(ctx context.Context, command KeaCommand) (*KeaResponse, error) {\n    // Try primary server first\n    if c.isHealthy(c.primaryURL) {\n        resp, err := c.executeOnServer(ctx, c.primaryURL, command)\n        if err == nil {\n            return resp, nil\n        }\n        c.markUnhealthy(c.primaryURL)\n    }\n\n    // Fallback to secondary server\n    if c.secondaryURL != \"\" &amp;&amp; c.isHealthy(c.secondaryURL) {\n        resp, err := c.executeOnServer(ctx, c.secondaryURL, command)\n        if err == nil {\n            return resp, nil\n        }\n        c.markUnhealthy(c.secondaryURL)\n    }\n\n    return nil, fmt.Errorf(\"both primary and secondary servers unavailable\")\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#health-check-implementation","title":"Health Check Implementation","text":"<pre><code>func (c *KeaClient) performHealthCheck(ctx context.Context, serverURL string) bool {\n    // Use list-commands as health check\n    command := KeaCommand{\n        Command: \"list-commands\",\n        Service: []string{\"dhcp4\"},\n    }\n\n    _, err := c.executeOnServer(ctx, serverURL, command)\n    return err == nil\n}\n\n// Periodic health monitoring\nfunc (c *KeaClient) startHealthMonitoring(ctx context.Context) {\n    ticker := time.NewTicker(30 * time.Second)\n    defer ticker.Stop()\n\n    for {\n        select {\n        case &lt;-ticker.C:\n            c.healthStatus[c.primaryURL] = c.performHealthCheck(ctx, c.primaryURL)\n            if c.secondaryURL != \"\" {\n                c.healthStatus[c.secondaryURL] = c.performHealthCheck(ctx, c.secondaryURL)\n            }\n        case &lt;-ctx.Done():\n            return\n        }\n    }\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#state-synchronization","title":"State Synchronization","text":"<ul> <li>Consistent Reservations: Ensures reservations exist on both primary and secondary servers</li> <li>Automatic Healing: Recreates missing reservations during server recovery</li> <li>Conflict Resolution: Handles cases where servers have different reservation states</li> </ul>"},{"location":"reference/operators/kea-operator1/#comprehensive-error-handling-and-recovery","title":"Comprehensive Error Handling and Recovery","text":""},{"location":"reference/operators/kea-operator1/#retry-mechanism-implementation","title":"Retry Mechanism Implementation","text":"<pre><code>type RetryConfig struct {\n    MaxAttempts     int\n    InitialDelay    time.Duration\n    BackoffFactor   float64\n    MaxDelay        time.Duration\n}\n\nfunc (c *KeaClient) executeWithRetry(ctx context.Context, command KeaCommand, config RetryConfig) (*KeaResponse, error) {\n    var lastErr error\n    delay := config.InitialDelay\n\n    for attempt := 1; attempt &lt;= config.MaxAttempts; attempt++ {\n        resp, err := c.execute(ctx, command)\n        if err == nil {\n            return resp, nil\n        }\n\n        lastErr = err\n\n        // Don't retry on certain errors\n        if isNonRetryableError(err) {\n            return nil, err\n        }\n\n        // Wait before retry (except on last attempt)\n        if attempt &lt; config.MaxAttempts {\n            select {\n            case &lt;-time.After(delay):\n                // Exponential backoff\n                delay = time.Duration(float64(delay) * config.BackoffFactor)\n                if delay &gt; config.MaxDelay {\n                    delay = config.MaxDelay\n                }\n            case &lt;-ctx.Done():\n                return nil, ctx.Err()\n            }\n        }\n    }\n\n    return nil, fmt.Errorf(\"failed after %d attempts: %w\", config.MaxAttempts, lastErr)\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#error-classification-and-handling","title":"Error Classification and Handling","text":"<pre><code>type ErrorType int\n\nconst (\n    ErrorTypeNetwork ErrorType = iota\n    ErrorTypeAuthentication\n    ErrorTypePermission  \n    ErrorTypeResource\n    ErrorTypeValidation\n    ErrorTypeTimeout\n)\n\nfunc classifyError(err error) ErrorType {\n    switch {\n    case isNetworkError(err):\n        return ErrorTypeNetwork\n    case isAuthError(err):\n        return ErrorTypeAuthentication\n    case isTimeoutError(err):\n        return ErrorTypeTimeout\n    default:\n        return ErrorTypeResource\n    }\n}\n\nfunc (r *NetworkConfigurationReconciler) handleError(ctx context.Context, err error, nc *NetworkConfiguration) (ctrl.Result, error) {\n    errorType := classifyError(err)\n\n    switch errorType {\n    case ErrorTypeNetwork, ErrorTypeTimeout:\n        // Transient errors - requeue with backoff\n        return ctrl.Result{RequeueAfter: 30 * time.Second}, nil\n\n    case ErrorTypeAuthentication:\n        // Auth errors - longer delay, may need manual intervention\n        r.recordEvent(nc, \"Warning\", \"AuthenticationFailed\", err.Error())\n        return ctrl.Result{RequeueAfter: 5 * time.Minute}, nil\n\n    case ErrorTypeValidation:\n        // Configuration errors - don't requeue automatically\n        r.recordEvent(nc, \"Warning\", \"ValidationFailed\", err.Error())\n        return ctrl.Result{}, nil\n\n    default:\n        // Unknown errors - standard requeue\n        return ctrl.Result{RequeueAfter: 1 * time.Minute}, err\n    }\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<pre><code>type CircuitBreaker struct {\n    maxFailures     int\n    resetTimeout    time.Duration\n    state          CircuitState\n    failures       int\n    lastFailTime   time.Time\n    mutex          sync.RWMutex\n}\n\ntype CircuitState int\n\nconst (\n    StateClosed CircuitState = iota\n    StateOpen\n    StateHalfOpen\n)\n\nfunc (cb *CircuitBreaker) Execute(fn func() error) error {\n    cb.mutex.Lock()\n    defer cb.mutex.Unlock()\n\n    switch cb.state {\n    case StateOpen:\n        if time.Since(cb.lastFailTime) &gt; cb.resetTimeout {\n            cb.state = StateHalfOpen\n            cb.failures = 0\n        } else {\n            return fmt.Errorf(\"circuit breaker open\")\n        }\n    }\n\n    err := fn()\n    if err != nil {\n        cb.failures++\n        cb.lastFailTime = time.Now()\n\n        if cb.failures &gt;= cb.maxFailures {\n            cb.state = StateOpen\n        }\n        return err\n    }\n\n    // Success - reset circuit breaker\n    cb.failures = 0\n    cb.state = StateClosed\n    return nil\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#comprehensive-logging-and-metrics","title":"Comprehensive Logging and Metrics","text":"<pre><code>type OperationMetrics struct {\n    TotalOperations     prometheus.Counter\n    SuccessfulOperations prometheus.Counter\n    FailedOperations    prometheus.Counter\n    OperationDuration  prometheus.Histogram\n    ActiveReservations prometheus.Gauge\n}\n\nfunc (s *KeaService) CreateReservationWithMetrics(ctx context.Context, mac, ip string, subnetID int) error {\n    timer := prometheus.NewTimer(s.metrics.OperationDuration)\n    defer timer.ObserveDuration()\n\n    s.metrics.TotalOperations.Inc()\n\n    logger := s.logger.WithValues(\n        \"operation\", \"create_reservation\",\n        \"mac\", mac,\n        \"ip\", ip,\n        \"subnet_id\", subnetID,\n    )\n\n    logger.Info(\"Creating DHCP reservation\")\n\n    err := s.createReservation(ctx, mac, ip, subnetID)\n    if err != nil {\n        s.metrics.FailedOperations.Inc()\n        logger.Error(err, \"Failed to create DHCP reservation\")\n        return err\n    }\n\n    s.metrics.SuccessfulOperations.Inc()\n    s.metrics.ActiveReservations.Inc()\n    logger.Info(\"Successfully created DHCP reservation\")\n\n    return nil\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#graceful-degradation-strategies","title":"Graceful Degradation Strategies","text":""},{"location":"reference/operators/kea-operator1/#command-availability-detection","title":"Command Availability Detection","text":"<pre><code>func (s *KeaService) detectAvailableCommands(ctx context.Context) error {\n    commands, err := s.client.ListCommands(ctx)\n    if err != nil {\n        return fmt.Errorf(\"failed to detect available commands: %w\", err)\n    }\n\n    s.availableCommands = make(map[string]bool)\n    for _, cmd := range commands {\n        s.availableCommands[cmd] = true\n    }\n\n    // Check for required commands\n    requiredCommands := []string{\"subnet4-list\", \"lease4-get-by-hw-address\", \"reservation-add\"}\n    for _, required := range requiredCommands {\n        if !s.availableCommands[required] {\n            return fmt.Errorf(\"required command not available: %s\", required)\n        }\n    }\n\n    return nil\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#fallback-mechanisms","title":"Fallback Mechanisms","text":"<pre><code>func (s *KeaService) ProcessMACWithFallback(ctx context.Context, mac string) error {\n    // Try normal processing first\n    err := s.ProcessMAC(ctx, mac)\n    if err == nil {\n        return nil\n    }\n\n    // Check if it's a command availability issue\n    if isCommandUnavailableError(err) {\n        s.logger.Info(\"Command unavailable, skipping processing\", \"mac\", mac, \"error\", err)\n        return nil // Graceful degradation\n    }\n\n    // Try alternative approach if available\n    if s.hasAlternativeMethod() {\n        return s.ProcessMACAlternative(ctx, mac)\n    }\n\n    return err\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/operators/kea-operator1/#common-issues","title":"Common Issues","text":""},{"location":"reference/operators/kea-operator1/#no-lease-found-error","title":"\"No Lease Found\" Error","text":"<pre><code># Verify device has obtained DHCP lease\ncurl -X POST http://kea-server:8000 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"command\": \"lease4-get-by-hw-address\",\n    \"arguments\": {\"hw-address\": \"00:50:56:12:34:56\"}\n  }'\n</code></pre>"},{"location":"reference/operators/kea-operator1/#unsupported-kea-command-error","title":"\"Unsupported Kea Command\" Error","text":"<ul> <li>Check Kea version and ensure REST API is enabled</li> <li>Verify required commands are available in your Kea build</li> </ul>"},{"location":"reference/operators/kea-operator1/#connection-issues","title":"Connection Issues","text":"<pre><code># Test Kea connectivity\ncurl -X POST http://kea-server:8000 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"command\": \"list-commands\"}'\n</code></pre>"},{"location":"reference/operators/kea-operator1/#debug-commands","title":"Debug Commands","text":"<pre><code># Check operator logs\nkubectl logs -n kea-operator-system deployment/kea-operator-controller-manager\n\n# Verify NetworkConfiguration status\nkubectl get networkconfigurations -A\n\n# Check NetworkNamespace IPv4 prefixes\nkubectl get networknamespaces -o jsonpath='{.items[*].status.ipv4Prefix}'\n</code></pre>"},{"location":"reference/operators/kea-operator1/#rest-api-testing","title":"REST API Testing","text":"<p>The repository includes REST API testing tools in <code>hack/rest/</code>:</p> <ul> <li>Lease queries: Test lease discovery functionality</li> <li>Reservation management: Verify reservation creation/deletion</li> <li>Subnet operations: Test subnet discovery</li> </ul>"},{"location":"reference/operators/kea-operator1/#integration-with-viti-stack","title":"Integration with Viti Stack","text":""},{"location":"reference/operators/kea-operator1/#network-management","title":"Network Management","text":"<ul> <li>IPAM Integration: Works with Viti IPAM operator for IP address management</li> <li>Network Namespaces: Integrates with NetworkNamespace for subnet organization</li> <li>Multi-Provider Support: Supports various infrastructure providers (VMware, Proxmox, etc.)</li> </ul>"},{"location":"reference/operators/kea-operator1/#declarative-configuration","title":"Declarative Configuration","text":"<ul> <li>GitOps Compatible: Network configurations stored in version control</li> <li>Kubernetes Native: Uses standard Kubernetes resource patterns</li> <li>Event-Driven: Responds to configuration changes automatically</li> </ul>"},{"location":"reference/operators/kea-operator1/#enterprise-features","title":"Enterprise Features","text":"<ul> <li>Security: TLS encryption and certificate-based authentication  </li> <li>Monitoring: Comprehensive metrics and logging for operations</li> <li>Scalability: Handles large numbers of network interfaces and reservations</li> </ul>"},{"location":"reference/operators/kea-operator1/#performance-optimization-and-monitoring","title":"Performance Optimization and Monitoring","text":""},{"location":"reference/operators/kea-operator1/#concurrency-and-scaling","title":"Concurrency and Scaling","text":""},{"location":"reference/operators/kea-operator1/#parallel-mac-processing","title":"Parallel MAC Processing","text":"<pre><code>func (s *KeaService) ProcessMACAddressesConcurrently(ctx context.Context, macs []string, ipv4Prefix string) error {\n    // Create worker pool for concurrent processing\n    const maxWorkers = 10\n    semaphore := make(chan struct{}, maxWorkers)\n\n    var wg sync.WaitGroup\n    errChan := make(chan error, len(macs))\n\n    for _, mac := range macs {\n        wg.Add(1)\n        go func(mac string) {\n            defer wg.Done()\n\n            // Acquire semaphore\n            semaphore &lt;- struct{}{}\n            defer func() { &lt;-semaphore }()\n\n            if err := s.ProcessMAC(ctx, mac, ipv4Prefix); err != nil {\n                errChan &lt;- fmt.Errorf(\"processing MAC %s: %w\", mac, err)\n            }\n        }(mac)\n    }\n\n    // Wait for all workers to complete\n    wg.Wait()\n    close(errChan)\n\n    // Collect errors\n    var errors []error\n    for err := range errChan {\n        errors = append(errors, err)\n    }\n\n    if len(errors) &gt; 0 {\n        return fmt.Errorf(\"failed to process %d MACs: %v\", len(errors), errors)\n    }\n\n    return nil\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#connection-pool-optimization","title":"Connection Pool Optimization","text":"<pre><code>func NewOptimizedKeaClient(config *ClientConfig) *KeaClient {\n    transport := &amp;http.Transport{\n        MaxIdleConns:        100,\n        MaxConnsPerHost:     10,\n        MaxIdleConnsPerHost: 10,\n        IdleConnTimeout:     30 * time.Second,\n        KeepAlive:          30 * time.Second,\n        TLSHandshakeTimeout: 10 * time.Second,\n    }\n\n    return &amp;KeaClient{\n        httpClient: &amp;http.Client{\n            Transport: transport,\n            Timeout:   config.Timeout,\n        },\n    }\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#caching-and-performance","title":"Caching and Performance","text":""},{"location":"reference/operators/kea-operator1/#subnet-cache-implementation","title":"Subnet Cache Implementation","text":"<pre><code>type SubnetCache struct {\n    cache      map[string]*CachedSubnet\n    mutex      sync.RWMutex\n    ttl        time.Duration\n}\n\ntype CachedSubnet struct {\n    subnet     *Subnet\n    cachedAt   time.Time\n}\n\nfunc (c *SubnetCache) GetSubnet(prefix string) (*Subnet, bool) {\n    c.mutex.RLock()\n    defer c.mutex.RUnlock()\n\n    cached, exists := c.cache[prefix]\n    if !exists {\n        return nil, false\n    }\n\n    // Check if cache entry is still valid\n    if time.Since(cached.cachedAt) &gt; c.ttl {\n        return nil, false\n    }\n\n    return cached.subnet, true\n}\n\nfunc (c *SubnetCache) SetSubnet(prefix string, subnet *Subnet) {\n    c.mutex.Lock()\n    defer c.mutex.Unlock()\n\n    c.cache[prefix] = &amp;CachedSubnet{\n        subnet:   subnet,\n        cachedAt: time.Now(),\n    }\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"reference/operators/kea-operator1/#prometheus-metrics","title":"Prometheus Metrics","text":"<pre><code>var (\n    dhcpReservationsTotal = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"kea_operator_dhcp_reservations_total\",\n            Help: \"Total number of DHCP reservations processed\",\n        },\n        []string{\"operation\", \"status\", \"subnet_id\"},\n    )\n\n    dhcpOperationDuration = prometheus.NewHistogramVec(\n        prometheus.HistogramOpts{\n            Name:    \"kea_operator_dhcp_operation_duration_seconds\",\n            Help:    \"Duration of DHCP operations\",\n            Buckets: prometheus.DefBuckets,\n        },\n        []string{\"operation\", \"server\"},\n    )\n\n    keaServerHealth = prometheus.NewGaugeVec(\n        prometheus.GaugeOpts{\n            Name: \"kea_operator_server_health\",\n            Help: \"Health status of Kea servers (1=healthy, 0=unhealthy)\",\n        },\n        []string{\"server\", \"type\"},\n    )\n\n    activeNetworkConfigurations = prometheus.NewGauge(\n        prometheus.GaugeOpts{\n            Name: \"kea_operator_active_network_configurations\",\n            Help: \"Number of active NetworkConfiguration resources\",\n        },\n    )\n)\n</code></pre>"},{"location":"reference/operators/kea-operator1/#health-check-endpoints","title":"Health Check Endpoints","text":"<pre><code>func (r *NetworkConfigurationReconciler) SetupHealthChecks(mgr ctrl.Manager) error {\n    // Add health check endpoint\n    mgr.GetWebhookServer().Register(\"/healthz\", &amp;webhook.Admission{\n        Handler: &amp;HealthChecker{keaService: r.KeaService},\n    })\n\n    // Add readiness check\n    mgr.GetWebhookServer().Register(\"/readyz\", &amp;webhook.Admission{\n        Handler: &amp;ReadinessChecker{keaService: r.KeaService},\n    })\n\n    return nil\n}\n\ntype HealthChecker struct {\n    keaService KeaServiceInterface\n}\n\nfunc (h *HealthChecker) Handle(ctx context.Context, req webhook.AdmissionRequest) webhook.AdmissionResponse {\n    // Check Kea connectivity\n    if err := h.keaService.HealthCheck(ctx); err != nil {\n        return webhook.Errored(http.StatusServiceUnavailable, err)\n    }\n\n    return webhook.Allowed(\"healthy\")\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#performance-tuning-guidelines","title":"Performance Tuning Guidelines","text":""},{"location":"reference/operators/kea-operator1/#reconciliation-optimization","title":"Reconciliation Optimization","text":"<pre><code>func (r *NetworkConfigurationReconciler) SetupWithManager(mgr ctrl.Manager) error {\n    return ctrl.NewControllerManagedBy(mgr).\n        For(&amp;vitinetv1alpha1.NetworkConfiguration{}).\n        WithOptions(controller.Options{\n            MaxConcurrentReconciles: 5, // Limit concurrent reconciliations\n            RateLimiter: workqueue.NewItemExponentialFailureRateLimiter(\n                100*time.Millisecond, // Base delay\n                30*time.Second,       // Max delay\n            ),\n        }).\n        Complete(r)\n}\n</code></pre>"},{"location":"reference/operators/kea-operator1/#batch-processing-optimization","title":"Batch Processing Optimization","text":"<ul> <li>MAC Grouping: Process MACs in batches to reduce API calls</li> <li>Subnet Caching: Cache subnet information to avoid repeated queries</li> <li>Connection Reuse: Maintain persistent connections to Kea servers</li> <li>Async Processing: Use goroutines for parallel reservation creation</li> </ul>"},{"location":"reference/operators/kea-operator1/#best-practices","title":"Best Practices","text":""},{"location":"reference/operators/kea-operator1/#mac-address-management","title":"MAC Address Management","text":"<ul> <li>Consistent Format: Use standardized MAC address format across configurations</li> <li>Unique Identifiers: Ensure MAC addresses are unique within network segments</li> <li>Documentation: Maintain records of MAC-to-device mappings</li> </ul>"},{"location":"reference/operators/kea-operator1/#network-design","title":"Network Design","text":"<ul> <li>Subnet Planning: Plan IP ranges to avoid conflicts between environments</li> <li>Reservation Strategy: Reserve IP ranges for static assignments vs. DHCP pools</li> <li>Security: Implement proper network segmentation and access controls</li> </ul>"},{"location":"reference/operators/kea-operator1/#operational-excellence","title":"Operational Excellence","text":"<ul> <li>Monitoring: Monitor DHCP lease utilization and reservation status</li> <li>Backup: Regular backups of Kea configuration and lease databases</li> <li>Testing: Test reservation changes in development environments first</li> </ul> <p>ISC Kea DHCP</p> <p>Kea is a modern, high-performance DHCP server that offers:</p> <ul> <li>REST API: Full programmatic control over DHCP operations</li> <li>High Availability: Built-in support for HA configurations</li> <li>Performance: Designed for high-scale enterprise environments</li> <li>Flexibility: Extensive configuration options and hooks system</li> <li>Standards Compliance: Full DHCP protocol compliance with extensions</li> </ul>"},{"location":"reference/operators/kubevirt-operator/","title":"KubeVirt Operator","text":""},{"location":"reference/operators/kubevirt-operator/#kubevirt-operator","title":"KubeVirt Operator","text":"<p>Work in progress!</p> <p>The KubeVirt Operator manages virtual machines on Kubernetes clusters by bridging Viti Stack infrastructure resources with KubeVirt virtualization capabilities. It reconciles <code>Machine</code> Custom Resource Definitions to create and manage KubeVirt <code>VirtualMachine</code> and <code>VirtualMachineInstance</code> resources, providing declarative VM lifecycle management.</p>"},{"location":"reference/operators/kubevirt-operator/#architecture","title":"Architecture","text":""},{"location":"reference/operators/kubevirt-operator/#controller-structure","title":"Controller Structure","text":"<p>The operator implements the Kubernetes controller pattern with the following components:</p> <ul> <li>Machine Controller: Reconciles <code>vitistack.io/v1alpha1/Machine</code> resources</li> <li>KubeVirt Integration: Translates Machine specs to KubeVirt VirtualMachine resources</li> <li>Network Management: Configures VM networking through NetworkConfiguration CRDs</li> <li>Storage Provisioning: Handles persistent volume claims and storage class integration</li> <li>Lifecycle Management: Manages VM creation, updates, and cleanup operations</li> </ul>"},{"location":"reference/operators/kubevirt-operator/#repository-structure","title":"Repository Structure","text":"<pre><code>\u251c\u2500\u2500 cmd/                           # Main entry point\n\u251c\u2500\u2500 controllers/v1alpha1/          # Machine controller implementation\n\u2502   \u2514\u2500\u2500 machine_controller.go     # Primary reconciliation logic\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 crd/                      # Custom Resource Definitions\n\u2502   \u251c\u2500\u2500 rbac/                     # Role-based access control\n\u2502   \u251c\u2500\u2500 manager/                  # Operator deployment\n\u2502   \u251c\u2500\u2500 prometheus/               # Monitoring configuration\n\u2502   \u251c\u2500\u2500 network-policy/           # Network security policies\n\u2502   \u2514\u2500\u2500 samples/                  # Example resources\n\u251c\u2500\u2500 charts/kubevirt-operator/     # Helm deployment charts\n\u251c\u2500\u2500 examples/                     # Machine resource examples\n\u251c\u2500\u2500 internal/                     # Internal implementation packages\n\u251c\u2500\u2500 pkg/                          # Public packages and utilities\n\u251c\u2500\u2500 test/                         # Test suites\n\u2514\u2500\u2500 docs/                         # Setup and configuration documentation\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#api-reference","title":"API Reference","text":""},{"location":"reference/operators/kubevirt-operator/#machine-resource","title":"Machine Resource","text":"<p>The primary resource managed by the KubeVirt Operator:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: Machine\nmetadata:\n  name: string                     # Machine identifier\n  namespace: string               # Kubernetes namespace\n  labels:\n    cluster.vitistack.io/cluster-name: string # Associated cluster\n    vitistack.io/machine-template: string     # Template reference\nspec:\n  # Template Configuration\n  template: string                 # Machine template name (small, medium, large)\n\n  # Resource Overrides\n  resources:\n    cpu:\n      cores: int                  # CPU cores override\n      threads: int                # CPU threads override\n      sockets: int                # CPU sockets override\n    memory:\n      size: string                # Memory size (e.g., \"2Gi\", \"4Gi\")\n\n  # Storage Configuration\n  disks:\n  - name: string                  # Disk identifier\n    size: string                  # Disk size (e.g., \"20Gi\", \"100Gi\")\n    storageClass: string          # Kubernetes StorageClass\n    accessMode: string            # Volume access mode: ReadWriteOnce, ReadWriteMany\n    volumeMode: string            # Volume mode: Filesystem, Block\n\n  # Network Configuration\n  networks:\n  - name: string                  # Network interface name\n    networkName: string           # NetworkConfiguration reference\n    model: string                 # NIC model: virtio, e1000, rtl8139\n    macAddress: string            # MAC address (optional)\n\n  # Boot Configuration\n  bootOrder: []string             # Boot device order: disk, network, cdrom\n\n  # Cloud-Init Configuration\n  cloudInit:\n    userData: string              # Cloud-init user data\n    networkData: string           # Cloud-init network configuration\n    secretRef:                    # Reference to secret containing cloud-init\n      name: string               # Secret name\n      key: string                # Secret key\n\n  # Virtual Machine Settings\n  domain:\n    machine:\n      type: string                # Machine type: pc-q35, pc-i440fx\n    features:\n      acpi: bool                  # Enable ACPI\n      apic: bool                  # Enable APIC\n      hyperv: bool                # Enable Hyper-V optimizations\n    firmware:\n      bootloader:\n        efi: bool                 # Use EFI bootloader\n        secureBoot: bool          # Enable secure boot\n\nstatus:\n  phase: string                   # Current phase: Pending, Creating, Running, Stopped, Failed\n  conditions: []Condition         # Status conditions\n  vmName: string                  # Created VirtualMachine name\n  vmiName: string                 # Active VirtualMachineInstance name\n  ipAddresses: []string           # Assigned IP addresses\n  nodeName: string                # Kubernetes node hosting the VM\n  lastUpdated: string             # Last reconciliation timestamp\n  resourceVersion: string         # Current resource version\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#machine-templates","title":"Machine Templates","text":"<p>Predefined resource configurations for common VM sizes:</p>"},{"location":"reference/operators/kubevirt-operator/#small-template","title":"Small Template","text":"<pre><code>template: small\n# Translates to:\nresources:\n  cpu:\n    cores: 1\n    threads: 1\n    sockets: 1\n  memory:\n    size: \"2Gi\"\ndisks:\n  - name: \"root\"\n    size: \"20Gi\"\n    storageClass: \"default\"\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#medium-template","title":"Medium Template","text":"<pre><code>template: medium\n# Translates to:\nresources:\n  cpu:\n    cores: 2\n    threads: 1\n    sockets: 1\n  memory:\n    size: \"4Gi\"\ndisks:\n  - name: \"root\"\n    size: \"40Gi\"\n    storageClass: \"default\"\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#large-template","title":"Large Template","text":"<pre><code>template: large\n# Translates to:\nresources:\n  cpu:\n    cores: 4\n    threads: 1\n    sockets: 1\n  memory:\n    size: \"8Gi\"\ndisks:\n  - name: \"root\"\n    size: \"80Gi\"\n    storageClass: \"default\"\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#generated-kubevirt-resources","title":"Generated KubeVirt Resources","text":"<p>The operator creates corresponding KubeVirt resources:</p>"},{"location":"reference/operators/kubevirt-operator/#virtualmachine-resource","title":"VirtualMachine Resource","text":"<pre><code>apiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: string # Generated from Machine name\n  namespace: string # Inherited from Machine\n  labels:\n    vitistack.io/managed-by: kubevirt-operator\n    vitistack.io/machine: string # Reference to source Machine\n  ownerReferences:\n    - apiVersion: vitistack.io/v1alpha1\n      kind: Machine\n      name: string # Parent Machine name\n      uid: string # Parent Machine UID\nspec:\n  running: bool # VM power state\n  template:\n    metadata:\n      labels:\n        vitistack.io/machine: string\n    spec:\n      domain:\n        cpu:\n          cores: int # From Machine resources.cpu.cores\n          threads: int # From Machine resources.cpu.threads\n          sockets: int # From Machine resources.cpu.sockets\n        memory:\n          guest: string # From Machine resources.memory.size\n        devices:\n          disks: [] # Generated from Machine disks\n          interfaces: [] # Generated from Machine networks\n          networkInterfaceMultiqueue: bool\n        machine:\n          type: string # From Machine domain.machine.type\n        features: {} # From Machine domain.features\n        firmware: {} # From Machine domain.firmware\n      networks: [] # Network configurations\n      volumes: [] # Volume configurations\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#configuration-reference","title":"Configuration Reference","text":""},{"location":"reference/operators/kubevirt-operator/#environment-variables","title":"Environment Variables","text":"Variable Type Default Description <code>KUBECONFIG</code> string - Kubernetes configuration file path <code>RECONCILE_INTERVAL</code> duration 30s Machine reconciliation interval <code>MAX_CONCURRENT_RECONCILES</code> int 5 Maximum concurrent reconciliations <code>METRICS_BIND_ADDRESS</code> string <code>:8080</code> Metrics server bind address <code>HEALTH_PROBE_BIND_ADDRESS</code> string <code>:8081</code> Health probe bind address <code>LEADER_ELECTION</code> bool true Enable leader election <code>NAMESPACE</code> string - Operator namespace"},{"location":"reference/operators/kubevirt-operator/#machine-template-configuration","title":"Machine Template Configuration","text":""},{"location":"reference/operators/kubevirt-operator/#template-definitions","title":"Template Definitions","text":"<p>Templates are hardcoded configurations that can be referenced by name:</p> Template CPU Memory Root Disk Use Case <code>small</code> 1 core 2Gi 20Gi Development, testing <code>medium</code> 2 cores 4Gi 40Gi Light workloads <code>large</code> 4 cores 8Gi 80Gi Production workloads"},{"location":"reference/operators/kubevirt-operator/#resource-override-behavior","title":"Resource Override Behavior","text":"<p>When both template and resource overrides are specified:</p> <pre><code>spec:\n  template: medium # Base: 2 cores, 4Gi memory\n  resources:\n    cpu:\n      cores: 4 # Override: Results in 4 cores\n    memory:\n      size: \"8Gi\" # Override: Results in 8Gi memory\n</code></pre> <p>Final configuration: 4 cores, 8Gi memory, 40Gi disk (from template)</p>"},{"location":"reference/operators/kubevirt-operator/#storage-configuration","title":"Storage Configuration","text":""},{"location":"reference/operators/kubevirt-operator/#storage-class-integration","title":"Storage Class Integration","text":"Parameter Type Description <code>storageClass</code> string Kubernetes StorageClass name <code>size</code> string Volume size (e.g., \"20Gi\", \"100Gi\") <code>accessMode</code> string Volume access mode <code>volumeMode</code> string Volume mode: Filesystem or Block"},{"location":"reference/operators/kubevirt-operator/#supported-access-modes","title":"Supported Access Modes","text":"Mode Description Multi-Node Use Case <code>ReadWriteOnce</code> Single node read-write No Standard VM disks <code>ReadWriteMany</code> Multi-node read-write Yes Shared storage <code>ReadOnlyMany</code> Multi-node read-only Yes Read-only data"},{"location":"reference/operators/kubevirt-operator/#network-configuration","title":"Network Configuration","text":""},{"location":"reference/operators/kubevirt-operator/#networkconfiguration-crd-integration","title":"NetworkConfiguration CRD Integration","text":"<p>The operator integrates with Viti Stack NetworkConfiguration resources:</p> <pre><code>networks:\n  - name: \"eth0\"\n    networkName: \"prod-network\" # References NetworkConfiguration\n    model: \"virtio\"\n    macAddress: \"52:54:00:12:34:56\"\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#network-interface-models","title":"Network Interface Models","text":"Model Description Performance Compatibility <code>virtio</code> Paravirtualized NIC High Modern OS <code>e1000</code> Intel E1000 emulation Medium Legacy OS <code>rtl8139</code> Realtek RTL8139 Low Very old OS"},{"location":"reference/operators/kubevirt-operator/#operational-reference","title":"Operational Reference","text":""},{"location":"reference/operators/kubevirt-operator/#reconciliation-workflow","title":"Reconciliation Workflow","text":"<p>The Machine controller implements the following reconciliation logic:</p> <ol> <li>Resource Validation: Validates Machine specification and template references</li> <li>Template Resolution: Applies machine template and processes overrides</li> <li>Network Preparation: Ensures NetworkConfiguration resources exist</li> <li>Storage Provisioning: Creates PersistentVolumeClaims for disks</li> <li>VirtualMachine Creation: Generates KubeVirt VirtualMachine resource</li> <li>Status Monitoring: Watches VirtualMachineInstance status</li> <li>Network Configuration: Applies network settings and IP assignments</li> <li>Cleanup Management: Handles resource deletion and finalizers</li> </ol>"},{"location":"reference/operators/kubevirt-operator/#machine-lifecycle-states","title":"Machine Lifecycle States","text":"Phase Description Next States <code>Pending</code> Machine created, awaiting reconciliation Creating, Failed <code>Creating</code> Resources being provisioned Running, Failed <code>Running</code> VM successfully running Stopped, Failed <code>Stopped</code> VM powered off Running, Failed <code>Failed</code> Unrecoverable error -"},{"location":"reference/operators/kubevirt-operator/#kubevirt-resource-mapping","title":"KubeVirt Resource Mapping","text":""},{"location":"reference/operators/kubevirt-operator/#cpu-configuration-mapping","title":"CPU Configuration Mapping","text":"Machine Spec KubeVirt VirtualMachine Description <code>resources.cpu.cores: 2</code> <code>domain.cpu.cores: 2</code> Total CPU cores <code>resources.cpu.threads: 1</code> <code>domain.cpu.threads: 1</code> Threads per core <code>resources.cpu.sockets: 1</code> <code>domain.cpu.sockets: 1</code> CPU sockets"},{"location":"reference/operators/kubevirt-operator/#memory-configuration-mapping","title":"Memory Configuration Mapping","text":"Machine Spec KubeVirt VirtualMachine Description <code>resources.memory.size: \"4Gi\"</code> <code>domain.memory.guest: \"4Gi\"</code> Guest memory allocation"},{"location":"reference/operators/kubevirt-operator/#disk-configuration-mapping","title":"Disk Configuration Mapping","text":"<pre><code># Machine specification\ndisks:\n- name: \"root\"\n  size: \"20Gi\"\n  storageClass: \"fast-ssd\"\n  accessMode: \"ReadWriteOnce\"\n\n# Generated PersistentVolumeClaim\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: \"{machine-name}-root\"\nspec:\n  storageClassName: \"fast-ssd\"\n  accessModes: [\"ReadWriteOnce\"]\n  resources:\n    requests:\n      storage: \"20Gi\"\n\n# Generated VirtualMachine disk reference\nvolumes:\n- name: \"root\"\n  persistentVolumeClaim:\n    claimName: \"{machine-name}-root\"\ndisks:\n- name: \"root\"\n  disk:\n    bus: \"virtio\"\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#processing-specifications","title":"Processing Specifications","text":""},{"location":"reference/operators/kubevirt-operator/#template-processing-algorithm","title":"Template Processing Algorithm","text":"<pre><code>func ProcessMachineSpec(machine *Machine) *ProcessedSpec {\n    spec := &amp;ProcessedSpec{}\n\n    // 1. Apply base template\n    if template := GetTemplate(machine.Spec.Template); template != nil {\n        spec.CPU = template.CPU\n        spec.Memory = template.Memory\n        spec.Disks = template.Disks\n    }\n\n    // 2. Apply resource overrides\n    if machine.Spec.Resources.CPU != nil {\n        spec.CPU = machine.Spec.Resources.CPU\n    }\n    if machine.Spec.Resources.Memory != nil {\n        spec.Memory = machine.Spec.Resources.Memory\n    }\n\n    // 3. Merge disk configurations\n    if len(machine.Spec.Disks) &gt; 0 {\n        spec.Disks = mergeDiskConfigs(spec.Disks, machine.Spec.Disks)\n    }\n\n    return spec\n}\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#network-configuration-processing","title":"Network Configuration Processing","text":"<pre><code>func ProcessNetworkConfiguration(machine *Machine) []NetworkConfig {\n    var configs []NetworkConfig\n\n    for _, network := range machine.Spec.Networks {\n        // Lookup NetworkConfiguration CRD\n        netConfig := GetNetworkConfiguration(network.NetworkName)\n\n        config := NetworkConfig{\n            Name:       network.Name,\n            Model:      network.Model,\n            MacAddress: network.MacAddress,\n            VLAN:       netConfig.Spec.VLAN,\n            Bridge:     netConfig.Spec.Bridge,\n        }\n        configs = append(configs, config)\n    }\n\n    return configs\n}\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#cloud-init-processing","title":"Cloud-Init Processing","text":"<pre><code># Machine specification with cloud-init\nspec:\n  cloudInit:\n    userData: |\n      #cloud-config\n      users:\n      - name: admin\n        sudo: ALL=(ALL) NOPASSWD:ALL\n        ssh_authorized_keys:\n        - ssh-rsa AAAAB3...\n    secretRef:\n      name: \"machine-secrets\"\n      key: \"userdata\"\n\n# Generated VirtualMachine volume\nvolumes:\n  - name: \"cloudinitdisk\"\n    cloudInitNoCloud:\n      userData: |\n        #cloud-config\n        users: ...\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#error-handling-reference","title":"Error Handling Reference","text":""},{"location":"reference/operators/kubevirt-operator/#reconciliation-error-types","title":"Reconciliation Error Types","text":"Error Type Condition Recovery Action <code>TemplateNotFound</code> Invalid template reference Fix template name in Machine spec <code>NetworkConfigurationNotFound</code> Missing NetworkConfiguration CRD Create required NetworkConfiguration <code>StorageClassNotFound</code> Invalid StorageClass Update storageClass or create StorageClass <code>InsufficientResources</code> Node resource exhaustion Scale cluster or reduce resource requests <code>KubeVirtApiError</code> KubeVirt API failure Check KubeVirt installation and permissions"},{"location":"reference/operators/kubevirt-operator/#status-conditions","title":"Status Conditions","text":"Condition Type Status Reason Description <code>Ready</code> True/False Various Overall machine readiness <code>VirtualMachineReady</code> True/False <code>VMCreated</code>/<code>VMFailed</code> VirtualMachine resource status <code>StorageReady</code> True/False <code>PVCBound</code>/<code>PVCPending</code> Storage provisioning status <code>NetworkReady</code> True/False <code>NetworkConfigured</code>/<code>NetworkFailed</code> Network configuration status"},{"location":"reference/operators/kubevirt-operator/#finalizer-management","title":"Finalizer Management","text":"<p>The operator uses finalizers for proper cleanup:</p> <pre><code>metadata:\n  finalizers:\n    - machine.vitistack.io/cleanup\n</code></pre> <p>Cleanup Process:</p> <ol> <li>Delete VirtualMachine and VirtualMachineInstance</li> <li>Delete PersistentVolumeClaims</li> <li>Clean up NetworkConfiguration references</li> <li>Remove finalizer</li> </ol>"},{"location":"reference/operators/kubevirt-operator/#monitoring-reference","title":"Monitoring Reference","text":""},{"location":"reference/operators/kubevirt-operator/#prometheus-metrics","title":"Prometheus Metrics","text":"Metric Name Type Labels Description <code>kubevirt_operator_machines_total</code> Gauge <code>phase</code>, <code>template</code> Total machines by phase <code>kubevirt_operator_reconciliation_duration_seconds</code> Histogram <code>controller</code> Reconciliation duration <code>kubevirt_operator_reconciliation_errors_total</code> Counter <code>controller</code>, <code>error_type</code> Reconciliation errors <code>kubevirt_operator_virtual_machines_total</code> Gauge <code>status</code> Created VirtualMachine resources <code>kubevirt_operator_storage_provisioning_duration_seconds</code> Histogram <code>storage_class</code> Storage provisioning time <code>kubevirt_operator_network_configuration_errors_total</code> Counter <code>network_name</code> Network configuration errors"},{"location":"reference/operators/kubevirt-operator/#health-endpoints","title":"Health Endpoints","text":"Endpoint Purpose Status Codes <code>/healthz</code> Liveness probe 200 (healthy), 500 (unhealthy) <code>/readyz</code> Readiness probe 200 (ready), 500 (not ready) <code>/metrics</code> Prometheus metrics 200 (metrics available)"},{"location":"reference/operators/kubevirt-operator/#logging-reference","title":"Logging Reference","text":""},{"location":"reference/operators/kubevirt-operator/#structured-logging-fields","title":"Structured Logging Fields","text":"<pre><code>{\n  \"timestamp\": \"2024-01-01T12:00:00Z\",\n  \"level\": \"info\",\n  \"controller\": \"Machine\",\n  \"machine\": \"test-vm\",\n  \"namespace\": \"default\",\n  \"phase\": \"Creating\",\n  \"message\": \"Creating VirtualMachine resource\"\n}\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#security-reference","title":"Security Reference","text":""},{"location":"reference/operators/kubevirt-operator/#rbac-requirements","title":"RBAC Requirements","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: kubevirt-operator\nrules:\n  - apiGroups: [\"vitistack.io\"]\n    resources: [\"machines\", \"networkconfigurations\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n  - apiGroups: [\"kubevirt.io\"]\n    resources: [\"virtualmachines\", \"virtualmachineinstances\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n  - apiGroups: [\"\"]\n    resources: [\"persistentvolumeclaims\", \"secrets\", \"configmaps\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n  - apiGroups: [\"\"]\n    resources: [\"events\"]\n    verbs: [\"create\", \"patch\"]\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#service-account-configuration","title":"Service Account Configuration","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: kubevirt-operator-controller-manager\n  namespace: kubevirt-operator-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: kubevirt-operator-manager-rolebinding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kubevirt-operator\nsubjects:\n  - kind: ServiceAccount\n    name: kubevirt-operator-controller-manager\n    namespace: kubevirt-operator-system\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#deployment-reference","title":"Deployment Reference","text":""},{"location":"reference/operators/kubevirt-operator/#helm-chart-configuration","title":"Helm Chart Configuration","text":"Parameter Default Description <code>image.repository</code> <code>ghcr.io/vitistack/kubevirt-operator</code> Container image <code>image.tag</code> Chart version Image tag <code>image.pullPolicy</code> <code>IfNotPresent</code> Image pull policy <code>replicaCount</code> 1 Operator replicas <code>resources.limits.cpu</code> <code>500m</code> CPU limit <code>resources.limits.memory</code> <code>512Mi</code> Memory limit <code>resources.requests.cpu</code> <code>100m</code> CPU request <code>resources.requests.memory</code> <code>256Mi</code> Memory request <code>nodeSelector</code> <code>{}</code> Node selection constraints <code>tolerations</code> <code>[]</code> Pod tolerations <code>affinity</code> <code>{}</code> Pod affinity rules"},{"location":"reference/operators/kubevirt-operator/#prerequisites","title":"Prerequisites","text":""},{"location":"reference/operators/kubevirt-operator/#kubevirt-installation","title":"KubeVirt Installation","text":"<p>The operator requires KubeVirt to be installed in the cluster:</p> <pre><code># Install KubeVirt operator\nkubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/v0.59.0/kubevirt-operator.yaml\n\n# Create KubeVirt custom resource\nkubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/v0.59.0/kubevirt-cr.yaml\n\n# Verify installation\nkubectl get pods -n kubevirt\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#storage-requirements","title":"Storage Requirements","text":"<ul> <li>At least one StorageClass with dynamic provisioning</li> <li>RWO (ReadWriteOnce) access mode support</li> <li>Sufficient storage capacity for VM disks</li> </ul>"},{"location":"reference/operators/kubevirt-operator/#installation-methods","title":"Installation Methods","text":""},{"location":"reference/operators/kubevirt-operator/#helm-installation","title":"Helm Installation","text":"<pre><code># Add Helm repository\nhelm repo add vitistack oci://ghcr.io/vitistack/helm\n\n# Install operator\nhelm install kubevirt-operator vitistack/kubevirt-operator \\\n  --namespace vitistack \\\n  --create-namespace\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#upgrade-to-latest-version","title":"Upgrade to latest version","text":"<pre><code>helm install kubevirt-operator vitistack/kubevirt-operator \\\n  --namespace vitistack \\\n  --create-namespace \\\n  --reuse-values\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#manual-installation","title":"Manual Installation","text":"<pre><code># Apply CRDs and operator\nkubectl apply -f config/crd/\nkubectl apply -f config/rbac/\nkubectl apply -f config/manager/\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#example-configurations","title":"Example Configurations","text":""},{"location":"reference/operators/kubevirt-operator/#basic-virtual-machine","title":"Basic Virtual Machine","text":"<pre><code>apiVersion: vitistack.io/v1alpha1\nkind: Machine\nmetadata:\n  name: basic-vm\n  namespace: default\nspec:\n  template: medium\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#virtual-machine-with-overrides","title":"Virtual Machine with Overrides","text":"<pre><code>apiVersion: vitistack.io/v1alpha1\nkind: Machine\nmetadata:\n  name: custom-vm\n  namespace: default\nspec:\n  template: small\n  resources:\n    cpu:\n      cores: 4\n    memory:\n      size: \"8Gi\"\n  disks:\n    - name: \"data\"\n      size: \"100Gi\"\n      storageClass: \"fast-ssd\"\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#virtual-machine-with-networking","title":"Virtual Machine with Networking","text":"<pre><code>apiVersion: vitistack.io/v1alpha1\nkind: Machine\nmetadata:\n  name: networked-vm\n  namespace: default\nspec:\n  template: medium\n  networks:\n    - name: \"eth0\"\n      networkName: \"prod-network\"\n      model: \"virtio\"\n    - name: \"eth1\"\n      networkName: \"storage-network\"\n      model: \"virtio\"\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#virtual-machine-with-cloud-init","title":"Virtual Machine with Cloud-Init","text":"<pre><code>apiVersion: vitistack.io/v1alpha1\nkind: Machine\nmetadata:\n  name: cloud-init-vm\n  namespace: default\nspec:\n  template: large\n  cloudInit:\n    userData: |\n      #cloud-config\n      users:\n      - name: admin\n        sudo: ALL=(ALL) NOPASSWD:ALL\n        ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB...\n      packages:\n      - curl\n      - vim\n      runcmd:\n      - systemctl enable docker\n      - systemctl start docker\n</code></pre>"},{"location":"reference/operators/kubevirt-operator/#troubleshooting-reference","title":"Troubleshooting Reference","text":""},{"location":"reference/operators/kubevirt-operator/#common-issues","title":"Common Issues","text":"Issue Symptom Resolution VM not starting Machine stuck in Creating phase Check KubeVirt installation and node resources Storage provisioning failure PVC in Pending state Verify StorageClass exists and has available capacity Network configuration error VM created but no network access Check NetworkConfiguration CRD and multus installation Template not found Machine validation error Use valid template name: small, medium, or large"},{"location":"reference/operators/kubevirt-operator/#debug-commands","title":"Debug Commands","text":"<p>Check Machine Status:</p> <pre><code>kubectl get machines -A\nkubectl describe machine &lt;machine-name&gt;\n</code></pre> <p>Check Generated Resources:</p> <pre><code>kubectl get vm,vmi,pvc -l vitistack.io/machine=&lt;machine-name&gt;\n</code></pre> <p>View Operator Logs:</p> <pre><code>kubectl logs -n kubevirt-operator-system deployment/kubevirt-operator-controller-manager -f\n</code></pre> <p>Check KubeVirt Status:</p> <pre><code>kubectl get pods -n kubevirt\nkubectl get vmi -A\n</code></pre> <p>This reference documentation provides comprehensive technical details for system administrators and developers working with the KubeVirt Operator, assuming familiarity with Kubernetes, KubeVirt, and virtualization concepts.</p>"},{"location":"reference/operators/physical-operator/","title":"Physical Operator","text":""},{"location":"reference/operators/physical-operator/#physical-operator","title":"Physical Operator","text":"<p>Work in progress - information to come!</p>"},{"location":"reference/operators/proxmox-operator/","title":"Proxmox Operator","text":""},{"location":"reference/operators/proxmox-operator/#proxmox-operator","title":"Proxmox Operator","text":"<p>Work in progress!</p> <p>The Proxmox Operator manages Proxmox Virtual Environment (PVE) resources through Kubernetes Custom Resource Definitions. It provides declarative infrastructure management by reconciling Kubernetes resources with Proxmox clusters, nodes, and virtual machines.</p>"},{"location":"reference/operators/proxmox-operator/#architecture","title":"Architecture","text":""},{"location":"reference/operators/proxmox-operator/#controller-structure","title":"Controller Structure","text":"<p>The operator implements the Kubernetes controller pattern with the following components:</p> <ul> <li>Machine Controller: Reconciles <code>vitistack.io/v1alpha1/Machine</code> resources</li> <li>ProxmoxCluster Controller: Manages Proxmox cluster connections</li> <li>ProxmoxNode Controller: Handles individual node management</li> <li>Initialize Service: Validates Proxmox connectivity and authentication</li> </ul>"},{"location":"reference/operators/proxmox-operator/#repository-structure","title":"Repository Structure","text":"<pre><code>\u251c\u2500\u2500 cmd/                           # Main entry point\n\u251c\u2500\u2500 internal/\n\u2502   \u251c\u2500\u2500 controller/v1alpha1/       # Custom resource controllers\n\u2502   \u2514\u2500\u2500 services/initializeservice/ # Proxmox initialization logic\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 crd/                       # Custom Resource Definitions\n\u2502   \u251c\u2500\u2500 rbac/                      # Role-based access control\n\u2502   \u251c\u2500\u2500 manager/                   # Operator deployment\n\u2502   \u251c\u2500\u2500 prometheus/                # Monitoring configuration\n\u2502   \u251c\u2500\u2500 network-policy/            # Network security policies\n\u2502   \u2514\u2500\u2500 samples/                   # Example resources\n\u251c\u2500\u2500 charts/proxmox-operator/       # Helm deployment charts\n\u2514\u2500\u2500 test/                          # Test suites\n</code></pre>"},{"location":"reference/operators/proxmox-operator/#api-reference","title":"API Reference","text":""},{"location":"reference/operators/proxmox-operator/#machine-resource","title":"Machine Resource","text":"<p>The primary resource managed by the Proxmox Operator:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: Machine\nmetadata:\n  name: string                     # Machine identifier\n  namespace: string               # Kubernetes namespace\n  labels:\n    cluster.vitistack.io/cluster-name: string # Associated cluster\nspec:\n  # Core Configuration\n  name: string                    # VM name in Proxmox\n  vmid: int                      # Proxmox VM ID (100-999999999)\n  node: string                   # Target Proxmox node\n  template: string               # Base template/image\n\n  # Resource Allocation\n  cpu:\n    cores: int                   # CPU cores (1-128)\n    sockets: int                 # CPU sockets (1-4)\n    threadsPerCore: int          # Threads per core (1-2)\n  memory: int                    # RAM in bytes\n\n  # Storage Configuration\n  disks:\n  - name: string                 # Disk identifier\n    size: string                 # Size (e.g., \"20G\", \"1T\")\n    storage: string              # Storage pool name\n    type: string                 # Disk type: ide, sata, scsi, virtio\n    cache: string                # Cache mode: none, writethrough, writeback\n    format: string               # Format: raw, qcow2, vmdk\n\n  # Network Configuration\n  networks:\n  - name: string                 # Network interface name\n    bridge: string               # Proxmox bridge (vmbr0, vmbr1, etc.)\n    model: string                # NIC model: e1000, virtio, rtl8139\n    macAddress: string           # MAC address (optional)\n    vlan: int                    # VLAN tag (1-4094)\n    firewall: bool               # Enable Proxmox firewall\n\n  # Proxmox-Specific Settings\n  osType: string                 # OS type: linux, windows, solaris, other\n  bootOrder: []string            # Boot device order: disk, network, cdrom\n  agent: bool                    # Enable QEMU guest agent\n  balloon: bool                  # Enable memory ballooning\n  protection: bool               # Enable deletion protection\n\nstatus:\n  phase: string                  # Current phase: Pending, Creating, Running, Stopped, Error\n  conditions: []Condition        # Status conditions\n  vmid: int                     # Assigned VM ID\n  node: string                  # Actual Proxmox node\n  ipAddresses: []string         # Assigned IP addresses\n  lastUpdated: string           # Last reconciliation timestamp\n</code></pre>"},{"location":"reference/operators/proxmox-operator/#proxmoxcluster-resource","title":"ProxmoxCluster Resource","text":"<p>Represents a Proxmox cluster configuration:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: ProxmoxCluster\nmetadata:\n  name: string\nspec:\n  # Connection Configuration\n  endpoint: string               # Proxmox API URL (https://proxmox.example.com:8006)\n  insecureSkipTLSVerify: bool   # Skip TLS certificate verification\n\n  # Authentication\n  credentials:\n    username: string             # Proxmox username (user@pam, user@pve)\n    password:                   # Password reference\n      secretRef:\n        name: string            # Secret name\n        key: string             # Secret key\n    tokenID: string             # API token ID (alternative to password)\n    tokenSecret:                # API token secret\n      secretRef:\n        name: string\n        key: string\n\n  # Cluster Settings\n  nodes: []string               # Available Proxmox nodes\n  storages: []string            # Available storage pools\n  networks: []string            # Available network bridges\n\nstatus:\n  ready: bool                   # Cluster connectivity status\n  version: string               # Proxmox VE version\n  nodes: []NodeStatus           # Per-node status information\n  lastHealthCheck: string       # Last health check timestamp\n</code></pre>"},{"location":"reference/operators/proxmox-operator/#proxmoxnode-resource","title":"ProxmoxNode Resource","text":"<p>Represents individual Proxmox nodes:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: ProxmoxNode\nmetadata:\n  name: string\nspec:\n  cluster: string               # Reference to ProxmoxCluster\n  nodeName: string             # Proxmox node name\n  maxVMs: int                  # Maximum VMs per node\n\nstatus:\n  ready: bool                  # Node availability\n  resources:\n    cpu:\n      total: int               # Total CPU cores\n      used: int                # Used CPU cores\n    memory:\n      total: int               # Total memory in bytes\n      used: int                # Used memory in bytes\n    storage:\n      total: int               # Total storage in bytes\n      used: int                # Used storage in bytes\n  vmCount: int                 # Current VM count\n</code></pre>"},{"location":"reference/operators/proxmox-operator/#configuration-reference","title":"Configuration Reference","text":""},{"location":"reference/operators/proxmox-operator/#environment-variables","title":"Environment Variables","text":"Variable Type Default Description <code>PROXMOX_DEFAULT_CLUSTER</code> string - Default cluster for machines without explicit cluster <code>RECONCILE_INTERVAL</code> duration 30s Reconciliation interval <code>MAX_CONCURRENT_RECONCILES</code> int 5 Maximum concurrent reconciliations <code>METRICS_BIND_ADDRESS</code> string <code>:8080</code> Metrics server bind address <code>HEALTH_PROBE_BIND_ADDRESS</code> string <code>:8081</code> Health probe bind address"},{"location":"reference/operators/proxmox-operator/#proxmox-api-configuration","title":"Proxmox API Configuration","text":""},{"location":"reference/operators/proxmox-operator/#authentication-methods","title":"Authentication Methods","text":"<p>Username/Password Authentication:</p> <pre><code>credentials:\n  username: \"operator@pve\"\n  password:\n    secretRef:\n      name: \"proxmox-credentials\"\n      key: \"password\"\n</code></pre> <p>API Token Authentication (Recommended):</p> <pre><code>credentials:\n  username: \"operator@pve\"\n  tokenID: \"operator-token\"\n  tokenSecret:\n    secretRef:\n      name: \"proxmox-credentials\" \n      key: \"token-secret\"\n</code></pre>"},{"location":"reference/operators/proxmox-operator/#tls-configuration","title":"TLS Configuration","text":"Parameter Type Default Description <code>insecureSkipTLSVerify</code> bool false Skip certificate validation <code>caCertificate</code> string - Custom CA certificate <code>clientCertificate</code> string - Client certificate for mutual TLS <code>clientKey</code> string - Client private key"},{"location":"reference/operators/proxmox-operator/#operational-reference","title":"Operational Reference","text":""},{"location":"reference/operators/proxmox-operator/#reconciliation-logic","title":"Reconciliation Logic","text":"<p>The Proxmox Operator implements the following reconciliation workflow:</p> <ol> <li>Resource Validation: Validates Machine specification against Proxmox constraints</li> <li>Cluster Connection: Establishes connection to target Proxmox cluster</li> <li>Node Selection: Selects appropriate Proxmox node based on resources and constraints</li> <li>VM ID Assignment: Allocates unique VM ID if not specified</li> <li>VM Creation: Creates virtual machine with specified configuration</li> <li>Resource Provisioning: Configures CPU, memory, storage, and network</li> <li>State Monitoring: Monitors VM state and reports status</li> <li>Lifecycle Management: Handles start, stop, restart, and deletion operations</li> </ol>"},{"location":"reference/operators/proxmox-operator/#proxmox-api-integration","title":"Proxmox API Integration","text":""},{"location":"reference/operators/proxmox-operator/#vm-management-operations","title":"VM Management Operations","text":"<p>VM Creation:</p> <pre><code>POST /api2/json/nodes/{node}/qemu\nContent-Type: application/json\n\n{\n  \"vmid\": 100,\n  \"name\": \"test-vm\",\n  \"cores\": 2,\n  \"memory\": 2048,\n  \"net0\": \"virtio,bridge=vmbr0\",\n  \"scsi0\": \"local-lvm:20\"\n}\n</code></pre> <p>VM Configuration Update:</p> <pre><code>PUT /api2/json/nodes/{node}/qemu/{vmid}/config\nContent-Type: application/json\n\n{\n  \"cores\": 4,\n  \"memory\": 4096\n}\n</code></pre> <p>VM State Control:</p> <pre><code>POST /api2/json/nodes/{node}/qemu/{vmid}/status/{action}\n# Actions: start, stop, shutdown, reset, suspend, resume\n</code></pre> <p>VM Status Query:</p> <pre><code>GET /api2/json/nodes/{node}/qemu/{vmid}/status/current\n</code></pre>"},{"location":"reference/operators/proxmox-operator/#resource-queries","title":"Resource Queries","text":"<p>Cluster Status:</p> <pre><code>GET /api2/json/cluster/status\n</code></pre> <p>Node Resources:</p> <pre><code>GET /api2/json/nodes/{node}/status\n</code></pre> <p>Storage Information:</p> <pre><code>GET /api2/json/nodes/{node}/storage\n</code></pre> <p>Network Configuration:</p> <pre><code>GET /api2/json/nodes/{node}/network\n</code></pre>"},{"location":"reference/operators/proxmox-operator/#vm-id-management","title":"VM ID Management","text":"Range Purpose Auto-Assignment 100-999 System templates No 1000-9999 Manual assignment No 10000-99999 Operator managed Yes 100000+ Reserved No <p>Auto-assignment algorithm:</p> <ol> <li>Query existing VMs to identify used IDs</li> <li>Find lowest available ID in operator range (10000+)</li> <li>Reserve ID during VM creation</li> <li>Handle conflicts with retry logic</li> </ol>"},{"location":"reference/operators/proxmox-operator/#processing-specifications","title":"Processing Specifications","text":""},{"location":"reference/operators/proxmox-operator/#resource-allocation","title":"Resource Allocation","text":""},{"location":"reference/operators/proxmox-operator/#cpu-configuration","title":"CPU Configuration","text":"Specification Proxmox Mapping Validation <code>cpu.cores</code> <code>cores</code> parameter 1-128 cores per VM <code>cpu.sockets</code> <code>sockets</code> parameter 1-4 sockets per VM <code>cpu.threadsPerCore</code> Calculated as <code>cores/(sockets*threads)</code> 1-2 threads per core"},{"location":"reference/operators/proxmox-operator/#memory-management","title":"Memory Management","text":"<ul> <li>Specification: Bytes (e.g., 2147483648 for 2GB)</li> <li>Proxmox Format: Megabytes (e.g., 2048)</li> <li>Conversion: <code>proxmox_mb = bytes / 1048576</code></li> <li>Constraints: Minimum 64MB, maximum node memory limit</li> </ul>"},{"location":"reference/operators/proxmox-operator/#storage-configuration","title":"Storage Configuration","text":"<p>Disk Naming Convention:</p> <pre><code>{type}{index}: {storage}:{size}[,format={format}][,cache={cache}]\nExample: scsi0: local-lvm:20,format=raw,cache=writeback\n</code></pre> <p>Supported Storage Types:</p> Type Interface Use Case <code>ide</code> IDE Legacy systems, CD-ROM <code>sata</code> SATA Standard storage <code>scsi</code> SCSI High-performance storage <code>virtio</code> VirtIO Paravirtualized storage (recommended)"},{"location":"reference/operators/proxmox-operator/#network-configuration","title":"Network Configuration","text":""},{"location":"reference/operators/proxmox-operator/#bridge-mapping","title":"Bridge Mapping","text":"Specification Proxmox Format Example <code>bridge: vmbr0</code> <code>bridge=vmbr0</code> <code>virtio,bridge=vmbr0</code> <code>model: virtio</code> <code>virtio</code> model prefix <code>virtio,bridge=vmbr0</code> <code>macAddress: aa:bb:cc:dd:ee:ff</code> <code>,macaddr=aa:bb:cc:dd:ee:ff</code> <code>virtio,bridge=vmbr0,macaddr=aa:bb:cc:dd:ee:ff</code>"},{"location":"reference/operators/proxmox-operator/#vlan-configuration","title":"VLAN Configuration","text":"<pre><code>networks:\n- name: eth0\n  bridge: vmbr0\n  vlan: 100        # Results in: virtio,bridge=vmbr0,tag=100\n</code></pre>"},{"location":"reference/operators/proxmox-operator/#error-handling-reference","title":"Error Handling Reference","text":""},{"location":"reference/operators/proxmox-operator/#error-classification","title":"Error Classification","text":"Error Type HTTP Status Retry Behavior Resolution Authentication 401 No Update credentials Authorization 403 No Check user permissions Resource Not Found 404 No Verify cluster/node exists Conflict 409 Yes Retry with backoff Server Error 500 Yes Check Proxmox status Network Error - Yes Verify connectivity"},{"location":"reference/operators/proxmox-operator/#reconciliation-backoff","title":"Reconciliation Backoff","text":"Attempt Delay Maximum Delay 1 1 second - 2 2 seconds - 3 4 seconds - 4+ 8 seconds 5 minutes"},{"location":"reference/operators/proxmox-operator/#common-error-patterns","title":"Common Error Patterns","text":"Error Message Cause Resolution <code>VM {vmid} already exists</code> VM ID conflict Use different VM ID or delete existing VM <code>insufficient resources on node</code> Resource exhaustion Select different node or increase resources <code>storage '{storage}' not found</code> Invalid storage pool Verify storage pool exists and is accessible <code>bridge '{bridge}' not found</code> Invalid network bridge Check network configuration on target node <code>template '{template}' not found</code> Missing VM template Create or upload required template"},{"location":"reference/operators/proxmox-operator/#monitoring-reference","title":"Monitoring Reference","text":""},{"location":"reference/operators/proxmox-operator/#prometheus-metrics","title":"Prometheus Metrics","text":"Metric Name Type Labels Description <code>proxmox_operator_machines_total</code> Gauge <code>phase</code>, <code>node</code> Total machines by phase <code>proxmox_operator_reconciliation_duration_seconds</code> Histogram <code>controller</code> Reconciliation duration <code>proxmox_operator_api_requests_total</code> Counter <code>method</code>, <code>endpoint</code>, <code>status</code> Proxmox API requests <code>proxmox_operator_errors_total</code> Counter <code>error_type</code>, <code>controller</code> Error occurrences <code>proxmox_cluster_nodes_available</code> Gauge <code>cluster</code> Available nodes per cluster <code>proxmox_cluster_connection_status</code> Gauge <code>cluster</code> Cluster connectivity status"},{"location":"reference/operators/proxmox-operator/#health-endpoints","title":"Health Endpoints","text":"Endpoint Purpose Status Codes <code>/healthz</code> Liveness probe 200 (healthy), 500 (unhealthy) <code>/readyz</code> Readiness probe 200 (ready), 500 (not ready) <code>/metrics</code> Prometheus metrics 200 (metrics available)"},{"location":"reference/operators/proxmox-operator/#status-conditions","title":"Status Conditions","text":"Condition Type Status Reason Description <code>Ready</code> True/False Various Overall machine readiness <code>VMCreated</code> True/False <code>CreationSucceeded</code>/<code>CreationFailed</code> VM creation status <code>ResourcesAllocated</code> True/False <code>AllocationSucceeded</code>/<code>InsufficientResources</code> Resource allocation <code>NetworkConfigured</code> True/False <code>NetworkReady</code>/<code>NetworkError</code> Network configuration"},{"location":"reference/operators/proxmox-operator/#security-reference","title":"Security Reference","text":""},{"location":"reference/operators/proxmox-operator/#rbac-requirements","title":"RBAC Requirements","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: proxmox-operator\nrules:\n- apiGroups: [\"vitistack.io\"]\n  resources: [\"machines\", \"proxmoxclusters\", \"proxmoxnodes\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"\"]\n  resources: [\"secrets\", \"events\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"]\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre>"},{"location":"reference/operators/proxmox-operator/#secret-management","title":"Secret Management","text":"<p>Credential Storage:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: proxmox-credentials\ntype: Opaque\nstringData:\n  username: \"operator@pve\"\n  password: \"secure-password\"\n  # OR for token authentication:\n  token-id: \"operator-token\"\n  token-secret: \"token-secret-value\"\n</code></pre>"},{"location":"reference/operators/proxmox-operator/#network-security","title":"Network Security","text":"<p>Proxmox API Access Requirements:</p> <ul> <li>Port: 8006 (HTTPS) or 8007 (SPICE proxy)</li> <li>Protocol: HTTPS (TLS 1.2+)</li> <li>Firewall: Allow operator pods to reach Proxmox nodes</li> </ul>"},{"location":"reference/operators/proxmox-operator/#deployment-reference","title":"Deployment Reference","text":""},{"location":"reference/operators/proxmox-operator/#helm-chart-configuration","title":"Helm Chart Configuration","text":"Parameter Default Description <code>image.repository</code> <code>ghcr.io/vitistack/proxmox-operator</code> Container image <code>image.tag</code> Chart version Image tag <code>replicaCount</code> 1 Operator replicas <code>resources.limits.cpu</code> <code>200m</code> CPU limit <code>resources.limits.memory</code> <code>256Mi</code> Memory limit <code>nodeSelector</code> <code>{}</code> Node selection constraints <code>tolerations</code> <code>[]</code> Pod tolerations <code>affinity</code> <code>{}</code> Pod affinity rules"},{"location":"reference/operators/proxmox-operator/#resource-requirements","title":"Resource Requirements","text":"<p>Minimum:</p> <ul> <li>CPU: 50m</li> <li>Memory: 128Mi</li> </ul> <p>Recommended:</p> <ul> <li>CPU: 200m  </li> <li>Memory: 256Mi</li> </ul> <p>Scaling Guidelines:</p> <ul> <li>CPU scales with number of managed VMs</li> <li>Memory scales with cluster size and reconciliation frequency</li> <li>Network bandwidth depends on Proxmox API call frequency</li> </ul>"},{"location":"reference/operators/proxmox-operator/#troubleshooting-reference","title":"Troubleshooting Reference","text":""},{"location":"reference/operators/proxmox-operator/#debug-commands","title":"Debug Commands","text":"<p>Check Operator Status:</p> <pre><code>kubectl get pods -n proxmox-operator-system\nkubectl logs -n proxmox-operator-system deployment/proxmox-operator-controller-manager\n</code></pre> <p>Verify CRD Resources:</p> <pre><code>kubectl get machines -A\nkubectl get proxmoxclusters -A  \nkubectl describe machine &lt;machine-name&gt;\n</code></pre> <p>Test Proxmox Connectivity:</p> <pre><code># Test API endpoint\ncurl -k https://proxmox.example.com:8006/api2/json/version\n\n# Validate credentials\ncurl -k -d \"username=operator@pve&amp;password=password\" \\\n  https://proxmox.example.com:8006/api2/json/access/ticket\n</code></pre>"},{"location":"reference/operators/proxmox-operator/#log-analysis","title":"Log Analysis","text":"<p>Common Log Patterns:</p> <pre><code># Successful VM creation\n\"Successfully created VM\" vmid=10001 node=pve-node1\n\n# Authentication failure  \n\"Authentication failed\" error=\"invalid credentials\"\n\n# Resource allocation error\n\"Insufficient resources\" node=pve-node1 requested_memory=4096 available=2048\n\n# Network configuration error\n\"Bridge not found\" bridge=vmbr0 node=pve-node1\n</code></pre> <p>This reference documentation provides comprehensive technical details for system administrators and developers working with the Proxmox Operator, assuming familiarity with Kubernetes operators, Proxmox VE, and virtualization concepts.</p>"},{"location":"reference/operators/talos-operator/","title":"Talos Operator","text":""},{"location":"reference/operators/talos-operator/#talos-operator","title":"Talos Operator","text":"<p>Work in progress!</p> <p>The Talos Operator manages Talos Linux Kubernetes clusters through declarative Custom Resource Definitions. It automates cluster lifecycle management by reconciling <code>KubernetesCluster</code> resources and generating the necessary <code>Machine</code> resources for cluster topology deployment.</p>"},{"location":"reference/operators/talos-operator/#architecture","title":"Architecture","text":""},{"location":"reference/operators/talos-operator/#controller-structure","title":"Controller Structure","text":"<p>The operator implements a single primary controller:</p> <ul> <li>KubernetesCluster Controller: Reconciles <code>vitistack.io/v1alpha1/KubernetesCluster</code> resources</li> <li>Machine Generation: Creates Machine resources based on cluster topology</li> <li>File Management: Saves machine manifests to filesystem for debugging and inspection</li> <li>Lifecycle Management: Handles cluster creation, updates, and deletion with proper cleanup</li> </ul>"},{"location":"reference/operators/talos-operator/#repository-structure","title":"Repository Structure","text":"<pre><code>\u251c\u2500\u2500 cmd/                           # Main entry point\n\u251c\u2500\u2500 api/controllers/v1alpha1/      # KubernetesCluster controller\n\u2502   \u251c\u2500\u2500 kubernetescluster_controller.go\n\u2502   \u2514\u2500\u2500 kubernetescluster_controller_test.go\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 default/                   # Default configuration overlay\n\u2502   \u251c\u2500\u2500 manager/                   # Operator deployment manifests\n\u2502   \u251c\u2500\u2500 rbac/                      # Role-based access control\n\u2502   \u251c\u2500\u2500 prometheus/                # Monitoring configuration\n\u2502   \u2514\u2500\u2500 network-policy/            # Network security policies\n\u251c\u2500\u2500 charts/talos-operator/         # Helm deployment charts\n\u251c\u2500\u2500 examples/                      # Sample KubernetesCluster resources\n\u251c\u2500\u2500 internal/                      # Internal implementation packages\n\u251c\u2500\u2500 pkg/consts/                    # Constants and configuration\n\u251c\u2500\u2500 hack/results/                  # Generated machine manifests (runtime)\n\u2514\u2500\u2500 test/                          # Test suites\n</code></pre>"},{"location":"reference/operators/talos-operator/#api-reference","title":"API Reference","text":""},{"location":"reference/operators/talos-operator/#kubernetescluster-resource","title":"KubernetesCluster Resource","text":"<p>The primary resource managed by the Talos Operator:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: KubernetesCluster\nmetadata:\n  name: string                     # Cluster identifier\n  namespace: string               # Kubernetes namespace\n  finalizers:\n    - cluster.vitistack.io/finalizer # Cleanup finalizer\nspec:\n  # Cluster Configuration\n  clusterName: string             # Talos cluster name\n  kubernetesVersion: string       # Kubernetes version (e.g., \"v1.28.3\")\n  talosVersion: string            # Talos Linux version (e.g., \"v1.5.5\")\n\n  # Network Configuration\n  clusterEndpoint: string         # Kubernetes API server endpoint\n  podSubnets: []string           # Pod CIDR ranges\n  serviceSubnets: []string       # Service CIDR ranges\n\n  # Node Topology\n  controlPlane:\n    replicas: int                 # Number of control plane nodes (1, 3, 5)\n    machineTemplate:              # Template for control plane machines\n      spec: MachineSpec          # Machine specification\n\n  workers:\n  - name: string                 # Worker group name\n    replicas: int                # Number of worker nodes\n    machineTemplate:             # Template for worker machines\n      spec: MachineSpec          # Machine specification\n\n  # Talos Configuration\n  talosConfig:\n    # Machine Configuration\n    machine:\n      type: string               # Machine type: controlplane, worker\n      token: string              # Bootstrap token\n      ca:                        # Certificate authority configuration\n        crt: string             # CA certificate\n        key: string             # CA private key\n      certSANs: []string         # Certificate subject alternative names\n\n    # Cluster Configuration\n    cluster:\n      name: string               # Cluster name\n      controlPlane:\n        endpoint: string         # Control plane endpoint\n      network:\n        dnsDomain: string        # Cluster DNS domain (default: cluster.local)\n        podSubnets: []string     # Pod CIDR ranges\n        serviceSubnets: []string # Service CIDR ranges\n\n    # Installation Configuration\n    install:\n      disk: string               # Installation disk (e.g., \"/dev/sda\")\n      image: string              # Talos system image\n      bootloader: bool           # Install bootloader\n      wipe: bool                 # Wipe disk before installation\n\nstatus:\n  phase: string                  # Current phase: Pending, Provisioning, Running, Failed\n  conditions: []Condition        # Status conditions\n  controlPlaneReady: bool        # Control plane readiness\n  workersReady: int             # Number of ready worker nodes\n  machineCount: int             # Total generated machines\n  observedGeneration: int       # Last observed resource generation\n  lastUpdated: string           # Last reconciliation timestamp\n</code></pre>"},{"location":"reference/operators/talos-operator/#generated-machine-resources","title":"Generated Machine Resources","text":"<p>The operator generates Machine resources with the following structure:</p> <pre><code>apiVersion: vitistack.io/v1alpha1\nkind: Machine\nmetadata:\n  name: string # Generated machine name\n  namespace: string # Inherited from KubernetesCluster\n  labels:\n    cluster.vitistack.io/cluster-name: string # Cluster reference\n    cluster.vitistack.io/role: string # Node role: controlplane, worker\n    cluster.vitistack.io/worker-group: string # Worker group name (workers only)\n  ownerReferences:\n    - apiVersion: vitistack.io/v1alpha1\n      kind: KubernetesCluster\n      name: string # Parent cluster name\n      uid: string # Parent cluster UID\nspec:\n  # Inherited from machineTemplate in KubernetesCluster\n  # Machine-specific configuration based on role and template\n\n  # Talos-specific additions\n  talosConfig:\n    machineType: string # controlplane or worker\n    clusterEndpoint: string # Kubernetes API endpoint\n    installDisk: string # Target installation disk\n</code></pre>"},{"location":"reference/operators/talos-operator/#configuration-reference","title":"Configuration Reference","text":""},{"location":"reference/operators/talos-operator/#environment-variables","title":"Environment Variables","text":"Variable Type Default Description <code>KUBEBUILDER_ASSETS</code> string - Path to Kubebuilder test binaries <code>RECONCILE_INTERVAL</code> duration 30s KubernetesCluster reconciliation interval <code>MAX_CONCURRENT_RECONCILES</code> int 1 Maximum concurrent reconciliations <code>METRICS_BIND_ADDRESS</code> string <code>:8080</code> Metrics server bind address <code>HEALTH_PROBE_BIND_ADDRESS</code> string <code>:8081</code> Health probe bind address <code>RESULTS_PATH</code> string <code>hack/results</code> Path for generated machine manifests"},{"location":"reference/operators/talos-operator/#talos-configuration-parameters","title":"Talos Configuration Parameters","text":""},{"location":"reference/operators/talos-operator/#machine-configuration","title":"Machine Configuration","text":"Parameter Type Required Description <code>machine.type</code> string Yes Machine type: <code>controlplane</code> or <code>worker</code> <code>machine.token</code> string Yes Bootstrap token for cluster joining <code>machine.ca.crt</code> string Yes Certificate Authority certificate <code>machine.ca.key</code> string Yes Certificate Authority private key <code>machine.certSANs</code> []string No Additional certificate SANs <code>machine.kubelet.image</code> string No Kubelet container image <code>machine.kubelet.extraArgs</code> map[string]string No Additional kubelet arguments"},{"location":"reference/operators/talos-operator/#cluster-configuration","title":"Cluster Configuration","text":"Parameter Type Default Description <code>cluster.name</code> string - Cluster identifier <code>cluster.controlPlane.endpoint</code> string - Kubernetes API server endpoint <code>cluster.network.dnsDomain</code> string <code>cluster.local</code> Cluster DNS domain <code>cluster.network.podSubnets</code> []string <code>[\"10.244.0.0/16\"]</code> Pod CIDR ranges <code>cluster.network.serviceSubnets</code> []string <code>[\"10.96.0.0/12\"]</code> Service CIDR ranges"},{"location":"reference/operators/talos-operator/#installation-configuration","title":"Installation Configuration","text":"Parameter Type Default Description <code>install.disk</code> string <code>/dev/sda</code> Target installation disk <code>install.image</code> string - Talos system image URL <code>install.bootloader</code> bool true Install bootloader <code>install.wipe</code> bool false Wipe disk before installation <code>install.extraKernelArgs</code> []string - Additional kernel arguments"},{"location":"reference/operators/talos-operator/#operational-reference","title":"Operational Reference","text":""},{"location":"reference/operators/talos-operator/#reconciliation-workflow","title":"Reconciliation Workflow","text":"<p>The KubernetesCluster controller implements the following reconciliation logic:</p> <ol> <li>Resource Validation: Validates KubernetesCluster specification</li> <li>Finalizer Management: Adds finalizer for cleanup handling</li> <li>Machine Generation: Creates Machine resources based on topology:</li> <li>Control plane machines: <code>{cluster-name}-cp-{index}</code></li> <li>Worker machines: <code>{cluster-name}-{worker-group}-{index}</code></li> <li>File Generation: Saves machine manifests to <code>hack/results/{cluster-name}/</code></li> <li>Owner Reference: Sets KubernetesCluster as owner for automatic cleanup</li> <li>Status Update: Reports cluster status and machine count</li> <li>Deletion Handling: Cleans up machines and generated files on deletion</li> </ol>"},{"location":"reference/operators/talos-operator/#machine-naming-conventions","title":"Machine Naming Conventions","text":"Node Type Naming Pattern Example Control Plane <code>{cluster-name}-cp-{index}</code> <code>prod-cluster-cp-0</code> Worker <code>{cluster-name}-{worker-group}-{index}</code> <code>prod-cluster-workers-0</code> <p>Index starts from 0 and increments based on replica count.</p>"},{"location":"reference/operators/talos-operator/#file-system-operations","title":"File System Operations","text":""},{"location":"reference/operators/talos-operator/#generated-manifest-structure","title":"Generated Manifest Structure","text":"<pre><code>hack/results/{cluster-name}/\n\u251c\u2500\u2500 {cluster-name}-cp-0.yaml          # Control plane machine 0\n\u251c\u2500\u2500 {cluster-name}-cp-1.yaml          # Control plane machine 1\n\u251c\u2500\u2500 {cluster-name}-cp-2.yaml          # Control plane machine 2\n\u251c\u2500\u2500 {cluster-name}-workers-0.yaml     # Worker group machine 0\n\u251c\u2500\u2500 {cluster-name}-workers-1.yaml     # Worker group machine 1\n\u2514\u2500\u2500 {cluster-name}-custom-0.yaml      # Custom worker group machine 0\n</code></pre>"},{"location":"reference/operators/talos-operator/#file-operations","title":"File Operations","text":"Operation Trigger Behavior Create New KubernetesCluster Generate all machine manifests Update Spec change Regenerate affected machine manifests Delete Resource deletion Remove all cluster manifest files"},{"location":"reference/operators/talos-operator/#processing-specifications","title":"Processing Specifications","text":""},{"location":"reference/operators/talos-operator/#cluster-topology-processing","title":"Cluster Topology Processing","text":""},{"location":"reference/operators/talos-operator/#control-plane-generation","title":"Control Plane Generation","text":"<pre><code>for i := 0; i &lt; spec.ControlPlane.Replicas; i++ {\n    machine := &amp;Machine{\n        ObjectMeta: metav1.ObjectMeta{\n            Name:      fmt.Sprintf(\"%s-cp-%d\", clusterName, i),\n            Namespace: cluster.Namespace,\n            Labels: map[string]string{\n                \"cluster.vitistack.io/cluster-name\": clusterName,\n                \"cluster.vitistack.io/role\":         \"controlplane\",\n            },\n        },\n        Spec: spec.ControlPlane.MachineTemplate.Spec,\n    }\n    // Set owner reference and create machine\n}\n</code></pre>"},{"location":"reference/operators/talos-operator/#worker-group-generation","title":"Worker Group Generation","text":"<pre><code>for _, workerGroup := range spec.Workers {\n    for i := 0; i &lt; workerGroup.Replicas; i++ {\n        machine := &amp;Machine{\n            ObjectMeta: metav1.ObjectMeta{\n                Name:      fmt.Sprintf(\"%s-%s-%d\", clusterName, workerGroup.Name, i),\n                Namespace: cluster.Namespace,\n                Labels: map[string]string{\n                    \"cluster.vitistack.io/cluster-name\":    clusterName,\n                    \"cluster.vitistack.io/role\":            \"worker\",\n                    \"cluster.vitistack.io/worker-group\":    workerGroup.Name,\n                },\n            },\n            Spec: workerGroup.MachineTemplate.Spec,\n        }\n        // Set owner reference and create machine\n    }\n}\n</code></pre>"},{"location":"reference/operators/talos-operator/#talos-configuration-generation","title":"Talos Configuration Generation","text":""},{"location":"reference/operators/talos-operator/#machine-type-mapping","title":"Machine Type Mapping","text":"KubernetesCluster Role Talos Machine Type Configuration <code>controlPlane</code> <code>controlplane</code> API server, etcd, scheduler, controller-manager <code>workers[].name</code> <code>worker</code> Kubelet, container runtime"},{"location":"reference/operators/talos-operator/#network-configuration-processing","title":"Network Configuration Processing","text":"<pre><code># From KubernetesCluster spec\nspec:\n  podSubnets: [\"10.244.0.0/16\"]\n  serviceSubnets: [\"10.96.0.0/12\"]\n\n# Generated in Machine talosConfig\ncluster:\n  network:\n    podSubnets: [\"10.244.0.0/16\"]\n    serviceSubnets: [\"10.96.0.0/12\"]\n</code></pre>"},{"location":"reference/operators/talos-operator/#error-handling-reference","title":"Error Handling Reference","text":""},{"location":"reference/operators/talos-operator/#reconciliation-errors","title":"Reconciliation Errors","text":"Error Type Condition Recovery Action <code>ValidationError</code> Invalid cluster specification Fix specification and reapply <code>MachineCreationError</code> Failed to create Machine resource Retry with exponential backoff <code>FileSystemError</code> Failed to write manifest files Check filesystem permissions <code>OwnerReferenceError</code> Failed to set owner references Retry operation"},{"location":"reference/operators/talos-operator/#status-conditions","title":"Status Conditions","text":"Condition Type Status Reason Description <code>Ready</code> True/False Various Overall cluster readiness <code>MachinesCreated</code> True/False <code>CreationSucceeded</code>/<code>CreationFailed</code> Machine resource creation <code>FilesGenerated</code> True/False <code>GenerationSucceeded</code>/<code>GenerationFailed</code> Manifest file generation <code>TopologyValid</code> True/False <code>ValidationSucceeded</code>/<code>ValidationFailed</code> Cluster topology validation"},{"location":"reference/operators/talos-operator/#error-recovery-patterns","title":"Error Recovery Patterns","text":""},{"location":"reference/operators/talos-operator/#exponential-backoff","title":"Exponential Backoff","text":"Attempt Delay Maximum Delay 1 1 second - 2 2 seconds - 3 4 seconds - 4+ 8 seconds 5 minutes"},{"location":"reference/operators/talos-operator/#cleanup-procedures","title":"Cleanup Procedures","text":"<p>Finalizer Handling:</p> <pre><code>metadata:\n  finalizers:\n    - cluster.vitistack.io/finalizer\n</code></pre> <p>Cleanup Steps:</p> <ol> <li>Delete all owned Machine resources</li> <li>Remove generated manifest files</li> <li>Remove finalizer</li> <li>Allow resource deletion</li> </ol>"},{"location":"reference/operators/talos-operator/#monitoring-reference","title":"Monitoring Reference","text":""},{"location":"reference/operators/talos-operator/#prometheus-metrics","title":"Prometheus Metrics","text":"Metric Name Type Labels Description <code>talos_operator_clusters_total</code> Gauge <code>phase</code> Total clusters by phase <code>talos_operator_machines_generated</code> Counter <code>cluster</code>, <code>role</code> Generated machines count <code>talos_operator_reconciliation_duration_seconds</code> Histogram <code>controller</code> Reconciliation duration <code>talos_operator_reconciliation_errors_total</code> Counter <code>controller</code>, <code>error_type</code> Reconciliation errors <code>talos_operator_file_operations_total</code> Counter <code>operation</code>, <code>status</code> File operations"},{"location":"reference/operators/talos-operator/#health-endpoints","title":"Health Endpoints","text":"Endpoint Purpose Response Codes <code>/healthz</code> Liveness probe 200 (healthy), 500 (unhealthy) <code>/readyz</code> Readiness probe 200 (ready), 500 (not ready) <code>/metrics</code> Prometheus metrics 200 (metrics available)"},{"location":"reference/operators/talos-operator/#logging-reference","title":"Logging Reference","text":""},{"location":"reference/operators/talos-operator/#log-levels","title":"Log Levels","text":"Level Usage Example <code>INFO</code> Normal operations <code>\"Successfully created machine\"</code> <code>WARN</code> Non-fatal issues <code>\"Machine already exists, skipping\"</code> <code>ERROR</code> Reconciliation failures <code>\"Failed to create machine resource\"</code> <code>DEBUG</code> Detailed tracing <code>\"Processing worker group: workers\"</code>"},{"location":"reference/operators/talos-operator/#structured-logging-fields","title":"Structured Logging Fields","text":"<pre><code>{\n  \"level\": \"info\",\n  \"timestamp\": \"2024-01-01T12:00:00Z\",\n  \"controller\": \"KubernetesCluster\",\n  \"cluster\": \"prod-cluster\",\n  \"namespace\": \"default\",\n  \"message\": \"Successfully reconciled cluster\"\n}\n</code></pre>"},{"location":"reference/operators/talos-operator/#security-reference","title":"Security Reference","text":""},{"location":"reference/operators/talos-operator/#rbac-requirements","title":"RBAC Requirements","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: talos-operator\nrules:\n  - apiGroups: [\"vitistack.io\"]\n    resources: [\"kubernetesclusters\", \"machines\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n  - apiGroups: [\"\"]\n    resources: [\"events\"]\n    verbs: [\"create\", \"patch\"]\n  - apiGroups: [\"coordination.k8s.io\"]\n    resources: [\"leases\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n</code></pre>"},{"location":"reference/operators/talos-operator/#service-account-configuration","title":"Service Account Configuration","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: talos-operator-controller-manager\n  namespace: talos-operator-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: talos-operator-manager-rolebinding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: talos-operator\nsubjects:\n  - kind: ServiceAccount\n    name: talos-operator-controller-manager\n    namespace: talos-operator-system\n</code></pre>"},{"location":"reference/operators/talos-operator/#deployment-reference","title":"Deployment Reference","text":""},{"location":"reference/operators/talos-operator/#helm-chart-configuration","title":"Helm Chart Configuration","text":"Parameter Default Description <code>image.repository</code> <code>ghcr.io/vitistack/talos-operator</code> Container image <code>image.tag</code> Chart version Image tag <code>image.pullPolicy</code> <code>IfNotPresent</code> Image pull policy <code>replicaCount</code> 1 Operator replicas <code>resources.limits.cpu</code> <code>500m</code> CPU limit <code>resources.limits.memory</code> <code>128Mi</code> Memory limit <code>resources.requests.cpu</code> <code>10m</code> CPU request <code>resources.requests.memory</code> <code>64Mi</code> Memory request"},{"location":"reference/operators/talos-operator/#installation-methods","title":"Installation Methods","text":""},{"location":"reference/operators/talos-operator/#helm-installation","title":"Helm Installation","text":"<pre><code># Add Helm repository\nhelm repo add vitistack oci://ghcr.io/vitistack/helm\n\n# Install operator\nhelm install talos-operator vitistack/talos-operator \\\n  --namespace vitistack \\\n  --create-namespace\n</code></pre>"},{"location":"reference/operators/talos-operator/#upgrade-to-latest-version","title":"Upgrade to latest version","text":"<pre><code>helm install talos-operator vitistack/talos-operator \\\n  --namespace vitistack \\\n  --create-namespace \\\n  --reuse-values\n</code></pre>"},{"location":"reference/operators/talos-operator/#direct-deployment","title":"Direct Deployment","text":"<pre><code># Deploy using Makefile\nmake deploy IMG=ghcr.io/vitistack/talos-operator:latest\n</code></pre>"},{"location":"reference/operators/talos-operator/#example-configurations","title":"Example Configurations","text":""},{"location":"reference/operators/talos-operator/#simple-cluster","title":"Simple Cluster","text":"<pre><code>apiVersion: vitistack.io/v1alpha1\nkind: KubernetesCluster\nmetadata:\n  name: simple-cluster\nspec:\n  clusterName: simple-cluster\n  kubernetesVersion: v1.28.3\n  talosVersion: v1.5.5\n  clusterEndpoint: https://192.168.1.100:6443\n\n  controlPlane:\n    replicas: 3\n    machineTemplate:\n      spec:\n        # Machine specification for control plane nodes\n\n  workers:\n    - name: workers\n      replicas: 2\n      machineTemplate:\n        spec:\n          # Machine specification for worker nodes\n</code></pre>"},{"location":"reference/operators/talos-operator/#multi-worker-group-cluster","title":"Multi-Worker Group Cluster","text":"<pre><code>apiVersion: vitistack.io/v1alpha1\nkind: KubernetesCluster\nmetadata:\n  name: complex-cluster\nspec:\n  clusterName: complex-cluster\n  kubernetesVersion: v1.28.3\n  talosVersion: v1.5.5\n  clusterEndpoint: https://192.168.1.100:6443\n\n  controlPlane:\n    replicas: 3\n    machineTemplate:\n      spec: {} # Control plane machine spec\n\n  workers:\n    - name: general-workers\n      replicas: 3\n      machineTemplate:\n        spec: {} # General worker spec\n\n    - name: gpu-workers\n      replicas: 2\n      machineTemplate:\n        spec: {} # GPU worker spec\n\n    - name: storage-workers\n      replicas: 1\n      machineTemplate:\n        spec: {} # Storage worker spec\n</code></pre> <p>This reference documentation provides comprehensive technical details for system administrators and developers working with the Talos Operator, assuming familiarity with Kubernetes operators, Talos Linux, and cluster-api concepts.</p>"},{"location":"reference/operators/vitistack-operator/","title":"Vitistack Operator","text":""},{"location":"reference/operators/vitistack-operator/#vitistack-operator","title":"Vitistack Operator","text":"<p>Work in progress!</p> <p>The Vitistack Operator provides a centralized API for managing and orchestrating Viti Stack infrastructure components. It acts as the core control plane operator that aggregates data from various infrastructure operators and provides a unified interface for infrastructure management and monitoring.</p>"},{"location":"reference/operators/vitistack-operator/#architecture","title":"Architecture","text":""},{"location":"reference/operators/vitistack-operator/#application-structure","title":"Application Structure","text":"<p>The Vitistack Operator is built as a REST API service rather than a traditional Kubernetes controller-based operator:</p> <ul> <li>HTTP Server: RESTful API service for infrastructure data aggregation</li> <li>Event Manager: Event processing and distribution system</li> <li>Repository Layer: Data persistence and retrieval abstraction</li> <li>Client Integrations: Connections to various Viti Stack operators</li> <li>Cache Layer: Performance optimization for frequently accessed data</li> </ul>"},{"location":"reference/operators/vitistack-operator/#repository-structure","title":"Repository Structure","text":"<pre><code>\u251c\u2500\u2500 cmd/vitistack-operator/        # Main application entry point\n\u251c\u2500\u2500 internal/\n\u2502   \u251c\u2500\u2500 cache/                     # Caching implementations\n\u2502   \u251c\u2500\u2500 clients/                   # External service clients\n\u2502   \u251c\u2500\u2500 handlers/                  # HTTP request handlers\n\u2502   \u251c\u2500\u2500 helpers/                   # Utility functions\n\u2502   \u251c\u2500\u2500 httpserver/                # HTTP server configuration\n\u2502   \u251c\u2500\u2500 listeners/                 # Event listeners\n\u2502   \u251c\u2500\u2500 middlewares/               # HTTP middleware components\n\u2502   \u251c\u2500\u2500 repositories/              # Data repository implementations\n\u2502   \u251c\u2500\u2500 repositoryinterfaces/      # Repository interface definitions\n\u2502   \u251c\u2500\u2500 routes/                    # API route definitions\n\u2502   \u251c\u2500\u2500 services/                  # Business logic services\n\u2502   \u2514\u2500\u2500 settings/                  # Configuration management\n\u251c\u2500\u2500 pkg/\n\u2502   \u251c\u2500\u2500 consts/                    # Application constants\n\u2502   \u2514\u2500\u2500 eventmanager/              # Event management system\n\u251c\u2500\u2500 charts/vitistack-operator/     # Helm deployment charts\n\u2514\u2500\u2500 hack/                          # Development and build scripts\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#api-reference","title":"API Reference","text":""},{"location":"reference/operators/vitistack-operator/#core-service-endpoints","title":"Core Service Endpoints","text":"<p>The Vitistack Operator exposes a RESTful API for infrastructure management:</p>"},{"location":"reference/operators/vitistack-operator/#infrastructure-data-api","title":"Infrastructure Data API","text":"<p>Base URL: <code>http://vitistack-operator:9991/api/v1</code></p>"},{"location":"reference/operators/vitistack-operator/#cluster-information","title":"Cluster Information","text":"<pre><code>GET /clusters\n</code></pre> <p>Retrieves information about all managed Kubernetes clusters.</p> <p>Response Schema:</p> <pre><code>{\n  \"clusters\": [\n    {\n      \"id\": \"string\",\n      \"name\": \"string\",\n      \"namespace\": \"string\",\n      \"status\": \"string\",\n      \"nodes\": {\n        \"total\": \"integer\",\n        \"ready\": \"integer\"\n      },\n      \"operators\": {\n        \"proxmox\": \"boolean\",\n        \"talos\": \"boolean\",\n        \"kea\": \"boolean\",\n        \"kubeVirt\": \"boolean\"\n      },\n      \"lastUpdated\": \"timestamp\"\n    }\n  ]\n}\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#infrastructure-resources","title":"Infrastructure Resources","text":"<pre><code>GET /infrastructure\n</code></pre> <p>Aggregated view of infrastructure resources across all operators.</p> <p>Response Schema:</p> <pre><code>{\n  \"summary\": {\n    \"machines\": {\n      \"total\": \"integer\",\n      \"running\": \"integer\",\n      \"pending\": \"integer\",\n      \"failed\": \"integer\"\n    },\n    \"networks\": {\n      \"total\": \"integer\",\n      \"active\": \"integer\"\n    },\n    \"storage\": {\n      \"total\": \"integer\",\n      \"available\": \"integer\"\n    }\n  },\n  \"operators\": {\n    \"proxmox\": {\n      \"status\": \"string\",\n      \"version\": \"string\",\n      \"resources\": {}\n    },\n    \"talos\": {\n      \"status\": \"string\",\n      \"version\": \"string\",\n      \"resources\": {}\n    },\n    \"kea\": {\n      \"status\": \"string\",\n      \"version\": \"string\",\n      \"resources\": {}\n    },\n    \"kubevirt\": {\n      \"status\": \"string\",\n      \"version\": \"string\",\n      \"resources\": {}\n    }\n  }\n}\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#resource-metrics","title":"Resource Metrics","text":"<pre><code>GET /metrics\n</code></pre> <p>Prometheus-compatible metrics endpoint for monitoring.</p> <p>Metrics Format:</p> <pre><code># HELP vitistack_operator_clusters_total Total number of managed clusters\n# TYPE vitistack_operator_clusters_total gauge\nvitistack_operator_clusters_total{status=\"active\"} 3\n\n# HELP vitistack_operator_machines_total Total number of managed machines\n# TYPE vitistack_operator_machines_total gauge\nvitistack_operator_machines_total{operator=\"proxmox\",status=\"running\"} 15\n\n# HELP vitistack_operator_api_requests_total Total API requests\n# TYPE vitistack_operator_api_requests_total counter\nvitistack_operator_api_requests_total{method=\"GET\",endpoint=\"/clusters\",status=\"200\"} 1234\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#health-endpoints","title":"Health Endpoints","text":"<pre><code>GET /health\n</code></pre> <p>Service health check endpoint.</p> <p>Response Schema:</p> <pre><code>{\n  \"status\": \"healthy|degraded|unhealthy\",\n  \"timestamp\": \"timestamp\",\n  \"checks\": {\n    \"database\": \"healthy|unhealthy\",\n    \"cache\": \"healthy|unhealthy\",\n    \"operators\": {\n      \"proxmox\": \"healthy|unhealthy|unreachable\",\n      \"talos\": \"healthy|unhealthy|unreachable\",\n      \"kea\": \"healthy|unhealthy|unreachable\",\n      \"kubevirt\": \"healthy|unhealthy|unreachable\"\n    }\n  }\n}\n</code></pre> <pre><code>GET /ready\n</code></pre> <p>Readiness probe endpoint for Kubernetes.</p> <p>Response Codes:</p> <ul> <li><code>200</code>: Service ready to accept traffic</li> <li><code>503</code>: Service not ready</li> </ul>"},{"location":"reference/operators/vitistack-operator/#configuration-reference","title":"Configuration Reference","text":""},{"location":"reference/operators/vitistack-operator/#environment-variables","title":"Environment Variables","text":"Variable Type Default Description <code>PORT</code> int 9991 HTTP server port <code>LOG_LEVEL</code> string info Logging level: debug, info, warn, error <code>CACHE_TTL</code> duration 300s Cache time-to-live <code>METRICS_ENABLED</code> bool true Enable Prometheus metrics <code>HEALTH_CHECK_INTERVAL</code> duration 30s Health check interval <code>API_TIMEOUT</code> duration 30s API request timeout <code>DATABASE_URL</code> string - Database connection string <code>REDIS_URL</code> string - Redis cache connection string"},{"location":"reference/operators/vitistack-operator/#operator-client-configuration","title":"Operator Client Configuration","text":""},{"location":"reference/operators/vitistack-operator/#proxmox-operator-integration","title":"Proxmox Operator Integration","text":"Parameter Type Default Description <code>PROXMOX_OPERATOR_ENDPOINT</code> string - Proxmox operator API endpoint <code>PROXMOX_OPERATOR_NAMESPACE</code> string default Proxmox operator namespace <code>PROXMOX_SYNC_INTERVAL</code> duration 60s Data synchronization interval"},{"location":"reference/operators/vitistack-operator/#talos-operator-integration","title":"Talos Operator Integration","text":"Parameter Type Default Description <code>TALOS_OPERATOR_ENDPOINT</code> string - Talos operator API endpoint <code>TALOS_OPERATOR_NAMESPACE</code> string default Talos operator namespace <code>TALOS_SYNC_INTERVAL</code> duration 60s Data synchronization interval"},{"location":"reference/operators/vitistack-operator/#kea-operator-integration","title":"Kea Operator Integration","text":"Parameter Type Default Description <code>KEA_OPERATOR_ENDPOINT</code> string - Kea operator API endpoint <code>KEA_OPERATOR_NAMESPACE</code> string default Kea operator namespace <code>KEA_SYNC_INTERVAL</code> duration 30s Data synchronization interval"},{"location":"reference/operators/vitistack-operator/#kubevirt-operator-integration","title":"KubeVirt Operator Integration","text":"Parameter Type Default Description <code>KUBEVIRT_OPERATOR_ENDPOINT</code> string - KubeVirt operator API endpoint <code>KUBEVIRT_OPERATOR_NAMESPACE</code> string kubevirt KubeVirt operator namespace <code>KUBEVIRT_SYNC_INTERVAL</code> duration 45s Data synchronization interval"},{"location":"reference/operators/vitistack-operator/#service-architecture-reference","title":"Service Architecture Reference","text":""},{"location":"reference/operators/vitistack-operator/#http-server-configuration","title":"HTTP Server Configuration","text":""},{"location":"reference/operators/vitistack-operator/#server-settings","title":"Server Settings","text":"Parameter Type Default Description <code>server.host</code> string 0.0.0.0 Server bind address <code>server.port</code> int 9991 Server port <code>server.readTimeout</code> duration 30s Read timeout <code>server.writeTimeout</code> duration 30s Write timeout <code>server.idleTimeout</code> duration 120s Idle connection timeout <code>server.maxHeaderBytes</code> int 1048576 Maximum header size (1MB)"},{"location":"reference/operators/vitistack-operator/#middleware-configuration","title":"Middleware Configuration","text":"<p>Logging Middleware:</p> <pre><code>{\n  \"enabled\": true,\n  \"format\": \"json\",\n  \"includeHeaders\": false,\n  \"excludePaths\": [\"/health\", \"/ready\"]\n}\n</code></pre> <p>CORS Middleware:</p> <pre><code>{\n  \"enabled\": true,\n  \"allowedOrigins\": [\"*\"],\n  \"allowedMethods\": [\"GET\", \"POST\", \"PUT\", \"DELETE\"],\n  \"allowedHeaders\": [\"Content-Type\", \"Authorization\"],\n  \"exposedHeaders\": [\"X-Total-Count\"],\n  \"maxAge\": 86400\n}\n</code></pre> <p>Rate Limiting:</p> <pre><code>{\n  \"enabled\": true,\n  \"requestsPerMinute\": 60,\n  \"burstSize\": 10,\n  \"keyGenerator\": \"ip\"\n}\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#repository-layer","title":"Repository Layer","text":""},{"location":"reference/operators/vitistack-operator/#database-configuration","title":"Database Configuration","text":"<p>Supported Databases:</p> <ul> <li>PostgreSQL (recommended)</li> <li>MySQL</li> <li>SQLite (development only)</li> </ul> <p>Connection Pool Settings:</p> Parameter Type Default Description <code>db.maxOpenConns</code> int 25 Maximum open connections <code>db.maxIdleConns</code> int 5 Maximum idle connections <code>db.connMaxLifetime</code> duration 300s Connection maximum lifetime <code>db.connMaxIdleTime</code> duration 60s Connection maximum idle time"},{"location":"reference/operators/vitistack-operator/#cache-configuration","title":"Cache Configuration","text":"<p>Redis Settings:</p> Parameter Type Default Description <code>cache.redis.host</code> string localhost Redis host <code>cache.redis.port</code> int 6379 Redis port <code>cache.redis.db</code> int 0 Redis database number <code>cache.redis.password</code> string - Redis password <code>cache.redis.maxRetries</code> int 3 Maximum retry attempts <code>cache.redis.poolSize</code> int 10 Connection pool size"},{"location":"reference/operators/vitistack-operator/#event-management-system","title":"Event Management System","text":""},{"location":"reference/operators/vitistack-operator/#event-types","title":"Event Types","text":"Event Type Description Data Schema <code>infrastructure.machine.created</code> New machine provisioned Machine resource details <code>infrastructure.machine.deleted</code> Machine deprovisioned Machine identifier and metadata <code>infrastructure.cluster.updated</code> Cluster configuration changed Cluster diff information <code>infrastructure.operator.status</code> Operator status change Operator name and new status <code>infrastructure.network.configured</code> Network configuration applied Network configuration details"},{"location":"reference/operators/vitistack-operator/#event-processing","title":"Event Processing","text":"<p>Event Handler Configuration:</p> <pre><code>{\n  \"handlers\": {\n    \"database\": {\n      \"enabled\": true,\n      \"batchSize\": 100,\n      \"flushInterval\": \"10s\"\n    },\n    \"webhook\": {\n      \"enabled\": false,\n      \"endpoints\": [\n        {\n          \"url\": \"https://webhook.example.com/events\",\n          \"timeout\": \"30s\",\n          \"retries\": 3\n        }\n      ]\n    },\n    \"metrics\": {\n      \"enabled\": true,\n      \"updateInterval\": \"5s\"\n    }\n  }\n}\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#data-models-reference","title":"Data Models Reference","text":""},{"location":"reference/operators/vitistack-operator/#infrastructure-resource-model","title":"Infrastructure Resource Model","text":"<pre><code>{\n  \"id\": \"string\",\n  \"type\": \"machine|cluster|network|storage\",\n  \"name\": \"string\",\n  \"namespace\": \"string\",\n  \"operator\": \"proxmox|talos|kea|kubevirt\",\n  \"status\": \"pending|running|failed|unknown\",\n  \"metadata\": {\n    \"labels\": {},\n    \"annotations\": {},\n    \"createdAt\": \"timestamp\",\n    \"updatedAt\": \"timestamp\"\n  },\n  \"spec\": {},\n  \"status\": {}\n}\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#operator-status-model","title":"Operator Status Model","text":"<pre><code>{\n  \"operator\": \"string\",\n  \"version\": \"string\",\n  \"status\": \"healthy|degraded|unhealthy|unreachable\",\n  \"lastSeen\": \"timestamp\",\n  \"resources\": {\n    \"total\": \"integer\",\n    \"healthy\": \"integer\",\n    \"failed\": \"integer\"\n  },\n  \"capabilities\": [\"string\"],\n  \"endpoints\": {\n    \"api\": \"string\",\n    \"metrics\": \"string\",\n    \"health\": \"string\"\n  }\n}\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#cluster-resource-model","title":"Cluster Resource Model","text":"<pre><code>{\n  \"id\": \"string\",\n  \"name\": \"string\",\n  \"namespace\": \"string\",\n  \"type\": \"kubernetes|proxmox|talos\",\n  \"status\": \"active|inactive|failed\",\n  \"nodes\": {\n    \"total\": \"integer\",\n    \"ready\": \"integer\",\n    \"unready\": \"integer\"\n  },\n  \"networking\": {\n    \"podCIDR\": \"string\",\n    \"serviceCIDR\": \"string\",\n    \"dnsName\": \"string\"\n  },\n  \"operators\": {\n    \"installed\": [\"string\"],\n    \"healthy\": [\"string\"]\n  },\n  \"metadata\": {\n    \"createdAt\": \"timestamp\",\n    \"updatedAt\": \"timestamp\",\n    \"version\": \"string\"\n  }\n}\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#monitoring-reference","title":"Monitoring Reference","text":""},{"location":"reference/operators/vitistack-operator/#prometheus-metrics","title":"Prometheus Metrics","text":"Metric Name Type Labels Description <code>vitistack_operator_info</code> Gauge <code>version</code> Operator information <code>vitistack_operator_clusters_total</code> Gauge <code>status</code>, <code>type</code> Total clusters by status <code>vitistack_operator_machines_total</code> Gauge <code>operator</code>, <code>status</code>, <code>cluster</code> Total machines by operator <code>vitistack_operator_api_requests_total</code> Counter <code>method</code>, <code>endpoint</code>, <code>status</code> API request count <code>vitistack_operator_api_request_duration_seconds</code> Histogram <code>method</code>, <code>endpoint</code> API request duration <code>vitistack_operator_cache_hits_total</code> Counter <code>cache_type</code> Cache hit count <code>vitistack_operator_cache_misses_total</code> Counter <code>cache_type</code> Cache miss count <code>vitistack_operator_operator_status</code> Gauge <code>operator</code>, <code>status</code> Operator health status <code>vitistack_operator_events_processed_total</code> Counter <code>event_type</code>, <code>status</code> Processed event count <code>vitistack_operator_database_connections</code> Gauge <code>state</code> Database connection count"},{"location":"reference/operators/vitistack-operator/#health-check-endpoints","title":"Health Check Endpoints","text":"Endpoint Method Description Response Codes <code>/health</code> GET Overall health status 200, 503 <code>/health/live</code> GET Liveness check 200, 503 <code>/ready</code> GET Readiness check 200, 503 <code>/health/operators</code> GET Operator connectivity 200, 503 <code>/health/database</code> GET Database connectivity 200, 503 <code>/health/cache</code> GET Cache connectivity 200, 503"},{"location":"reference/operators/vitistack-operator/#logging-configuration","title":"Logging Configuration","text":""},{"location":"reference/operators/vitistack-operator/#log-levels","title":"Log Levels","text":"Level Usage Example <code>DEBUG</code> Detailed tracing <code>\"Processing request for cluster: prod-cluster\"</code> <code>INFO</code> Normal operations <code>\"Successfully synchronized operator data\"</code> <code>WARN</code> Non-critical issues <code>\"Operator connection timeout, retrying\"</code> <code>ERROR</code> Critical errors <code>\"Failed to update cluster status\"</code>"},{"location":"reference/operators/vitistack-operator/#structured-logging-format","title":"Structured Logging Format","text":"<pre><code>{\n  \"timestamp\": \"2024-01-01T12:00:00Z\",\n  \"level\": \"info\",\n  \"component\": \"httpserver\",\n  \"requestId\": \"req-123456\",\n  \"method\": \"GET\",\n  \"path\": \"/api/v1/clusters\",\n  \"statusCode\": 200,\n  \"duration\": \"150ms\",\n  \"userAgent\": \"kubectl/v1.28.0\",\n  \"message\": \"Request completed successfully\"\n}\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#security-reference","title":"Security Reference","text":""},{"location":"reference/operators/vitistack-operator/#authentication-authorization","title":"Authentication &amp; Authorization","text":""},{"location":"reference/operators/vitistack-operator/#api-key-authentication","title":"API Key Authentication","text":"<p>Header Format:</p> <pre><code>Authorization: Bearer &lt;api-key&gt;\n</code></pre> <p>API Key Management:</p> Parameter Type Description <code>auth.enabled</code> bool Enable API key authentication <code>auth.keyHeader</code> string Authorization header name <code>auth.keyPrefix</code> string Authorization header prefix <code>auth.keys</code> []string Valid API keys"},{"location":"reference/operators/vitistack-operator/#rbac-integration","title":"RBAC Integration","text":"<p>Kubernetes Service Account:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: vitistack-operator\n  namespace: vitistack-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: vitistack-operator\nrules:\n  - apiGroups: [\"vitistack.io\"]\n    resources: [\"*\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"events\"]\n    verbs: [\"create\", \"patch\"]\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#network-security","title":"Network Security","text":"<p>TLS Configuration:</p> Parameter Type Default Description <code>tls.enabled</code> bool false Enable TLS <code>tls.certFile</code> string - TLS certificate file <code>tls.keyFile</code> string - TLS private key file <code>tls.caFile</code> string - TLS CA certificate file <code>tls.minVersion</code> string 1.2 Minimum TLS version"},{"location":"reference/operators/vitistack-operator/#deployment-reference","title":"Deployment Reference","text":""},{"location":"reference/operators/vitistack-operator/#helm-chart-configuration","title":"Helm Chart Configuration","text":"Parameter Default Description <code>image.repository</code> <code>ghcr.io/vitistack/vitistack-operator</code> Container image <code>image.tag</code> Chart version Image tag <code>image.pullPolicy</code> <code>IfNotPresent</code> Image pull policy <code>replicaCount</code> 1 Number of replicas <code>service.type</code> <code>ClusterIP</code> Kubernetes service type <code>service.port</code> 9991 Service port <code>resources.limits.cpu</code> <code>500m</code> CPU limit <code>resources.limits.memory</code> <code>512Mi</code> Memory limit <code>resources.requests.cpu</code> <code>100m</code> CPU request <code>resources.requests.memory</code> <code>256Mi</code> Memory request"},{"location":"reference/operators/vitistack-operator/#installation-methods","title":"Installation Methods","text":""},{"location":"reference/operators/vitistack-operator/#helm-installation","title":"Helm Installation","text":"<pre><code># Add Helm repository\nhelm repo add vitistack oci://ghcr.io/vitistack/helm\n\n# Install operator\nhelm install vitistack-operator vitistack/vitistack-operator \\\n  --namespace vitistack \\\n  --create-namespace \\\n  --set config.logLevel=info \\\n  --set service.port=9991\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#upgrade-to-latest-version","title":"Upgrade to latest version","text":"<pre><code>helm install vitistack-operator vitistack/vitistack-operator \\\n  --namespace vitistack \\\n  --create-namespace \\\n  --reuse-values\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#direct-deployment","title":"Direct Deployment","text":"<pre><code># Build and run locally\ngo run cmd/vitistack-operator/main.go\n\n# Build container image\nCGO_ENABLED=0 go build -o dist/vitistack-operator \\\n  -ldflags '-w -extldflags \"-static\"' \\\n  cmd/vitistack-operator/main.go\ndocker build -t vitistack-operator:latest .\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#troubleshooting-reference","title":"Troubleshooting Reference","text":""},{"location":"reference/operators/vitistack-operator/#common-issues","title":"Common Issues","text":"Issue Symptom Resolution Operator connectivity failure 503 errors from <code>/health/operators</code> Verify operator endpoints and network connectivity Database connection timeout 500 errors, database health check fails Check database connectivity and credentials High memory usage OOM kills, performance degradation Tune cache settings and connection pools Cache performance issues Slow API responses Verify Redis connectivity and tune cache TTL"},{"location":"reference/operators/vitistack-operator/#debug-commands","title":"Debug Commands","text":"<p>Health Status Check:</p> <pre><code>kubectl exec -n vitistack-system deployment/vitistack-operator -- \\\n  curl localhost:9991/health\n</code></pre> <p>Operator Connectivity:</p> <pre><code>kubectl exec -n vitistack-system deployment/vitistack-operator -- \\\n  curl localhost:9991/health/operators\n</code></pre> <p>View Logs:</p> <pre><code>kubectl logs -n vitistack-system deployment/vitistack-operator -f\n</code></pre> <p>Metrics Inspection:</p> <pre><code>kubectl exec -n vitistack-system deployment/vitistack-operator -- \\\n  curl localhost:9991/metrics\n</code></pre>"},{"location":"reference/operators/vitistack-operator/#performance-tuning","title":"Performance Tuning","text":""},{"location":"reference/operators/vitistack-operator/#database-optimization","title":"Database Optimization","text":"Parameter Recommended Value Description <code>db.maxOpenConns</code> CPU cores \u00d7 4 Balance concurrency vs resources <code>db.maxIdleConns</code> <code>maxOpenConns / 5</code> Reduce idle connection overhead <code>db.connMaxLifetime</code> 300s Prevent stale connections"},{"location":"reference/operators/vitistack-operator/#cache-optimization","title":"Cache Optimization","text":"Parameter Recommended Value Description <code>cache.redis.poolSize</code> 20 Handle concurrent requests <code>cache.ttl</code> 300s Balance freshness vs performance <code>cache.redis.maxRetries</code> 3 Resilience without excessive delays <p>This reference documentation provides comprehensive technical details for system administrators and developers working with the Vitistack Operator, assuming familiarity with Kubernetes, REST APIs, and microservice architecture patterns.</p>"},{"location":"release_notes/","title":"Release Notes","text":""},{"location":"release_notes/#release-notes","title":"Release Notes","text":"<p>Release notes in Viti are brief documents that explain what has changed in new versions of software products. They typically include:</p> <ul> <li>New features - Newly added functionality</li> <li>Bug fixes - Resolved issues and problems</li> <li>Improvements - Enhancements to existing features</li> <li>Known issues - Current limitations or problems</li> <li>Breaking changes - Changes that may affect compatibility</li> </ul>"},{"location":"release_notes/#automated-generation","title":"Automated Generation","text":"<p>In Viti, all release notes are automatically generated from:</p> <ul> <li>Git commit messages</li> <li>Pull request descriptions</li> <li>Issue tracking data</li> <li>Version tags</li> </ul>"},{"location":"release_notes/#how-to-access-release-notes","title":"How to Access Release Notes","text":"<p>Release notes for each component are available:</p> <ul> <li>In the respective repository's releases section</li> <li>Through the Viti documentation portal</li> <li>Via automated notifications for major releases</li> </ul> <p>Note</p> <p>Release notes are generated automatically upon each release to ensure consistency and completeness.</p>"},{"location":"release_notes/v1.0.x/v1_0_x/","title":"Release notes - Viti v1.0.x","text":""},{"location":"release_notes/v1.0.x/v1_0_x/#release-notes-viti-v10x","title":"Release notes - Viti v1.0.x","text":"<p>v1.0.0</p>"},{"location":"release_notes/v1.0.x/v1_0_x/#new-feature","title":"New Feature","text":"<p>Text</p>"},{"location":"release_notes/v1.0.x/v1_0_x/#improvements","title":"Improvements","text":"<p>text</p>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#tutorials","title":"Tutorials","text":"<p>The Vitistack documentation follows Divio documentation structure which is a method for organizing documentation in a clear and efficient way. </p> <p>Tutorials is Step-by-step guides aimed at beginners, designed to teach by doing. </p> <p>They are wholly learning-oriented, and specifically, they are oriented towards learning how rather than learning what.</p>"},{"location":"tutorials/how/","title":"How Vitistack works","text":""},{"location":"tutorials/how/#how-vitistack-works","title":"How Vitistack works","text":"<p>Work in progress - information to come!</p>"}]}